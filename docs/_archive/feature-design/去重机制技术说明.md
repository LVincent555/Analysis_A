# 智能去重机制技术说明

## 📋 问题背景

在Excel数据中，可能出现**同一股票代码重复出现**的情况，例如：

```
行号  代码    名称      综合评分
3138  300695  兆丰股份   -4.7    ✅ 正常
3139  300695  兆丰股份   -4.2    ❌ 异常（分数明显偏离）
```

如果直接导入，会触发数据库唯一约束错误。

---

## 🎯 解决方案

### 设计原则

1. **保守策略**：只删除明确异常的数据
2. **严格条件**：多重判断确保安全
3. **保留ERROR**：复杂重复仍触发错误，需人工处理
4. **完整日志**：所有操作可追溯

---

## 🔧 实现逻辑

### 核心算法（deduplicate_helper.py）

```python
class DataDeduplicator:
    def deduplicate_stock_data(df):
        """
        1. 检测重复的股票代码
        2. 对每组重复，计算"合理性分数"
        3. 只删除明显异常的记录
        """
        
        for 重复的股票代码 in 所有重复:
            # 计算每条记录的统计指标
            for 每条记录 in 重复组:
                # 获取周围±5行的分数
                周围分数 = 上下5行的综合评分
                
                # 计算偏离度（标准差）
                偏离度 = |当前分数 - 周围均值| / 周围标准差
            
            # 严格条件判断
            if 分数差异 > 0.3 and 最大偏离度 > 2.0σ:
                删除偏离最大的记录
            else:
                保留所有（触发ERROR）
```

---

## 📊 严格条件

### 条件1：分数差异阈值

```python
score_diff = max(scores) - min(scores)

if score_diff < 0.3:
    # 差异太小，不是明显异常
    # 保留所有，触发ERROR
    return []
```

**原因**：分数差异小可能是正常的数据波动。

---

### 条件2：偏离度阈值

```python
# 计算周围±5行的分数统计
context_mean = 周围分数.mean()
context_std = 周围分数.std()

deviation = abs(score - context_mean) / context_std

if deviation > 2.0:
    # 偏离超过2个标准差，明显异常
    删除
```

**原因**：
- 2σ原则：正态分布中，95%的数据在±2σ范围内
- 超过2σ的数据可认为是异常值

---

## 🔍 示例分析

### 案例：300695（兆丰股份）

**原始数据：**
```
行号  代码    综合评分   周围均值  偏离度
3136  300953   -4.7      -4.70     0.0σ
3137  300230   -4.7      -4.70     0.0σ
3138  300695   -4.7      -4.70     0.0σ  ✅ 保留
3139  300695   -4.2      -4.70     2.5σ  ❌ 删除（异常）
3140  002715   -4.7      -4.68     0.1σ
3141  600903   -4.7      -4.68     0.1σ
```

**判断过程：**
1. ✅ 检测到重复：300695 出现2次
2. ✅ 分数差异：|-4.7 - (-4.2)| = 0.5 > 0.3
3. ✅ 行3139偏离度：2.5σ > 2.0
4. ✅ 删除行3139

**结果：**
```
📊 读取到 5434 条记录
⚠️  检测到 1 个重复的股票代码
  [去重] 股票 300695(兆丰股份) 行3139 分数=-4.2 周围均值=-4.7 偏离=2.5σ → 删除
📊 去重后剩余 5433 条记录
```

---

## ⚠️ 不会去重的情况

### 情况1：分数差异太小

```
300614  -4.7    ✅ 保留
300614  -4.6    ✅ 保留（差异0.1 < 0.3）

结果：触发ERROR，需人工检查
```

### 情况2：偏离度不够大

```
周围分数: [-4.0, -4.2, -4.1, -3.9, -4.3]
当前分数: -4.5
偏离度: 1.2σ < 2.0

结果：保留，触发ERROR
```

### 情况3：多条记录都异常

```
300695  -4.7  偏离=0.5σ  ✅
300695  -4.2  偏离=2.1σ  ❌ 删除
300695  -3.8  偏离=3.5σ  ？？？

结果：只删除最异常的一条，
      如果还有重复，触发ERROR
```

---

## 🛡️ 安全保障

### 1. 多重检查

```python
# 第一层：智能去重（删除明显异常）
df, stats = deduplicator.deduplicate_stock_data(df)

# 第二层：严格检查（捕获复杂重复）
if still_has_duplicates(df):
    raise ValueError("仍存在重复")  # 触发ERROR
```

### 2. 完整日志

```python
logger.warning(
    f"[去重] 股票 {code}({name}) "
    f"行{rank} "
    f"分数={score:.2f} "
    f"周围均值={context_mean:.2f} "
    f"偏离={deviation:.2f}σ "
    f"→ 删除"
)
```

### 3. 去重摘要

```
==============================================================
📋 数据去重摘要
==============================================================
去重条数: 1

  • 股票 300695(兆丰股份) 行3139 原因: 分数偏离周围数据超过2个标准差
    分数=-4.20, 周围均值=-4.70, 偏离=2.50σ
==============================================================
```

---

## 📝 使用说明

### 自动使用（无需配置）

导入数据时自动触发：

```bash
python update_daily_data.py
```

### 独立测试

```python
from deduplicate_helper import DataDeduplicator
import pandas as pd

# 读取数据
df = pd.read_excel("data.xlsx")

# 去重
deduplicator = DataDeduplicator()
df_clean, stats = deduplicator.deduplicate_stock_data(df)

# 查看结果
print(f"原始记录: {len(df)}")
print(f"去重后: {len(df_clean)}")
print(f"删除: {stats['removed_count']}")
```

---

## 🔬 算法参数

可调整的参数（在deduplicate_helper.py中）：

```python
# 周围行数（默认±5行）
context_start = max(0, idx - 5)
context_end = min(len(df), idx + 6)

# 分数差异阈值（默认0.3）
if score_diff < 0.3:
    return []

# 偏离度阈值（默认2.0σ）
if max_deviation_info['deviation'] > 2.0:
    删除
```

---

## 🎓 统计学原理

### 标准差（Standard Deviation）

衡量数据的离散程度：

```
σ = sqrt(Σ(x - μ)² / n)

其中：
- μ：均值
- x：每个数据点
- n：数据个数
```

### 2σ原则

在正态分布中：
- 68%的数据在 μ ± 1σ
- 95%的数据在 μ ± 2σ
- 99.7%的数据在 μ ± 3σ

**因此，偏离 > 2σ 的数据可认为是异常值。**

---

## 📚 相关文件

- `deduplicate_helper.py` - 去重核心逻辑
- `import_data_robust.py` - 集成去重功能
- `README_数据管理.md` - 用户使用文档

---

**最后更新：2025-11-07**
