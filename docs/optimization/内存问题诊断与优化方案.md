# 内存问题诊断与优化方案

## 🔍 问题诊断（2025-11-25 00:06）

### 服务器配置
- **CPU**: 2核心
- **内存**: 2GB
- **症状**: 内存飙升 → Swap激活 → 磁盘IO爆炸 → 服务器宕机

---

## 🎯 根本原因分析

### 1. **数据库连接池过大** ⚠️⚠️⚠️ 
**最严重的问题！**

**当前配置**（database.py.example）：
```python
engine = create_engine(
    DATABASE_URL,
    pool_size=10,        # 连接池大小
    max_overflow=20,     # 最大溢出连接数
    pool_pre_ping=True
)
```

**内存占用**：
- 每个PostgreSQL连接：10-20MB
- 最大连接数：10 + 20 = 30个
- **总占用**：30 × 15MB = **450MB**（仅数据库连接！）

**问题**：
- 2GB服务器，数据库连接就占用450MB（22.5%）
- 加上系统内存（~500MB）+ 应用内存（~500MB）= 1.45GB
- **剩余内存不足600MB，非常危险！**

---

### 2. **重复保存数据对象** ⚠️⚠️
**第二严重的问题！**

**问题代码**（memory_cache.py）：
```python
# 第41-42行：保存原始Python对象
self.daily_data_by_date: Dict[date, List[DailyStockData]] = defaultdict(list)
self.daily_data_by_stock: Dict[str, Dict[date, DailyStockData]] = defaultdict(dict)

# 构建内存索引（第89行开始）
for data in daily_data_list:  # 86836条记录
    self.daily_data_by_date[data.date].append(data)  # ❌ 保存Python对象
    self.daily_data_by_stock[data.stock_code][data.date] = data  # ❌ 再次保存
    
# 第107行：又构建了numpy缓存
numpy_stock_cache.build_from_data(daily_data_list)  # ✅ 优化版本
```

**内存占用**：
- 每个`DailyStockData`对象：2-5KB（包含83个字段）
- 86836条记录 × 3KB = **260MB**（原始对象）
- Numpy缓存：**9.3MB**（压缩版本）
- **总计**：260MB + 9.3MB = **269.3MB**

**问题**：
- Numpy缓存已经存储了所有数据（仅9.3MB）
- 但原始Python对象没有释放（260MB）
- **内存浪费28倍！**（260MB vs 9.3MB）

---

### 3. **Numpy是否会导致内存飙升？**
**答案：不会！Numpy是优化方案，不是问题！**

**证据**（启动日志）：
```
✅ Numpy数组构建完成，内存占用: 4.64 MB
✅ Numpy缓存: 9.30 MB (86836 条记录)
```

**对比**：
- **不用Numpy**: 86836条 × 3KB = **260MB**
- **用Numpy**: **9.3MB**
- **节省**: 250.7MB（**96.4%压缩率**）

**结论**：
✅ Numpy是救星，不是问题！
❌ 问题是原始对象没释放

---

### 4. **缓存无限增长** ⚠️

**问题代码**（多处）：
```python
# ttl_cache.py（已修复）
ttl_cache = TTLCache(max_size=100)  # ✅ 已限制

# hot_spots_cache.py
self.cache: Dict[str, HotSpotRankingData] = {}  # ❌ 无限制

# industry_detail_service.py
self.industry_detail_cache: Dict[str, CachedIndustryDetail] = {}  # ❌ 无限制
self.stock_signals_cache: Dict[str, CachedStockSignals] = {}  # ❌ 无限制
```

**潜在风险**：
- 每个用户查询都可能创建新缓存
- 无限增长可能导致内存泄漏

---

## 🚀 优化方案

### 方案1：缩小数据库连接池 ⚡⚡⚡（立即见效）

**优化目标**：450MB → **60MB**（节省390MB，87%）

**修改database.py**：
```python
engine = create_engine(
    DATABASE_URL,
    pool_size=2,         # 10 → 2（2核心CPU，2个连接够用）
    max_overflow=2,      # 20 → 2（减少溢出连接）
    pool_pre_ping=True,
    pool_recycle=3600,   # 🔥 新增：1小时回收连接，避免长连接占用
    pool_timeout=30,     # 🔥 新增：30秒超时
)
```

**效果**：
- 最大连接数：2 + 2 = 4个
- 内存占用：4 × 15MB = **60MB**
- **节省**：390MB

---

### 方案2：释放原始数据对象 ⚡⚡⚡（立即见效）

**优化目标**：260MB → **0MB**（节省260MB，100%）

**修改memory_cache.py**：

#### 方案2.1：不保存原始对象（推荐）
```python
# 第61行：load_all_data方法
def load_all_data(self):
    """一次性加载数据到内存（限制最近30天）"""
    logger.info("🔄 开始加载数据到内存...")
    
    db = SessionLocal()
    try:
        # ... 加载股票基础信息
        
        # 加载每日数据
        daily_data_list = db.query(DailyStockData).filter(...).all()
        
        # 🔥 优化：只构建numpy缓存，不保存Python对象
        logger.info("  3.5/5 构建Numpy优化数组...")
        numpy_stock_cache.build_from_data(daily_data_list)
        
        # ❌ 删除这部分（不再保存原始对象）
        # for data in daily_data_list:
        #     self.daily_data_by_date[data.date].append(data)
        #     self.daily_data_by_stock[data.stock_code][data.date] = data
        
        # 🔥 新增：从numpy缓存构建日期索引
        self.dates = sorted(
            numpy_stock_cache.get_all_dates(),
            reverse=True
        )
        
        # 🔥 显式释放原始对象
        daily_data_list = None
        
    finally:
        db.close()
```

#### 方案2.2：修改查询方法（使用numpy缓存）
```python
def get_stock_data(self, stock_code: str, target_date: date):
    """从numpy缓存获取数据，而不是从Python对象"""
    return numpy_stock_cache.get_data(stock_code, target_date)

def get_top_n_stocks(self, target_date: date, n: int):
    """从numpy缓存获取Top N股票"""
    return numpy_stock_cache.get_top_n_by_rank(target_date, n)
```

**效果**：
- 删除`daily_data_by_date`和`daily_data_by_stock`
- 所有查询改用`numpy_stock_cache`
- **节省**：260MB

---

### 方案3：限制所有缓存大小 ⚡（预防性）

**修改hot_spots_cache.py**：
```python
class HotSpotsCache:
    def __init__(self):
        self.cache: Dict[str, HotSpotRankingData] = {}
        self.max_cache_size = 30  # 🔥 限制最多30天数据
    
    def _set(self, date_str: str, data: HotSpotRankingData):
        """设置缓存，超过限制时删除最旧的"""
        if len(self.cache) >= self.max_cache_size:
            oldest_key = min(self.cache.keys())
            del self.cache[oldest_key]
        self.cache[date_str] = data
```

**修改industry_detail_service.py**：
```python
from collections import OrderedDict

class IndustryDetailService:
    def __init__(self):
        self.industry_detail_cache = OrderedDict()  # 🔥 使用有序字典
        self.stock_signals_cache = OrderedDict()
        self.max_cache_size = 50  # 🔥 限制50个条目
    
    def _add_to_cache(self, cache_dict, key, value):
        """添加到缓存，超过限制时删除最旧的"""
        if len(cache_dict) >= self.max_cache_size:
            cache_dict.popitem(last=False)  # 删除最旧的
        cache_dict[key] = value
```

---

### 方案4：启用内存监控 ⚡（监控性）

**新增文件：backend/app/middleware/memory_monitor.py**：
```python
"""内存监控中间件"""
import psutil
import logging
from starlette.middleware.base import BaseHTTPMiddleware

logger = logging.getLogger(__name__)

class MemoryMonitor(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        # 获取内存使用情况
        memory = psutil.Process().memory_info()
        memory_mb = memory.rss / 1024 / 1024
        
        # 警告：内存超过1.5GB
        if memory_mb > 1500:
            logger.warning(f"⚠️  内存占用过高: {memory_mb:.1f}MB")
        
        response = await call_next(request)
        return response
```

**修改main.py**：
```python
# 添加内存监控
from .middleware.memory_monitor import MemoryMonitor
app.add_middleware(MemoryMonitor)
```

---

## 📊 综合优化效果

### 内存占用对比

| 项目 | 优化前 | 优化后 | 节省 |
|------|--------|--------|------|
| **数据库连接池** | 450MB | 60MB | 390MB (87%) |
| **原始数据对象** | 260MB | 0MB | 260MB (100%) |
| **Numpy缓存** | 9.3MB | 9.3MB | 0MB |
| **股票基础信息** | ~20MB | ~20MB | 0MB |
| **板块数据** | ~10MB | ~10MB | 0MB |
| **各种缓存** | ~50MB | ~50MB | 0MB |
| **系统开销** | ~500MB | ~500MB | 0MB |
| **应用基础** | ~200MB | ~200MB | 0MB |
| **总计** | **1499MB** | **849MB** | **650MB (43%)** |

### 可用内存对比

| 场景 | 优化前 | 优化后 | 改善 |
|------|--------|--------|------|
| **总内存** | 2048MB | 2048MB | - |
| **已用** | 1499MB (73%) | 849MB (41%) | ⬇️ 32% |
| **剩余** | 549MB (27%) | 1199MB (59%) | ⬆️ 118% |
| **Swap风险** | 高（随时触发） | 低（安全裕度） | ✅ 大幅降低 |

---

## 🎯 优化优先级

### 立即执行（今天）
1. ✅ **创建ttl_cache模块**（已完成）
2. ⚠️ **缩小数据库连接池**（修改database.py）
3. ⚠️ **释放原始数据对象**（修改memory_cache.py）

### 短期执行（本周）
4. 🔍 **限制所有缓存大小**（修改hot_spots_cache.py等）
5. 📊 **启用内存监控**（新增中间件）

### 长期优化（可选）
6. 升级服务器到4GB内存
7. 启用Redis外部缓存
8. 实现分布式部署

---

## ⚠️ 回答你的问题

### Q1: Numpy会不会导致内存飙升？
**A**: **不会！Numpy是优化方案！**
- Numpy将260MB压缩到9.3MB，节省96.4%内存
- 没有Numpy才会内存飙升！

### Q2: 数据库是否是内存问题原因？
**A**: **是的！数据库连接池是最大问题！**
- 30个连接占用450MB（22.5%总内存）
- 2核心CPU只需要2-4个连接
- 建议：pool_size=2, max_overflow=2

### Q3: Swap导致宕机的原因？
**A**: **内存不足 → Swap激活 → 磁盘IO爆炸 → 死机**

**链条**：
```
1. 内存占用1.5GB → 仅剩500MB
2. 突发请求 → 内存不足
3. 系统启用Swap → 磁盘替代内存
4. 磁盘IO远慢于内存（1000倍）
5. 磁盘IO爆炸（1.4M IOPS）
6. CPU等待IO → 单核100%
7. 服务器响应变慢 → 请求堆积
8. 内存继续膨胀 → Swap继续增长
9. 死循环 → 宕机
```

**根本原因**：
- 数据库连接池太大（450MB）
- 重复保存数据对象（260MB）
- 总计710MB浪费

---

## 🚀 立即行动

### 步骤1：修改database.py（如果存在）
```python
engine = create_engine(
    DATABASE_URL,
    pool_size=2,         # ⬅️ 改这里
    max_overflow=2,      # ⬅️ 改这里
    pool_pre_ping=True,
    pool_recycle=3600,   # ⬅️ 新增
    pool_timeout=30,     # ⬅️ 新增
)
```

### 步骤2：等待详细的memory_cache.py优化代码
我会在下一条消息中提供完整的修改方案。

### 步骤3：重启服务并监控
```bash
# 重启
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# 监控内存
watch -n 2 'free -h'
```

---

**优化完成时间**：2025-11-25 00:30（预计）
**预期效果**：内存占用从1.5GB降至850MB，不再宕机
