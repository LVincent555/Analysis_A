from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import pandas as pd
import os
import glob
from collections import defaultdict
from datetime import datetime
import hashlib
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

# åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¹¶å‘å¤„ç†
executor = ThreadPoolExecutor(max_workers=4)

app = FastAPI(title="è‚¡ç¥¨åˆ†æAPI", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥æŒ‡å®šå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ–‡ä»¶ç›®å½•
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")

# ç¼“å­˜å­˜å‚¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
cache_lock = threading.Lock()
analysis_cache = {}
file_hash_cache = {}

class DateRankInfo(BaseModel):
    date: str
    rank: int

class StockAnalysisResult(BaseModel):
    stock_code: str
    stock_name: str
    industry: str
    appearances: int
    latest_rank: int
    date_rank_info: List[DateRankInfo]

class StockDetail(BaseModel):
    stock_code: str
    stock_name: str
    industry: str
    price_change: float  # æ¶¨è·Œå¹…
    turnover_rate: float  # æ¢æ‰‹ç‡
    volume_days: float  # æ”¾é‡å¤©æ•°
    avg_volume_ratio_50: float  # å¹³å‡é‡æ¯”_50å¤©
    volatility: float  # æ³¢åŠ¨ç‡
    rank: int  # å½“å‰æ’å
    date: str  # æ—¥æœŸ

class IndustryStats(BaseModel):
    industry: str
    count: int
    percentage: float

class IndustryTrendData(BaseModel):
    date: str
    industry_counts: Dict[str, int]

class IndustryTrendResponse(BaseModel):
    dates: List[str]
    industries: List[str]
    data: List[IndustryTrendData]

class AnalysisResponse(BaseModel):
    period: str
    period_days: int
    analysis_dates: List[str]
    total_stocks: int
    stocks: List[StockAnalysisResult]

class AvailableDatesResponse(BaseModel):
    dates: List[str]
    latest_date: str
    total_files: int

def get_files_hash(files):
    """è®¡ç®—æ–‡ä»¶åˆ—è¡¨çš„å“ˆå¸Œå€¼"""
    hash_str = ""
    for date, file_path in files:
        if os.path.exists(file_path):
            mtime = os.path.getmtime(file_path)
            hash_str += f"{file_path}_{mtime}_"
    return hashlib.md5(hash_str.encode()).hexdigest()

def extract_date(filename):
    """ä»æ–‡ä»¶åæå–æ—¥æœŸ"""
    basename = os.path.basename(filename)
    date_str = basename[:8]
    return date_str

def get_excel_files(directory):
    """è‡ªåŠ¨è·å–ç›®å½•ä¸‹æ‰€æœ‰Excelæ–‡ä»¶å¹¶æŒ‰æ—¥æœŸæ’åº"""
    pattern = os.path.join(directory, '*_data_sma_feature_color.xlsx')
    files = glob.glob(pattern)
    
    # è¿‡æ»¤æ‰Excelä¸´æ—¶æ–‡ä»¶
    files = [f for f in files if not os.path.basename(f).startswith('~$')]
    
    # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    files_with_dates = []
    for file in files:
        date = extract_date(file)
        files_with_dates.append((date, file))
    
    files_with_dates.sort(reverse=True)
    return files_with_dates

def load_stock_data(file_path, filter_stocks=True, include_details=False, max_count=100):
    """åŠ è½½è‚¡ç¥¨æ•°æ®å¹¶è¿‡æ»¤"""
    df = pd.read_excel(file_path)
    
    # æ‰¾åˆ°å…³é”®åˆ—
    code_column = None
    name_column = None
    industry_column = None
    for col in df.columns:
        if 'ä»£ç ' in str(col) or 'code' in str(col).lower():
            code_column = col
        if 'åç§°' in str(col) or 'name' in str(col).lower():
            name_column = col
        if 'è¡Œä¸š' in str(col) or 'industry' in str(col).lower():
            industry_column = col
    
    if code_column is None:
        code_column = df.columns[1]
    if name_column is None:
        name_column = df.columns[2]
    
    # è·å–è‚¡ç¥¨æ•°æ®
    stocks_data = []
    for idx in range(min(max_count, len(df))):
        stock_code = str(df[code_column].iloc[idx]).strip()
        stock_name = str(df[name_column].iloc[idx]).strip() if name_column else stock_code
        stock_industry = str(df[industry_column].iloc[idx]).strip() if industry_column and pd.notna(df[industry_column].iloc[idx]) else "æœªçŸ¥"
        
        # æ ¹æ®filter_stockså‚æ•°è¿‡æ»¤
        if filter_stocks:
            if (stock_code.startswith('300') or stock_code.startswith('301') or 
                stock_code.startswith('688') or stock_code.startswith('920')):
                continue
        
        stock_item = {
            'code': stock_code,
            'name': stock_name,
            'industry': stock_industry,
            'rank': idx + 1
        }
        
        # å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œæ·»åŠ æ›´å¤šå­—æ®µ
        if include_details:
            row = df.iloc[idx]
            stock_item['price_change'] = float(row['æ¶¨è·Œå¹…']) if 'æ¶¨è·Œå¹…' in df.columns and pd.notna(row['æ¶¨è·Œå¹…']) else 0.0
            stock_item['turnover_rate'] = float(row['æ¢æ‰‹ç‡%']) if 'æ¢æ‰‹ç‡%' in df.columns and pd.notna(row['æ¢æ‰‹ç‡%']) else 0.0
            stock_item['volume_days'] = float(row['æ”¾é‡å¤©æ•°']) if 'æ”¾é‡å¤©æ•°' in df.columns and pd.notna(row['æ”¾é‡å¤©æ•°']) else 0.0
            stock_item['avg_volume_ratio_50'] = float(row['å¹³å‡é‡æ¯”_50å¤©']) if 'å¹³å‡é‡æ¯”_50å¤©' in df.columns and pd.notna(row['å¹³å‡é‡æ¯”_50å¤©']) else 0.0
            stock_item['volatility'] = float(row['æ³¢åŠ¨ç‡']) if 'æ³¢åŠ¨ç‡' in df.columns and pd.notna(row['æ³¢åŠ¨ç‡']) else 0.0
        
        stocks_data.append(stock_item)
    
    return stocks_data

def analyze_stocks_period(directory, days, filter_stocks=True):
    """åˆ†æç‰¹å®šæ—¶é—´å‘¨æœŸçš„è‚¡ç¥¨é‡å¤å‡ºç°"""
    files_with_dates = get_excel_files(directory)
    
    if len(files_with_dates) == 0:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    
    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œç”¨äºç¼“å­˜éªŒè¯
    files_hash = get_files_hash(files_with_dates)
    cache_key = f"{days}_{filter_stocks}_{files_hash}"
    
    # æ£€æŸ¥ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with cache_lock:
        if cache_key in analysis_cache:
            print(f"âœ“ ä½¿ç”¨ç¼“å­˜: {cache_key}")
            return analysis_cache[cache_key]
    
    print(f"âš™ è®¡ç®—æ–°æ•°æ®: {cache_key}")
    
    # åŠ è½½æ‰€æœ‰æ—¥æœŸçš„è‚¡ç¥¨æ•°æ®
    all_stocks_data = {}
    stock_names = {}  # ä¿å­˜è‚¡ç¥¨ä»£ç å¯¹åº”çš„åç§°
    stock_industries = {}  # ä¿å­˜è‚¡ç¥¨ä»£ç å¯¹åº”çš„è¡Œä¸š
    for date, file in files_with_dates:
        stocks = load_stock_data(file, filter_stocks)
        all_stocks_data[date] = {stock['code']: stock['rank'] for stock in stocks}
        # ä¿å­˜è‚¡ç¥¨åç§°å’Œè¡Œä¸šï¼ˆæ€»æ˜¯æ›´æ–°ä¸ºæœ€æ–°çš„ï¼‰
        for stock in stocks:
            stock_names[stock['code']] = stock['name']
            stock_industries[stock['code']] = stock['industry']
    
    # è·å–æœ€æ–°æ—¥æœŸ
    latest_date = files_with_dates[0][0]
    
    # ç¡®å®šè¦åˆ†æçš„æ—¥æœŸèŒƒå›´
    dates_to_check = [date for date, _ in files_with_dates[:days]]
    
    # ç»Ÿè®¡æ¯ä¸ªè‚¡ç¥¨å‡ºç°çš„æ¬¡æ•°å’Œè¯¦æƒ…
    stock_appearances = defaultdict(lambda: {'dates': [], 'ranks': []})
    
    for date in dates_to_check:
        if date in all_stocks_data:
            for stock_code, rank in all_stocks_data[date].items():
                stock_appearances[stock_code]['dates'].append(date)
                stock_appearances[stock_code]['ranks'].append(rank)
    
    # æ‰¾å‡ºç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨
    result_list = []
    
    for stock_code, info in stock_appearances.items():
        appearances = len(info['dates'])
        
        # æ ¹æ®å¤©æ•°è¦æ±‚åˆ¤æ–­
        if days <= 3:
            # 2å¤©æˆ–3å¤©ï¼šå¿…é¡»æ°å¥½å‡ºç°è¯¥å¤©æ•°
            if appearances == days and latest_date in info['dates']:
                latest_rank = info['ranks'][info['dates'].index(latest_date)]
                date_rank_info = [{'date': date, 'rank': rank}
                                for date, rank in zip(info['dates'], info['ranks'])]
                
                result_list.append({
                    'stock_code': stock_code,
                    'stock_name': stock_names.get(stock_code, stock_code),
                    'industry': stock_industries.get(stock_code, 'æœªçŸ¥'),
                    'appearances': appearances,
                    'latest_rank': latest_rank,
                    'date_rank_info': date_rank_info
                })
        else:
            # 5å¤©ã€7å¤©ã€14å¤©ï¼šåœ¨æœ€æ–°æ—¥æœŸå‡ºç°ï¼Œä¸”è‡³å°‘å‡ºç°2æ¬¡
            if latest_date in info['dates'] and appearances >= 2:
                latest_rank = info['ranks'][info['dates'].index(latest_date)]
                date_rank_info = [{'date': date, 'rank': rank}
                                for date, rank in zip(info['dates'], info['ranks'])]
                
                result_list.append({
                    'stock_code': stock_code,
                    'stock_name': stock_names.get(stock_code, stock_code),
                    'industry': stock_industries.get(stock_code, 'æœªçŸ¥'),
                    'appearances': appearances,
                    'latest_rank': latest_rank,
                    'date_rank_info': date_rank_info
                })
    
    # æŒ‰æœ€æ–°æ’åæ’åº
    result_list.sort(key=lambda x: x['latest_rank'])
    
    result = {
        'period': f'{days}å¤©',
        'period_days': days,
        'analysis_dates': dates_to_check,
        'total_stocks': len(result_list),
        'stocks': result_list
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with cache_lock:
        analysis_cache[cache_key] = result
    print(f"âœ“ ç¼“å­˜å·²ä¿å­˜: {cache_key}, å…±{len(result_list)}ä¸ªè‚¡ç¥¨")
    
    return result

def query_stock_history(directory, stock_code):
    """æŸ¥è¯¢å•ä¸ªè‚¡ç¥¨çš„å†å²æ•°æ®"""
    files_with_dates = get_excel_files(directory)
    
    if len(files_with_dates) == 0:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    
    stock_history = []
    
    # éå†æ‰€æœ‰æ—¥æœŸçš„æ–‡ä»¶
    for date, file_path in files_with_dates:
        try:
            df = pd.read_excel(file_path)
            
            # æ‰¾åˆ°ä»£ç åˆ—
            code_column = None
            for col in df.columns:
                if 'ä»£ç ' in str(col) or 'code' in str(col).lower():
                    code_column = col
                    break
            
            if code_column is None:
                code_column = df.columns[1]
            
            # æŸ¥æ‰¾è¯¥è‚¡ç¥¨
            stock_row = df[df[code_column].astype(str).str.strip() == stock_code]
            
            if not stock_row.empty:
                idx = stock_row.index[0]
                row = stock_row.iloc[0]
                
                # è·å–åç§°å’Œè¡Œä¸š
                name_column = None
                industry_column = None
                for col in df.columns:
                    if 'åç§°' in str(col) or 'name' in str(col).lower():
                        name_column = col
                    if 'è¡Œä¸š' in str(col) or 'industry' in str(col).lower():
                        industry_column = col
                
                stock_name = str(row[name_column]).strip() if name_column and pd.notna(row[name_column]) else stock_code
                stock_industry = str(row[industry_column]).strip() if industry_column and pd.notna(row[industry_column]) else "æœªçŸ¥"
                
                # æ„å»ºè¯¦ç»†æ•°æ®
                stock_detail = {
                    'stock_code': stock_code,
                    'stock_name': stock_name,
                    'industry': stock_industry,
                    'price_change': float(row['æ¶¨è·Œå¹…']) if 'æ¶¨è·Œå¹…' in df.columns and pd.notna(row['æ¶¨è·Œå¹…']) else 0.0,
                    'turnover_rate': float(row['æ¢æ‰‹ç‡%']) if 'æ¢æ‰‹ç‡%' in df.columns and pd.notna(row['æ¢æ‰‹ç‡%']) else 0.0,
                    'volume_days': float(row['æ”¾é‡å¤©æ•°']) if 'æ”¾é‡å¤©æ•°' in df.columns and pd.notna(row['æ”¾é‡å¤©æ•°']) else 0.0,
                    'avg_volume_ratio_50': float(row['å¹³å‡é‡æ¯”_50å¤©']) if 'å¹³å‡é‡æ¯”_50å¤©' in df.columns and pd.notna(row['å¹³å‡é‡æ¯”_50å¤©']) else 0.0,
                    'volatility': float(row['æ³¢åŠ¨ç‡']) if 'æ³¢åŠ¨ç‡' in df.columns and pd.notna(row['æ³¢åŠ¨ç‡']) else 0.0,
                    'rank': idx + 1,
                    'date': date
                }
                
                stock_history.append(stock_detail)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
    
    if not stock_history:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°è‚¡ç¥¨ä»£ç  {stock_code} çš„æ•°æ®")
    
    return stock_history

@app.get("/")
async def root():
    return {"message": "è‚¡ç¥¨åˆ†æAPI", "version": "1.0.0"}

@app.post("/api/cache/clear")
async def clear_cache():
    """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
    global analysis_cache, file_hash_cache
    cache_count = len(analysis_cache)
    analysis_cache.clear()
    file_hash_cache.clear()
    return {"message": f"å·²æ¸…ç©º {cache_count} ä¸ªç¼“å­˜é¡¹", "status": "success"}

@app.get("/api/dates", response_model=AvailableDatesResponse)
async def get_available_dates():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®æ—¥æœŸ"""
    try:
        files_with_dates = get_excel_files(DATA_DIR)
        dates = [date for date, _ in files_with_dates]
        
        return {
            "dates": dates,
            "latest_date": dates[0] if dates else "",
            "total_files": len(dates)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analyze/{period}", response_model=AnalysisResponse)
async def analyze_period(
    period: int,
    board_type: str = "main"
):
    """
    åˆ†ææŒ‡å®šæ—¶é—´å‘¨æœŸçš„è‚¡ç¥¨é‡å¤å‡ºç°ï¼ˆå¼‚æ­¥å¹¶å‘æ”¯æŒï¼‰
    
    - period: æ—¶é—´å‘¨æœŸï¼ˆ2, 3, 5, 7, 14ï¼‰
    - board_type: æ¿å—ç±»å‹ï¼ˆmain=ä¸»æ¿, all=åŒ…å«åŒåˆ›ï¼‰
    """
    if period not in [2, 3, 5, 7, 14]:
        raise HTTPException(status_code=400, detail="periodå¿…é¡»æ˜¯2, 3, 5, 7, 14ä¹‹ä¸€")
    
    if board_type not in ["main", "all"]:
        raise HTTPException(status_code=400, detail="board_typeå¿…é¡»æ˜¯mainæˆ–all")
    
    filter_stocks = (board_type == "main")
    
    try:
        # åœ¨çº¿ç¨‹æ± ä¸­å¼‚æ­¥æ‰§è¡Œï¼Œé¿å…é˜»å¡å…¶ä»–è¯·æ±‚
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            executor, 
            analyze_stocks_period, 
            DATA_DIR, 
            period, 
            filter_stocks
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analyze/all/{board_type}")
async def analyze_all_periods(board_type: str = "main"):
    """
    è·å–æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„åˆ†æç»“æœ
    
    - board_type: æ¿å—ç±»å‹ï¼ˆmain=ä¸»æ¿, all=åŒ…å«åŒåˆ›ï¼‰
    """
    if board_type not in ["main", "all"]:
        raise HTTPException(status_code=400, detail="board_typeå¿…é¡»æ˜¯mainæˆ–all")
    
    filter_stocks = (board_type == "main")
    periods = [2, 3, 5, 7, 14]
    results = {}
    
    try:
        for period in periods:
            result = analyze_stocks_period(DATA_DIR, period, filter_stocks)
            results[f"{period}å¤©"] = result
        
        return {
            "board_type": "ä¸»æ¿" if board_type == "main" else "å…¨éƒ¨ï¼ˆå«åŒåˆ›ï¼‰",
            "periods": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/stock/{stock_code}", response_model=List[StockDetail])
async def get_stock_history(stock_code: str):
    """
    æŸ¥è¯¢å•ä¸ªè‚¡ç¥¨çš„å†å²æ•°æ®å’Œæ’åå˜åŒ–
    
    - stock_code: è‚¡ç¥¨ä»£ç 
    """
    try:
        # åœ¨çº¿ç¨‹æ± ä¸­å¼‚æ­¥æ‰§è¡Œ
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            executor,
            query_stock_history,
            DATA_DIR,
            stock_code
        )
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def load_top_n_stocks(file_path, top_n=1000):
    """ä¸“é—¨ç”¨äºè¯»å–å‰Nåçš„æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼ˆä¸è¿‡æ»¤ï¼Œç”¨äºè¡Œä¸šç»Ÿè®¡ï¼‰"""
    df = pd.read_excel(file_path)
    
    # æ‰¾åˆ°å…³é”®åˆ—
    code_column = None
    name_column = None
    industry_column = None
    for col in df.columns:
        if 'ä»£ç ' in str(col) or 'code' in str(col).lower():
            code_column = col
        if 'åç§°' in str(col) or 'name' in str(col).lower():
            name_column = col
        if 'è¡Œä¸š' in str(col) or 'industry' in str(col).lower():
            industry_column = col
    
    if code_column is None:
        code_column = df.columns[1]
    if name_column is None:
        name_column = df.columns[2]
    
    # è¯»å–å‰Nåè‚¡ç¥¨
    stocks_data = []
    for idx in range(min(top_n, len(df))):
        stock_code = str(df[code_column].iloc[idx]).strip()
        stock_industry = str(df[industry_column].iloc[idx]).strip() if industry_column and pd.notna(df[industry_column].iloc[idx]) else "æœªçŸ¥"
        
        stocks_data.append({
            'code': stock_code,
            'industry': stock_industry,
            'rank': idx + 1
        })
    
    return stocks_data

def get_top1000_industry_stats(directory):
    """è·å–ä»Šæ—¥å‰1000åçš„è¡Œä¸šåˆ†å¸ƒç»Ÿè®¡"""
    cache_key = "top1000_industry_stats"
    
    # æ£€æŸ¥ç¼“å­˜
    with cache_lock:
        if cache_key in analysis_cache:
            print(f"âœ“ ä½¿ç”¨ç¼“å­˜: {cache_key}")
            return analysis_cache[cache_key]
    
    print(f"âš™ è®¡ç®—å‰1000åè¡Œä¸šç»Ÿè®¡: {cache_key}")
    files_with_dates = get_excel_files(directory)
    if len(files_with_dates) == 0:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    
    # è·å–æœ€æ–°æ—¥æœŸçš„æ–‡ä»¶
    latest_date, latest_file = files_with_dates[0]
    
    # ä½¿ç”¨ä¸“é—¨çš„å‡½æ•°åŠ è½½å‰1000åæ•°æ®ï¼ˆä¸è¿‡æ»¤ï¼‰
    top_1000 = load_top_n_stocks(latest_file, top_n=1000)
    
    # ç»Ÿè®¡è¡Œä¸š
    industry_count = defaultdict(int)
    for stock in top_1000:
        industry = stock.get('industry', 'æœªçŸ¥')
        industry_count[industry] += 1
    
    total = len(top_1000)
    stats = [
        {
            'industry': industry,
            'count': count,
            'percentage': round(count / total * 100, 2)
        }
        for industry, count in sorted(industry_count.items(), key=lambda x: x[1], reverse=True)
    ]
    
    result = {
        'date': latest_date,
        'total_stocks': total,
        'stats': stats
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜
    with cache_lock:
        analysis_cache[cache_key] = result
    print(f"âœ“ ç¼“å­˜å·²ä¿å­˜: {cache_key}, {len(stats)}ä¸ªè¡Œä¸š")
    
    return result

def get_industry_trend_analysis(directory):
    """è·å–æ‰€æœ‰æ—¥æœŸçš„è¡Œä¸šåˆ†å¸ƒè¶‹åŠ¿"""
    cache_key = "industry_trend_all"
    
    # æ£€æŸ¥ç¼“å­˜
    with cache_lock:
        if cache_key in analysis_cache:
            print(f"âœ“ ä½¿ç”¨ç¼“å­˜: {cache_key}")
            return analysis_cache[cache_key]
    
    print(f"âš™ è®¡ç®—è¡Œä¸šè¶‹åŠ¿æ•°æ®: {cache_key}")
    files_with_dates = get_excel_files(directory)
    
    if len(files_with_dates) == 0:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
    
    trend_data = []
    all_industries = set()
    
    # éå†æ‰€æœ‰æ–‡ä»¶ï¼Œè·å–æ¯å¤©çš„è¡Œä¸šåˆ†å¸ƒï¼ˆå‰1000åï¼‰
    for date, file_path in files_with_dates:
        try:
            # ä½¿ç”¨ä¸“é—¨çš„å‡½æ•°åŠ è½½å‰1000åæ•°æ®
            top_1000 = load_top_n_stocks(file_path, top_n=1000)
            
            industry_count = defaultdict(int)
            for stock in top_1000:
                industry = stock.get('industry', 'æœªçŸ¥')
                industry_count[industry] += 1
                all_industries.add(industry)
            
            trend_data.append({
                'date': date,
                'industry_counts': dict(industry_count)
            })
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
    trend_data.sort(key=lambda x: x['date'])
    
    result = {
        'dates': [item['date'] for item in trend_data],
        'industries': sorted(list(all_industries)),
        'data': trend_data
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜
    with cache_lock:
        analysis_cache[cache_key] = result
    print(f"âœ“ ç¼“å­˜å·²ä¿å­˜: {cache_key}, {len(trend_data)}ä¸ªæ—¥æœŸ, {len(all_industries)}ä¸ªè¡Œä¸š")
    
    return result

@app.get("/api/industry/top1000")
async def get_top1000_industry():
    """
    è·å–ä»Šæ—¥å‰1000åçš„è¡Œä¸šåˆ†å¸ƒç»Ÿè®¡
    """
    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            executor,
            get_top1000_industry_stats,
            DATA_DIR
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/industry/trend")
async def get_industry_trend():
    """
    è·å–æ‰€æœ‰æ—¥æœŸçš„è¡Œä¸šåˆ†å¸ƒè¶‹åŠ¿ï¼ˆå‰1000åï¼‰
    """
    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            executor,
            get_industry_trend_analysis,
            DATA_DIR
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½ç¼“å­˜ï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰"""
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print("âš™ï¸  å¼€å§‹å¹¶è¡Œé¢„åŠ è½½ç¼“å­˜...")
    
    # åœ¨åå°çº¿ç¨‹ä¸­é¢„åŠ è½½ï¼Œé¿å…é˜»å¡å¯åŠ¨
    def preload_cache():
        try:
            import concurrent.futures
            from concurrent.futures import ThreadPoolExecutor as PreloadExecutor
            
            # åˆ›å»ºä¸“é—¨ç”¨äºé¢„åŠ è½½çš„çº¿ç¨‹æ± 
            with PreloadExecutor(max_workers=8) as preload_pool:
                futures = []
                
                # é¢„åŠ è½½å¸¸ç”¨çš„åˆ†æå‘¨æœŸï¼ˆå¹¶è¡Œï¼‰
                periods = [2, 3, 5]
                board_types = ['main', 'all']
                
                for period in periods:
                    for board_type in board_types:
                        filter_stocks = (board_type == 'main')
                        future = preload_pool.submit(
                            analyze_stocks_period,
                            DATA_DIR,
                            period,
                            filter_stocks
                        )
                        futures.append((future, f"{period}å¤© ({board_type})"))
                
                # é¢„åŠ è½½è¡Œä¸šæ•°æ®ï¼ˆå¹¶è¡Œï¼‰
                industry_top1000_future = preload_pool.submit(get_top1000_industry_stats, DATA_DIR)
                futures.append((industry_top1000_future, "ä»Šæ—¥å‰1000åè¡Œä¸šç»Ÿè®¡"))
                
                industry_trend_future = preload_pool.submit(get_industry_trend_analysis, DATA_DIR)
                futures.append((industry_trend_future, "è¡Œä¸šè¶‹åŠ¿åˆ†æ"))
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ˜¾ç¤ºç»“æœ
                for future, name in futures:
                    try:
                        future.result()  # ç­‰å¾…å®Œæˆ
                        print(f"  âœ“ é¢„åŠ è½½å®Œæˆ: {name}")
                    except Exception as e:
                        print(f"  âœ— é¢„åŠ è½½å¤±è´¥: {name} - {e}")
            
            print("âœ… ç¼“å­˜é¢„åŠ è½½å…¨éƒ¨å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ ç¼“å­˜é¢„åŠ è½½å‡ºé”™: {e}")
    
    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé¢„åŠ è½½
    loop = asyncio.get_event_loop()
    loop.run_in_executor(executor, preload_cache)

if __name__ == "__main__":
    import uvicorn
    # ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ç½®reload=Falseä»¥ä¿æŒç¼“å­˜
    print("=" * 60)
    print("è‚¡ç¥¨åˆ†æç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
