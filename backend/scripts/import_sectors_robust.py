"""
æ¿å—æ•°æ®å¥å£®å¯¼å…¥è„šæœ¬
ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨å®ç°åŸå­æ€§ã€å¹‚ç­‰æ€§å’Œå®Œæ•´çš„å›æ»šæœºåˆ¶

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ã€å¹‚ç­‰æ€§ã€‘åŸºäºçŠ¶æ€æ–‡ä»¶åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¼å…¥
2. ã€åŸå­æ€§ã€‘æ•´ä¸ªæ–‡ä»¶åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­ï¼Œå¤±è´¥è‡ªåŠ¨å›æ»š
3. ã€çŠ¶æ€ç®¡ç†ã€‘æœ¬åœ°JSONæ–‡ä»¶è®°å½•æ‰€æœ‰å¯¼å…¥çŠ¶æ€
4. ã€æ–‡ä»¶æ ¡éªŒã€‘MD5å“ˆå¸Œæ£€æµ‹æ–‡ä»¶å˜åŒ–
5. ã€é”™è¯¯æ¢å¤ã€‘å¤±è´¥åå¯å®‰å…¨é‡è¯•
6. ã€æ— ä¾µå…¥æ€§ã€‘ä¸ä¿®æ”¹åŸå§‹Excelå’Œæ•°æ®åº“ç»“æ„

ä¸è‚¡ç¥¨å¯¼å…¥çš„åŒºåˆ«ï¼š
- ä½¿ç”¨ Sector å’Œ SectorDailyData æ¨¡å‹
- æ¿å—åç§°ï¼ˆä»£ç åˆ—ï¼‰ä½œä¸ºå”¯ä¸€æ ‡è¯†
- ä¸åŒ…å« jump å’Œ å¸‚å€¼ å­—æ®µ
"""
import sys
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import logging
import time
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from app.database import SessionLocal, test_connection
from app.db_models import Sector, SectorDailyData
from app.config import DATA_DIR
from import_state_manager import ImportStateManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Excelåˆ—ååˆ°æ•°æ®åº“å­—æ®µçš„æ˜ å°„ï¼ˆæ¿å—æ•°æ®ä¸åŒ…å«jumpå’Œå¸‚å€¼ï¼‰
SECTOR_COLUMN_MAPPING = {
    'æ€»åˆ†': 'total_score',
    'å¼€ç›˜': 'open_price',
    'æœ€é«˜': 'high_price',
    'æœ€ä½': 'low_price',
    'close': 'close_price',
    # æ³¨æ„ï¼šæ¿å—æ•°æ®æ²¡æœ‰ 'jump' å’Œ 'æ€»å¸‚å€¼(äº¿)'
    'æ¶¨è·Œå¹…': 'price_change',
    'æ¢æ‰‹ç‡%': 'turnover_rate_percent',
    'æ”¾é‡å¤©æ•°': 'volume_days',
    'å¹³å‡é‡æ¯”_50å¤©': 'avg_volume_ratio_50',
    'æˆäº¤é‡': 'volume',
    'æ”¾é‡å¤©æ•°_volume': 'volume_days_volume',
    'å¹³å‡é‡æ¯”_50å¤©_volume': 'avg_volume_ratio_50_volume',
    'æ³¢åŠ¨ç‡': 'volatility',
    'volatile_consec': 'volatile_consec',
    'BETA': 'beta',
    'BETA_consec': 'beta_consec',
    'ç›¸å…³æ€§': 'correlation',
    'é•¿æœŸ': 'long_term',
    'çŸ­æœŸ': 'short_term',
    'è¶…ä¹°': 'overbought',
    'è¶…å–': 'oversold',
    'macd_signal': 'macd_signal',
    'slowkdj_signal': 'slowkdj_signal',
    'lon_lonma': 'lon_lonma',
    'lon_consec': 'lon_consec',
    'lon_0': 'lon_0',
    'loncons_consec': 'loncons_consec',
    'lonma_0': 'lonma_0',
    'lonmacons_consec': 'lonmacons_consec',
    'dma': 'dma',
    'dma_consec': 'dma_consec',
    'dif_dem': 'dif_dem',
    'macd_consec': 'macd_consec',
    'dif_0': 'dif_0',
    'macdcons_consec': 'macdcons_consec',
    'dem_0': 'dem_0',
    'demcons_consec': 'demcons_consec',
    'pdi_adx': 'pdi_adx',
    'dmiadx_consec': 'dmiadx_consec',
    'pdi_ndi': 'pdi_ndi',
    'dmi_consec': 'dmi_consec',
    'obv': 'obv',
    'obv_consec': 'obv_consec',
    'k_kdj': 'k_kdj',
    'slowkdj_consec': 'slowkdj_consec',
    'rsi': 'rsi',
    'rsi_consec': 'rsi_consec',
    'cci_-90': 'cci_neg_90',
    'cci_lower_consec': 'cci_lower_consec',
    'cci_90': 'cci_pos_90',
    'cci_upper_consec': 'cci_upper_consec',
    'bands_lower': 'bands_lower',
    'bands_lower_consec': 'bands_lower_consec',
    'bands_middle': 'bands_middle',
    'bands_middle_consec': 'bands_middle_consec',
    'bands_upper': 'bands_upper',
    'bands_upper_consec': 'bands_upper_consec',
    'lon_lonma_diff': 'lon_lonma_diff',
    'lon': 'lon',
    'lonma': 'lonma',
    'histgram': 'histgram',
    'dif': 'dif',
    'dem': 'dem',
    'ADX': 'adx',
    'PLUS_DI': 'plus_di',
    'OBV': 'obv_2',
    'slowk': 'slowk',
    'RSI': 'rsi_2',
    'CCI_-90': 'cci_neg_90_2',
    'CCI_90': 'cci_pos_90_2',
    'lower': 'lower_band',
    'middle': 'middle_band',
    'upper': 'upper_band',
    'lst_close': 'lst_close',
    'code2': 'code2',
    'name2': 'name2',
    'zhangdiefu2': 'zhangdiefu2',
    'volume_consec2': 'volume_consec2',
    'volume_50_consec2': 'volume_50_consec2'
}


def extract_date_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åæå–æ—¥æœŸå­—ç¬¦ä¸²
    æ ¼å¼: YYYYMMDD_allbk_sma_feature_color.xlsx
    è¿”å›: '20251105'
    """
    try:
        return filename.split('_')[0]
    except Exception as e:
        logger.error(f"æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸ: {filename}, é”™è¯¯: {str(e)}")
        return None


def import_sector_excel_file(file_path: Path, state_manager: ImportStateManager) -> tuple:
    """
    å¯¼å…¥å•ä¸ªæ¿å—Excelæ–‡ä»¶
    
    äº‹åŠ¡è®¾è®¡ï¼š
    - æ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆäº‹åŠ¡ï¼‰
    - æ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­
    - æˆåŠŸåˆ™commitï¼Œå¤±è´¥åˆ™rollback
    
    Returns:
        (imported_count, skipped_count, success)
    """
    filename = file_path.name
    date_str = extract_date_from_filename(filename)
    
    if not date_str:
        logger.warning(f"[è·³è¿‡] æ–‡ä»¶ï¼ˆæ— æ³•æå–æ—¥æœŸï¼‰: {filename}")
        return 0, 0, False
    
    # === å¹‚ç­‰æ€§æ£€æŸ¥ï¼šåŸºäºçŠ¶æ€æ–‡ä»¶ ===
    if not state_manager.should_reimport(date_str, file_path):
        logger.info(f"[è·³è¿‡] æ–‡ä»¶ï¼ˆå·²æˆåŠŸå¯¼å…¥ï¼‰: {filename}")
        # æ–‡ä»¶å·²å¯¼å…¥ï¼Œè®¡å…¥è·³è¿‡ç»Ÿè®¡ï¼ˆå¯¼å…¥=0ï¼Œè·³è¿‡=1ï¼‰
        return 0, 1, True
    
    # è®°å½•å¼€å§‹å¯¼å…¥
    state_manager.start_import(date_str, filename, file_path)
    start_time = time.time()
    
    # === åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆäº‹åŠ¡è¾¹ç•Œï¼‰ ===
    db_session = SessionLocal()
    
    try:
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        target_date = datetime.strptime(date_str, '%Y%m%d').date()
        logger.info(f"ğŸ“‚ æ­£åœ¨å¯¼å…¥æ¿å—æ•°æ®: {filename} (æ—¥æœŸ: {target_date})")
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path)
        total_rows = len(df)
        logger.info(f"ğŸ“Š è¯»å–åˆ° {total_rows} ä¸ªæ¿å—è®°å½•")
        
        # === æ£€æŸ¥é‡å¤çš„æ¿å—åç§° ===
        df['ä»£ç _stripped'] = df['ä»£ç '].astype(str).str.strip()
        duplicates = df[df.duplicated(subset=['ä»£ç _stripped'], keep=False)]
        if not duplicates.empty:
            dup_names = duplicates['ä»£ç _stripped'].unique()
            logger.error(f"âŒ Excelæ–‡ä»¶ä¸­å­˜åœ¨é‡å¤çš„æ¿å—åç§°: {', '.join(dup_names)}")
            logger.error(f"   é‡å¤è®°å½•æ•°: {len(duplicates)}")
            for name in dup_names:
                dup_rows = df[df['ä»£ç _stripped'] == name]
                logger.error(f"   æ¿å— {name} å‡ºç°äº† {len(dup_rows)} æ¬¡ï¼Œåœ¨è¡Œ: {list(dup_rows.index + 2)}")  # +2å› ä¸ºExcelä»1å¼€å§‹ä¸”æœ‰è¡¨å¤´
            
            error_msg = f"Excelæ–‡ä»¶åŒ…å«é‡å¤çš„æ¿å—åç§°: {', '.join(dup_names)}"
            state_manager.mark_failed(date_str, error_msg, 0)
            raise ValueError(error_msg)
        
        imported_count = 0
        skipped_count = 0
        
        # === æ‰¹é‡å¯¼å…¥ï¼ˆåœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­ï¼‰ ===
        for idx, row in df.iterrows():
            try:
                # 1. å¤„ç†æ¿å—åç§°ï¼ˆä»"ä»£ç "åˆ—è·å–ï¼‰
                sector_name = str(row['ä»£ç ']).strip()
                
                # è·³è¿‡æ— æ•ˆæ•°æ®
                if not sector_name or sector_name == 'nan':
                    logger.warning(f"  è·³è¿‡æ— æ•ˆæ¿å—åç§°: è¡Œ {idx + 1}")
                    skipped_count += 1
                    continue
                
                rank = idx + 1
                
                # 2. ç¡®ä¿æ¿å—è®°å½•å­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
                sector = db_session.query(Sector).filter(
                    Sector.sector_name == sector_name
                ).first()
                
                if not sector:
                    sector = Sector(sector_name=sector_name)
                    db_session.add(sector)
                    db_session.flush()  # ç«‹å³è·å– sector.id
                
                # 3. åˆ›å»ºæ¯æ—¥æ•°æ®è®°å½•
                daily_data = SectorDailyData(
                    sector_id=sector.id,
                    date=target_date,
                    rank=rank
                )
                
                # 4. æ˜ å°„æ‰€æœ‰Excelåˆ—åˆ°æ•°æ®åº“å­—æ®µ
                for excel_col, db_col in SECTOR_COLUMN_MAPPING.items():
                    if excel_col in df.columns:
                        value = row[excel_col]
                        if pd.isna(value):
                            value = None
                        setattr(daily_data, db_col, value)
                
                db_session.add(daily_data)
                imported_count += 1
                
                # æ¯100æ¡æ˜¾ç¤ºè¿›åº¦ï¼ˆä½†ä¸æäº¤ï¼‰
                if imported_count % 100 == 0:
                    logger.info(f"  è¿›åº¦: {imported_count}/{total_rows} ({imported_count/total_rows*100:.1f}%)")
                
            except IntegrityError as e:
                # å”¯ä¸€ç´¢å¼•å†²çªï¼šè¯¥æ¿å—è¯¥æ—¥æœŸå·²å­˜åœ¨
                db_session.rollback()
                logger.warning(f"  è·³è¿‡é‡å¤æ•°æ®: {sector_name} - {target_date}")
                skipped_count += 1
                # é‡æ–°å¼€å§‹å½“å‰è¡Œçš„å°äº‹åŠ¡
                db_session = SessionLocal()
                continue
            
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼šç«‹å³å›æ»šå¹¶æŠ›å‡º
                db_session.rollback()
                error_msg = f"å¯¼å…¥è®°å½•å¤±è´¥: {sector_name}, é”™è¯¯: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                state_manager.mark_failed(date_str, error_msg, imported_count)
                raise
        
        # === å…³é”®ï¼šæ•´ä¸ªæ–‡ä»¶æˆåŠŸåæ‰æäº¤äº‹åŠ¡ ===
        db_session.commit()
        duration = time.time() - start_time
        
        logger.info(f"[å®Œæˆ] æ–‡ä»¶å¯¼å…¥å®Œæˆ: {filename}")
        logger.info(f"   å¯¼å…¥: {imported_count} æ¡, è·³è¿‡: {skipped_count} æ¡, è€—æ—¶: {duration:.1f}ç§’")
        
        # æ›´æ–°çŠ¶æ€æ–‡ä»¶
        state_manager.mark_success(date_str, imported_count, skipped_count, duration)
        
        return imported_count, skipped_count, True
        
    except Exception as e:
        # === ä»»ä½•é”™è¯¯éƒ½å›æ»šæ•´ä¸ªäº‹åŠ¡ ===
        db_session.rollback()
        error_msg = f"æ–‡ä»¶å¯¼å…¥å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è®°å½•å›æ»šçŠ¶æ€
        state_manager.mark_rolled_back(date_str, error_msg)
        
        return 0, 0, False
        
    finally:
        db_session.close()


def get_sector_data_files(sector_data_dir: str):
    """è·å–æ‰€æœ‰å¾…å¯¼å…¥çš„æ¿å—Excelæ–‡ä»¶"""
    data_dir = Path(sector_data_dir)
    if not data_dir.exists():
        logger.error(f"æ¿å—æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    # æ‰«ææ‰€æœ‰æ¿å—æ•°æ®æ–‡ä»¶ï¼ˆæ ¼å¼ï¼šYYYYMMDD_allbk_sma_feature_color.xlsxï¼‰
    pattern = "*_allbk_sma_feature_color.xlsx"
    files = list(data_dir.glob(pattern))
    
    logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ¿å—æ•°æ®æ–‡ä»¶")
    return sorted(files)


def main():
    """ä¸»å‡½æ•°ï¼šæ‰¹é‡å¯¼å…¥æ‰€æœ‰æ¿å—Excelæ–‡ä»¶"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ¿å—æ•°æ®å¯¼å…¥ä»»åŠ¡ï¼ˆå¥å£®ç‰ˆï¼‰")
    logger.info("=" * 60)
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_connection():
        logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # è·å–çŠ¶æ€ç®¡ç†å™¨ï¼ˆæ¿å—ä¸“ç”¨çŠ¶æ€æ–‡ä»¶ï¼‰
    state_manager = ImportStateManager(state_file="sector_import_state.json")
    
    # è·å–æ¿å—æ•°æ®ç›®å½•ï¼ˆå‡è®¾ä¸è‚¡ç¥¨æ•°æ®åœ¨åŒä¸€ç›®å½•ï¼‰
    sector_data_dir = DATA_DIR
    
    # è·å–å¾…å¯¼å…¥æ–‡ä»¶
    files = get_sector_data_files(sector_data_dir)
    if not files:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°å¾…å¯¼å…¥çš„æ¿å—æ•°æ®æ–‡ä»¶")
        logger.info("æç¤ºï¼šæ¿å—æ•°æ®æ–‡ä»¶æ ¼å¼åº”ä¸º YYYYMMDD_allbk_sma_feature_color.xlsx")
        return
    
    # ç»Ÿè®¡
    total_imported = 0
    total_skipped = 0
    success_files = 0
    failed_files = 0
    
    # éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in files:
        imported, skipped, success = import_sector_excel_file(file_path, state_manager)
        total_imported += imported
        total_skipped += skipped
        
        if success:
            success_files += 1
        else:
            failed_files += 1
    
    # æ‰“å°æ€»ç»“
    logger.info("=" * 60)
    logger.info("[æˆåŠŸ] æ¿å—æ•°æ®å¯¼å…¥ä»»åŠ¡å®Œæˆï¼")
    logger.info(f"æ–‡ä»¶ç»Ÿè®¡: æˆåŠŸ={success_files}, å¤±è´¥={failed_files}")
    logger.info(f"æ•°æ®ç»Ÿè®¡: å¯¼å…¥={total_imported}, è·³è¿‡={total_skipped}")
    logger.info("=" * 60)
    
    # æ‰“å°çŠ¶æ€æ‘˜è¦
    state_manager.print_summary()


if __name__ == "__main__":
    main()
