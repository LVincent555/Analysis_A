"""
å¥å£®çš„æ•°æ®å¯¼å…¥è„šæœ¬
ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨å®ç°åŸå­æ€§ã€å¹‚ç­‰æ€§å’Œå®Œæ•´çš„å›æ»šæœºåˆ¶

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ã€å¹‚ç­‰æ€§ã€‘åŸºäºçŠ¶æ€æ–‡ä»¶åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¼å…¥
2. ã€åŸå­æ€§ã€‘æ•´ä¸ªæ–‡ä»¶åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­ï¼Œå¤±è´¥è‡ªåŠ¨å›æ»š
3. ã€çŠ¶æ€ç®¡ç†ã€‘æœ¬åœ°JSONæ–‡ä»¶è®°å½•æ‰€æœ‰å¯¼å…¥çŠ¶æ€
4. ã€æ–‡ä»¶æ ¡éªŒã€‘MD5å“ˆå¸Œæ£€æµ‹æ–‡ä»¶å˜åŒ–
5. ã€é”™è¯¯æ¢å¤ã€‘å¤±è´¥åå¯å®‰å…¨é‡è¯•
6. ã€æ— ä¾µå…¥æ€§ã€‘ä¸ä¿®æ”¹åŸå§‹Excelå’Œæ•°æ®åº“ç»“æ„
"""
import sys
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import logging
import time
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from app.database import SessionLocal, test_connection
from app.db_models import Stock, DailyStockData
from app.config import DATA_DIR, FILE_PATTERN
from import_state_manager import get_state_manager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Excelåˆ—ååˆ°æ•°æ®åº“å­—æ®µçš„æ˜ å°„
COLUMN_MAPPING = {
    'æ€»åˆ†': 'total_score',
    'å¼€ç›˜': 'open_price',
    'æœ€é«˜': 'high_price',
    'æœ€ä½': 'low_price',
    'close': 'close_price',
    'jump': 'jump',
    'æ¶¨è·Œå¹…': 'price_change',
    'æ¢æ‰‹ç‡%': 'turnover_rate_percent',
    'æ”¾é‡å¤©æ•°': 'volume_days',
    'å¹³å‡é‡æ¯”_50å¤©': 'avg_volume_ratio_50',
    'æˆäº¤é‡': 'volume',
    'æ”¾é‡å¤©æ•°_volume': 'volume_days_volume',
    'å¹³å‡é‡æ¯”_50å¤©_volume': 'avg_volume_ratio_50_volume',
    'æ³¢åŠ¨ç‡': 'volatility',
    'volatile_consec': 'volatile_consec',
    'BETA': 'beta',
    'BETA_consec': 'beta_consec',
    'ç›¸å…³æ€§': 'correlation',
    'æ€»å¸‚å€¼(äº¿)': 'market_cap_billions',
    'é•¿æœŸ': 'long_term',
    'çŸ­æœŸ': 'short_term',
    'è¶…ä¹°': 'overbought',
    'è¶…å–': 'oversold',
    'macd_signal': 'macd_signal',
    'slowkdj_signal': 'slowkdj_signal',
    'lon_lonma': 'lon_lonma',
    'lon_consec': 'lon_consec',
    'lon_0': 'lon_0',
    'loncons_consec': 'loncons_consec',
    'lonma_0': 'lonma_0',
    'lonmacons_consec': 'lonmacons_consec',
    'dma': 'dma',
    'dma_consec': 'dma_consec',
    'dif_dem': 'dif_dem',
    'macd_consec': 'macd_consec',
    'dif_0': 'dif_0',
    'macdcons_consec': 'macdcons_consec',
    'dem_0': 'dem_0',
    'demcons_consec': 'demcons_consec',
    'pdi_adx': 'pdi_adx',
    'dmiadx_consec': 'dmiadx_consec',
    'pdi_ndi': 'pdi_ndi',
    'dmi_consec': 'dmi_consec',
    'obv': 'obv',
    'obv_consec': 'obv_consec',
    'k_kdj': 'k_kdj',
    'slowkdj_consec': 'slowkdj_consec',
    'rsi': 'rsi',
    'rsi_consec': 'rsi_consec',
    'cci_-90': 'cci_neg_90',
    'cci_lower_consec': 'cci_lower_consec',
    'cci_90': 'cci_pos_90',
    'cci_upper_consec': 'cci_upper_consec',
    'bands_lower': 'bands_lower',
    'bands_lower_consec': 'bands_lower_consec',
    'bands_middle': 'bands_middle',
    'bands_middle_consec': 'bands_middle_consec',
    'bands_upper': 'bands_upper',
    'bands_upper_consec': 'bands_upper_consec',
    'lon_lonma_diff': 'lon_lonma_diff',
    'lon': 'lon',
    'lonma': 'lonma',
    'histgram': 'histgram',
    'dif': 'dif',
    'dem': 'dem',
    'ADX': 'adx',
    'PLUS_DI': 'plus_di',
    'OBV': 'obv_2',
    'slowk': 'slowk',
    'RSI': 'rsi_2',
    'CCI_-90': 'cci_neg_90_2',
    'CCI_90': 'cci_pos_90_2',
    'lower': 'lower_band',
    'middle': 'middle_band',
    'upper': 'upper_band',
    'lst_close': 'lst_close',
    'code2': 'code2',
    'name2': 'name2',
    'zhangdiefu2': 'zhangdiefu2',
    'volume_consec2': 'volume_consec2',
    'volume_50_consec2': 'volume_50_consec2'
}


def extract_date_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åæå–æ—¥æœŸå­—ç¬¦ä¸²
    æ ¼å¼: YYYYMMDD_data_sma_feature_color.xlsx
    è¿”å›: '20251103'
    """
    try:
        return filename.split('_')[0]
    except Exception as e:
        logger.error(f"æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸ: {filename}, é”™è¯¯: {str(e)}")
        return None


def normalize_stock_code(code_str: str) -> str:
    """
    è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ï¼šè¡¥å…¨å‰å¯¼é›¶åˆ°6ä½
    """
    code = str(code_str).strip()
    if code.isdigit() and len(code) < 6:
        code = code.zfill(6)
    return code


def import_excel_file(file_path: Path, state_manager) -> tuple:
    """
    å¯¼å…¥å•ä¸ªExcelæ–‡ä»¶
    
    äº‹åŠ¡è®¾è®¡ï¼š
    - æ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆäº‹åŠ¡ï¼‰
    - æ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­
    - æˆåŠŸåˆ™commitï¼Œå¤±è´¥åˆ™rollback
    
    Returns:
        (imported_count, skipped_count, success)
    """
    filename = file_path.name
    date_str = extract_date_from_filename(filename)
    
    if not date_str:
        logger.warning(f"â­ï¸  è·³è¿‡æ–‡ä»¶ï¼ˆæ— æ³•æå–æ—¥æœŸï¼‰: {filename}")
        return 0, 0, False
    
    # === å¹‚ç­‰æ€§æ£€æŸ¥ï¼šåŸºäºçŠ¶æ€æ–‡ä»¶ ===
    if not state_manager.should_reimport(date_str, file_path):
        logger.info(f"â­ï¸  è·³è¿‡æ–‡ä»¶ï¼ˆå·²æˆåŠŸå¯¼å…¥ï¼‰: {filename}")
        import_info = state_manager.get_import_info(date_str)
        return import_info.get('imported_count', 0), 0, True
    
    # è®°å½•å¼€å§‹å¯¼å…¥
    state_manager.start_import(date_str, filename, file_path)
    start_time = time.time()
    
    # === åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆäº‹åŠ¡è¾¹ç•Œï¼‰ ===
    db_session = SessionLocal()
    
    try:
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        target_date = datetime.strptime(date_str, '%Y%m%d').date()
        logger.info(f"ğŸ“‚ æ­£åœ¨å¯¼å…¥: {filename} (æ—¥æœŸ: {target_date})")
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path)
        total_rows = len(df)
        logger.info(f"ğŸ“Š è¯»å–åˆ° {total_rows} æ¡è®°å½•")
        
        imported_count = 0
        skipped_count = 0
        
        # === æ‰¹é‡å¯¼å…¥ï¼ˆåœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­ï¼‰ ===
        for idx, row in df.iterrows():
            try:
                # 1. å¤„ç†è‚¡ç¥¨ä»£ç 
                stock_code = normalize_stock_code(row['ä»£ç '])
                stock_name = str(row['åç§°']).strip()
                industry = str(row['è¡Œä¸š']).strip() if pd.notna(row['è¡Œä¸š']) else None
                rank = idx + 1
                
                # 2. ç¡®ä¿è‚¡ç¥¨è®°å½•å­˜åœ¨
                stock = db_session.query(Stock).filter(
                    Stock.stock_code == stock_code
                ).first()
                
                if not stock:
                    stock = Stock(
                        stock_code=stock_code,
                        stock_name=stock_name,
                        industry=industry,
                        last_updated=datetime.now()
                    )
                    db_session.add(stock)
                    # ä¸è¦ç«‹å³flushï¼Œç­‰å¾…æ•´ä½“æäº¤
                
                # 3. åˆ›å»ºæ¯æ—¥æ•°æ®è®°å½•
                daily_data = DailyStockData(
                    stock_code=stock_code,
                    date=target_date,
                    rank=rank
                )
                
                # 4. æ˜ å°„æ‰€æœ‰Excelåˆ—åˆ°æ•°æ®åº“å­—æ®µ
                for excel_col, db_col in COLUMN_MAPPING.items():
                    if excel_col in df.columns:
                        value = row[excel_col]
                        if pd.isna(value):
                            value = None
                        setattr(daily_data, db_col, value)
                
                db_session.add(daily_data)
                imported_count += 1
                
                # æ¯1000æ¡æ˜¾ç¤ºè¿›åº¦ï¼ˆä½†ä¸æäº¤ï¼‰
                if imported_count % 1000 == 0:
                    logger.info(f"  è¿›åº¦: {imported_count}/{total_rows} ({imported_count/total_rows*100:.1f}%)")
                
            except IntegrityError as e:
                # å”¯ä¸€ç´¢å¼•å†²çªï¼šè¯¥è‚¡ç¥¨è¯¥æ—¥æœŸå·²å­˜åœ¨
                db_session.rollback()
                logger.warning(f"  è·³è¿‡é‡å¤æ•°æ®: {stock_code} - {target_date}")
                skipped_count += 1
                # é‡æ–°å¼€å§‹å½“å‰è¡Œçš„å°äº‹åŠ¡
                db_session = SessionLocal()
                continue
            
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼šç«‹å³å›æ»šå¹¶æŠ›å‡º
                db_session.rollback()
                error_msg = f"å¯¼å…¥è®°å½•å¤±è´¥: {stock_code}, é”™è¯¯: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                state_manager.mark_failed(date_str, error_msg, imported_count)
                raise
        
        # === å…³é”®ï¼šæ•´ä¸ªæ–‡ä»¶æˆåŠŸåæ‰æäº¤äº‹åŠ¡ ===
        db_session.commit()
        duration = time.time() - start_time
        
        logger.info(f"âœ… æ–‡ä»¶å¯¼å…¥å®Œæˆ: {filename}")
        logger.info(f"   å¯¼å…¥: {imported_count} æ¡, è·³è¿‡: {skipped_count} æ¡, è€—æ—¶: {duration:.1f}ç§’")
        
        # æ›´æ–°çŠ¶æ€æ–‡ä»¶
        state_manager.mark_success(date_str, imported_count, skipped_count, duration)
        
        return imported_count, skipped_count, True
        
    except Exception as e:
        # === ä»»ä½•é”™è¯¯éƒ½å›æ»šæ•´ä¸ªäº‹åŠ¡ ===
        db_session.rollback()
        error_msg = f"æ–‡ä»¶å¯¼å…¥å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è®°å½•å›æ»šçŠ¶æ€
        state_manager.mark_rolled_back(date_str, error_msg)
        
        return 0, 0, False
        
    finally:
        db_session.close()


def get_data_files():
    """è·å–æ‰€æœ‰å¾…å¯¼å…¥çš„Excelæ–‡ä»¶"""
    data_dir = Path(DATA_DIR)
    if not data_dir.exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    files = sorted(data_dir.glob(FILE_PATTERN))
    logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶")
    return files


def main():
    """ä¸»å‡½æ•°ï¼šæ‰¹é‡å¯¼å…¥æ‰€æœ‰Excelæ–‡ä»¶"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ•°æ®å¯¼å…¥ä»»åŠ¡ï¼ˆå¥å£®ç‰ˆï¼‰")
    logger.info("=" * 60)
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_connection():
        logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # è·å–çŠ¶æ€ç®¡ç†å™¨
    state_manager = get_state_manager()
    
    # è·å–å¾…å¯¼å…¥æ–‡ä»¶
    files = get_data_files()
    if not files:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°å¾…å¯¼å…¥çš„æ–‡ä»¶")
        return
    
    # ç»Ÿè®¡
    total_imported = 0
    total_skipped = 0
    success_files = 0
    failed_files = 0
    
    # éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in files:
        imported, skipped, success = import_excel_file(file_path, state_manager)
        total_imported += imported
        total_skipped += skipped
        
        if success:
            success_files += 1
        else:
            failed_files += 1
    
    # æ‰“å°æ€»ç»“
    logger.info("=" * 60)
    logger.info("âœ… å¯¼å…¥ä»»åŠ¡å®Œæˆï¼")
    logger.info(f"æ–‡ä»¶ç»Ÿè®¡: æˆåŠŸ={success_files}, å¤±è´¥={failed_files}")
    logger.info(f"æ•°æ®ç»Ÿè®¡: å¯¼å…¥={total_imported}, è·³è¿‡={total_skipped}")
    logger.info("=" * 60)
    
    # æ‰“å°çŠ¶æ€æ‘˜è¦
    state_manager.print_summary()


if __name__ == "__main__":
    main()
