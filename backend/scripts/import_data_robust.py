"""
å¥å£®çš„æ•°æ®å¯¼å…¥è„šæœ¬
ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨å®ç°åŸå­æ€§ã€å¹‚ç­‰æ€§å’Œå®Œæ•´çš„å›æ»šæœºåˆ¶

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ã€å¹‚ç­‰æ€§ã€‘åŸºäºçŠ¶æ€æ–‡ä»¶åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¼å…¥
2. ã€åŸå­æ€§ã€‘æ•´ä¸ªæ–‡ä»¶åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­ï¼Œå¤±è´¥è‡ªåŠ¨å›æ»š
3. ã€çŠ¶æ€ç®¡ç†ã€‘æœ¬åœ°JSONæ–‡ä»¶è®°å½•æ‰€æœ‰å¯¼å…¥çŠ¶æ€
4. ã€æ–‡ä»¶æ ¡éªŒã€‘MD5å“ˆå¸Œæ£€æµ‹æ–‡ä»¶å˜åŒ–
5. ã€é”™è¯¯æ¢å¤ã€‘å¤±è´¥åå¯å®‰å…¨é‡è¯•
6. ã€æ— ä¾µå…¥æ€§ã€‘ä¸ä¿®æ”¹åŸå§‹Excelå’Œæ•°æ®åº“ç»“æ„
"""
import sys
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import logging
import time
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from app.database import SessionLocal, test_connection
from app.db_models import Stock, DailyStockData
from app.config import DATA_DIR, FILE_PATTERNS
from import_state_manager import get_state_manager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Excelåˆ—ååˆ°æ•°æ®åº“å­—æ®µçš„æ˜ å°„
COLUMN_MAPPING = {
    'æ€»åˆ†': 'total_score',
    'å¼€ç›˜': 'open_price',
    'æœ€é«˜': 'high_price',
    'æœ€ä½': 'low_price',
    'close': 'close_price',
    'jump': 'jump',
    'æ¶¨è·Œå¹…': 'price_change',
    'æ¢æ‰‹ç‡%': 'turnover_rate_percent',
    'æ”¾é‡å¤©æ•°': 'volume_days',
    'å¹³å‡é‡æ¯”_50å¤©': 'avg_volume_ratio_50',
    'æˆäº¤é‡': 'volume',
    'æ”¾é‡å¤©æ•°_volume': 'volume_days_volume',
    'å¹³å‡é‡æ¯”_50å¤©_volume': 'avg_volume_ratio_50_volume',
    'æ³¢åŠ¨ç‡': 'volatility',
    'volatile_consec': 'volatile_consec',
    'BETA': 'beta',
    'BETA_consec': 'beta_consec',
    'ç›¸å…³æ€§': 'correlation',
    'æ€»å¸‚å€¼(äº¿)': 'market_cap_billions',
    'é•¿æœŸ': 'long_term',
    'çŸ­æœŸ': 'short_term',
    'è¶…ä¹°': 'overbought',
    'è¶…å–': 'oversold',
    'macd_signal': 'macd_signal',
    'slowkdj_signal': 'slowkdj_signal',
    'lon_lonma': 'lon_lonma',
    'lon_consec': 'lon_consec',
    'lon_0': 'lon_0',
    'loncons_consec': 'loncons_consec',
    'lonma_0': 'lonma_0',
    'lonmacons_consec': 'lonmacons_consec',
    'dma': 'dma',
    'dma_consec': 'dma_consec',
    'dif_dem': 'dif_dem',
    'macd_consec': 'macd_consec',
    'dif_0': 'dif_0',
    'macdcons_consec': 'macdcons_consec',
    'dem_0': 'dem_0',
    'demcons_consec': 'demcons_consec',
    'pdi_adx': 'pdi_adx',
    'dmiadx_consec': 'dmiadx_consec',
    'pdi_ndi': 'pdi_ndi',
    'dmi_consec': 'dmi_consec',
    'obv': 'obv',
    'obv_consec': 'obv_consec',
    'k_kdj': 'k_kdj',
    'slowkdj_consec': 'slowkdj_consec',
    'rsi': 'rsi',
    'rsi_consec': 'rsi_consec',
    'cci_-90': 'cci_neg_90',
    'cci_lower_consec': 'cci_lower_consec',
    'cci_90': 'cci_pos_90',
    'cci_upper_consec': 'cci_upper_consec',
    'bands_lower': 'bands_lower',
    'bands_lower_consec': 'bands_lower_consec',
    'bands_middle': 'bands_middle',
    'bands_middle_consec': 'bands_middle_consec',
    'bands_upper': 'bands_upper',
    'bands_upper_consec': 'bands_upper_consec',
    'lon_lonma_diff': 'lon_lonma_diff',
    'lon': 'lon',
    'lonma': 'lonma',
    'histgram': 'histgram',
    'dif': 'dif',
    'dem': 'dem',
    'ADX': 'adx',
    'PLUS_DI': 'plus_di',
    'OBV': 'obv_2',
    'slowk': 'slowk',
    'RSI': 'rsi_2',
    'CCI_-90': 'cci_neg_90_2',
    'CCI_90': 'cci_pos_90_2',
    'lower': 'lower_band',
    'middle': 'middle_band',
    'upper': 'upper_band',
    'lst_close': 'lst_close',
    'code2': 'code2',
    'name2': 'name2',
    'zhangdiefu2': 'zhangdiefu2',
    'volume_consec2': 'volume_consec2',
    'volume_50_consec2': 'volume_50_consec2'
}


def extract_date_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åæå–æ—¥æœŸå­—ç¬¦ä¸²
    æ ¼å¼: YYYYMMDD_data_sma_feature_color.xlsx
    è¿”å›: '20251103'
    """
    try:
        return filename.split('_')[0]
    except Exception as e:
        logger.error(f"æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸ: {filename}, é”™è¯¯: {str(e)}")
        return None


def normalize_stock_code(code_str: str) -> str:
    """
    è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ï¼šè¡¥å…¨å‰å¯¼é›¶åˆ°6ä½
    """
    code = str(code_str).strip()
    if code.isdigit() and len(code) < 6:
        code = code.zfill(6)
    return code


def import_excel_file(file_path: Path, state_manager) -> tuple:
    """
    å¯¼å…¥å•ä¸ªExcelæ–‡ä»¶
    
    äº‹åŠ¡è®¾è®¡ï¼š
    - æ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆäº‹åŠ¡ï¼‰
    - æ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­
    - æˆåŠŸåˆ™commitï¼Œå¤±è´¥åˆ™rollback
    
    Returns:
        (imported_count, skipped_count, success)
    """
    filename = file_path.name
    date_str = extract_date_from_filename(filename)
    
    if not date_str:
        logger.warning(f"[è·³è¿‡] æ–‡ä»¶ï¼ˆæ— æ³•æå–æ—¥æœŸï¼‰: {filename}")
        return 0, 0, False
    
    # === å¹‚ç­‰æ€§æ£€æŸ¥ï¼šåŸºäºçŠ¶æ€æ–‡ä»¶ ===
    if not state_manager.should_reimport(date_str, file_path):
        logger.info(f"[è·³è¿‡] æ–‡ä»¶ï¼ˆå·²æˆåŠŸå¯¼å…¥ï¼‰: {filename}")
        # æ–‡ä»¶å·²å¯¼å…¥ï¼Œè®¡å…¥è·³è¿‡ç»Ÿè®¡ï¼ˆå¯¼å…¥=0ï¼Œè·³è¿‡=1ï¼‰
        return 0, 1, True
    
    # è®°å½•å¼€å§‹å¯¼å…¥
    state_manager.start_import(date_str, filename, file_path)
    start_time = time.time()
    
    # === åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼ˆäº‹åŠ¡è¾¹ç•Œï¼‰ ===
    db_session = SessionLocal()
    
    try:
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        target_date = datetime.strptime(date_str, '%Y%m%d').date()
        logger.info(f"ğŸ“‚ æ­£åœ¨å¯¼å…¥: {filename} (æ—¥æœŸ: {target_date})")
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path)
        total_rows = len(df)
        logger.info(f"ğŸ“Š è¯»å–åˆ° {total_rows} æ¡è®°å½•")
        
        # === æ™ºèƒ½å»é‡ï¼ˆåªå»é™¤æ˜æ˜¾å¼‚å¸¸çš„é‡å¤ï¼‰===
        from deduplicate_helper import DataDeduplicator, print_dedup_summary
        
        deduplicator = DataDeduplicator()
        df, dedup_stats = deduplicator.deduplicate_stock_data(df)
        
        # è®°å½•å»é‡ä¿¡æ¯åˆ°JSON
        state_manager.record_dedup_info(date_str, dedup_stats)
        
        # æ‰“å°å»é‡æ‘˜è¦
        if dedup_stats.get('removed_count', 0) > 0:
            print_dedup_summary(dedup_stats)
            logger.info(f"ğŸ“Š å»é‡åå‰©ä½™ {len(df)} æ¡è®°å½•")
        elif dedup_stats.get('has_duplicates') and dedup_stats.get('removed_count', 0) == 0:
            logger.warning(f"âš ï¸  æ£€æµ‹åˆ°é‡å¤ä½†æœªå»é‡ï¼ˆæ¡ä»¶ä¸æ»¡è¶³ï¼‰ï¼Œå°†åœ¨åç»­æ£€æŸ¥ä¸­å¤„ç†")
        
        # === æ•°æ®ä¿®è¡¥ï¼šæ¢æ‰‹ç‡ ===
        from data_fixer import TurnoverRateFixer
        
        turnover_fixer = TurnoverRateFixer(date_str, data_type='stock')
        fix_config = state_manager.state.get('fix_config', {}).get('turnover_rate_fix', {})
        
        if fix_config.get('enabled', True) and fix_config.get('auto_fix', True):
            logger.info("ğŸ”§ å¼€å§‹æ¢æ‰‹ç‡æ•°æ®æ£€æµ‹...")
            df, fix_info = turnover_fixer.fix_dataframe(df)
            
            if fix_info.get('applied'):
                logger.info(
                    f"âœ… æ¢æ‰‹ç‡å·²ä¿®è¡¥ï¼"
                    f"å½±å“è¡Œæ•°={fix_info['affected_rows']}, "
                    f"å¹³å‡å€¼: {fix_info['avg_before']:.6f} â†’ {fix_info['avg_after']:.4f}"
                )
                # è®°å½•ä¿®è¡¥ä¿¡æ¯ï¼ˆæš‚æ—¶ä¿å­˜åœ¨å†…å­˜ä¸­ï¼ŒæˆåŠŸå¯¼å…¥åå†å†™å…¥çŠ¶æ€æ–‡ä»¶ï¼‰
                turnover_fix_info = fix_info
            else:
                logger.info("â„¹ï¸  æ¢æ‰‹ç‡æ•°æ®æ­£å¸¸ï¼Œæ— éœ€ä¿®è¡¥")
                turnover_fix_info = None
        else:
            logger.info("â„¹ï¸  æ¢æ‰‹ç‡ä¿®è¡¥å·²ç¦ç”¨")
            turnover_fix_info = None
        
        # === æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡å¤ï¼ˆä¸¥æ ¼æ£€æŸ¥ï¼Œè§¦å‘ERRORï¼‰===
        # âš ï¸ é‡è¦ï¼šç¡®ä¿ä¸´æ—¶åˆ—ä¸å­˜åœ¨ï¼Œé¿å…"ç¯ä¸‹é»‘"
        if 'ä»£ç _normalized' in df.columns:
            df = df.drop(columns=['ä»£ç _normalized'])
        
        # é‡æ–°æ ‡å‡†åŒ–æ£€æŸ¥
        df['ä»£ç _normalized'] = df['ä»£ç '].apply(normalize_stock_code)
        duplicates = df[df.duplicated(subset=['ä»£ç _normalized'], keep=False)]
        
        if not duplicates.empty:
            dup_codes = duplicates['ä»£ç _normalized'].unique()
            logger.error(f"âŒ Excelæ–‡ä»¶ä¸­å­˜åœ¨é‡å¤çš„è‚¡ç¥¨ä»£ç ï¼ˆå»é‡åä»å­˜åœ¨ï¼‰: {', '.join(dup_codes)}")
            logger.error(f"   é‡å¤è®°å½•æ•°: {len(duplicates)}")
            for code in dup_codes:
                dup_rows = df[df['ä»£ç _normalized'] == code]
                logger.error(f"   è‚¡ç¥¨ {code} å‡ºç°äº† {len(dup_rows)} æ¬¡ï¼Œåœ¨è¡Œ: {list(dup_rows.index + 2)}")  # +2å› ä¸ºExcelä»1å¼€å§‹ä¸”æœ‰è¡¨å¤´
            
            # æ¸…ç†ä¸´æ—¶åˆ—
            df = df.drop(columns=['ä»£ç _normalized'])
            
            error_msg = f"Excelæ–‡ä»¶åŒ…å«é‡å¤çš„è‚¡ç¥¨ä»£ç : {', '.join(dup_codes)}"
            state_manager.mark_failed(date_str, error_msg, 0)
            raise ValueError(error_msg)
        
        imported_count = 0
        skipped_count = 0
        
        # === æ‰¹é‡å¯¼å…¥ï¼ˆåœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­ï¼‰ ===
        for idx, row in df.iterrows():
            try:
                # 1. å¤„ç†è‚¡ç¥¨ä»£ç 
                stock_code = normalize_stock_code(row['ä»£ç '])
                stock_name = str(row['åç§°']).strip()
                industry = str(row['è¡Œä¸š']).strip() if pd.notna(row['è¡Œä¸š']) else None
                rank = idx + 1
                
                # 2. ç¡®ä¿è‚¡ç¥¨è®°å½•å­˜åœ¨
                stock = db_session.query(Stock).filter(
                    Stock.stock_code == stock_code
                ).first()
                
                if not stock:
                    stock = Stock(
                        stock_code=stock_code,
                        stock_name=stock_name,
                        industry=industry,
                        last_updated=datetime.now()
                    )
                    db_session.add(stock)
                    # ä¸è¦ç«‹å³flushï¼Œç­‰å¾…æ•´ä½“æäº¤
                
                # 3. åˆ›å»ºæ¯æ—¥æ•°æ®è®°å½•
                daily_data = DailyStockData(
                    stock_code=stock_code,
                    date=target_date,
                    rank=rank
                )
                
                # 4. æ˜ å°„æ‰€æœ‰Excelåˆ—åˆ°æ•°æ®åº“å­—æ®µ
                for excel_col, db_col in COLUMN_MAPPING.items():
                    if excel_col in df.columns:
                        value = row[excel_col]
                        if pd.isna(value):
                            value = None
                        setattr(daily_data, db_col, value)
                
                db_session.add(daily_data)
                imported_count += 1
                
                # æ¯1000æ¡æ˜¾ç¤ºè¿›åº¦ï¼ˆä½†ä¸æäº¤ï¼‰
                if imported_count % 1000 == 0:
                    logger.info(f"  è¿›åº¦: {imported_count}/{total_rows} ({imported_count/total_rows*100:.1f}%)")
                
            except IntegrityError as e:
                # å”¯ä¸€ç´¢å¼•å†²çªï¼šè¯¥è‚¡ç¥¨è¯¥æ—¥æœŸå·²å­˜åœ¨
                db_session.rollback()
                logger.warning(f"  è·³è¿‡é‡å¤æ•°æ®: {stock_code} - {target_date}")
                skipped_count += 1
                # é‡æ–°å¼€å§‹å½“å‰è¡Œçš„å°äº‹åŠ¡
                db_session = SessionLocal()
                continue
            
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼šç«‹å³å›æ»šå¹¶æŠ›å‡º
                db_session.rollback()
                error_msg = f"å¯¼å…¥è®°å½•å¤±è´¥: {stock_code}, é”™è¯¯: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                state_manager.mark_failed(date_str, error_msg, imported_count)
                raise
        
        # === å…³é”®ï¼šæ•´ä¸ªæ–‡ä»¶æˆåŠŸåæ‰æäº¤äº‹åŠ¡ ===
        db_session.commit()
        duration = time.time() - start_time
        
        logger.info(f"âœ… æ–‡ä»¶å¯¼å…¥å®Œæˆ: {filename}")
        logger.info(f"   å¯¼å…¥: {imported_count} æ¡, è·³è¿‡: {skipped_count} æ¡, è€—æ—¶: {duration:.1f}ç§’")
        
        # æ›´æ–°çŠ¶æ€æ–‡ä»¶
        state_manager.mark_success(date_str, imported_count, skipped_count, duration)
        
        # è®°å½•ä¿®è¡¥ä¿¡æ¯åˆ°çŠ¶æ€æ–‡ä»¶
        if turnover_fix_info and turnover_fix_info.get('applied'):
            turnover_fixer.record_fix_to_state(turnover_fix_info)
        
        return imported_count, skipped_count, True
        
    except Exception as e:
        # === ä»»ä½•é”™è¯¯éƒ½å›æ»šæ•´ä¸ªäº‹åŠ¡ ===
        db_session.rollback()
        error_msg = f"æ–‡ä»¶å¯¼å…¥å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # è®°å½•å›æ»šçŠ¶æ€
        state_manager.mark_rolled_back(date_str, error_msg)
        
        return 0, 0, False
        
    finally:
        db_session.close()


def get_data_files():
    """è·å–æ‰€æœ‰å¾…å¯¼å…¥çš„Excelæ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§æ¨¡å¼ï¼‰"""
    data_dir = Path(DATA_DIR)
    if not data_dir.exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    # æ‰«ææ‰€æœ‰æ¨¡å¼çš„æ–‡ä»¶
    all_files = []
    for pattern in FILE_PATTERNS:
        files = list(data_dir.glob(pattern))
        all_files.extend(files)
        logger.info(f"æ¨¡å¼ '{pattern}': æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    
    # å»é‡å¹¶æ’åº
    unique_files = sorted(set(all_files))
    logger.info(f"æ€»è®¡: {len(unique_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    return unique_files


def main():
    """ä¸»å‡½æ•°ï¼šæ‰¹é‡å¯¼å…¥æ‰€æœ‰Excelæ–‡ä»¶"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ•°æ®å¯¼å…¥ä»»åŠ¡ï¼ˆå¥å£®ç‰ˆï¼‰")
    logger.info("=" * 60)
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_connection():
        logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # è·å–çŠ¶æ€ç®¡ç†å™¨
    state_manager = get_state_manager()
    
    # è·å–å¾…å¯¼å…¥æ–‡ä»¶
    files = get_data_files()
    if not files:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°å¾…å¯¼å…¥çš„æ–‡ä»¶")
        return
    
    # ç»Ÿè®¡
    total_imported = 0
    total_skipped = 0
    success_files = 0
    failed_files = 0
    
    # éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in files:
        imported, skipped, success = import_excel_file(file_path, state_manager)
        total_imported += imported
        total_skipped += skipped
        
        if success:
            success_files += 1
        else:
            failed_files += 1
    
    # æ‰“å°æ€»ç»“
    logger.info("=" * 60)
    logger.info("âœ… å¯¼å…¥ä»»åŠ¡å®Œæˆï¼")
    logger.info(f"æ–‡ä»¶ç»Ÿè®¡: æˆåŠŸ={success_files}, å¤±è´¥={failed_files}")
    logger.info(f"æ•°æ®ç»Ÿè®¡: å¯¼å…¥={total_imported}, è·³è¿‡={total_skipped}")
    logger.info("=" * 60)
    
    # æ‰“å°çŠ¶æ€æ‘˜è¦
    state_manager.print_summary()


if __name__ == "__main__":
    main()
