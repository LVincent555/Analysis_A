#!/usr/bin/env python3
"""
æ¿å—çƒ­åº¦è®¡ç®— ETL è„šæœ¬
============================
ä» ext_board_daily_snap + daily_stock_data è®¡ç®—æ¿å—çƒ­åº¦ï¼Œå†™å…¥ ext_board_heat_daily

æ ¸å¿ƒç®—æ³•:
1. è®¡ç®—åˆ†æ‘Šæƒé‡ share_ijï¼ˆçƒ­åº¦å®ˆæ’ï¼‰
2. èšåˆ B1/B2/C1/C2 æŒ‡æ ‡
3. è®¡ç®—ç»¼åˆçƒ­åº¦ heat_raw â†’ heat_pct

ä½¿ç”¨æ–¹æ³•:
    python task_board_heat.py                     # è®¡ç®—æœ€æ–°äº¤é›†æ—¥æœŸ
    python task_board_heat.py --date 2025-12-04   # è®¡ç®—æŒ‡å®šæ—¥æœŸ
    python task_board_heat.py --all               # è®¡ç®—æ‰€æœ‰å¯ç”¨æ—¥æœŸ
    python task_board_heat.py --force             # å¼ºåˆ¶é‡ç®—ï¼ˆè¦†ç›–å·²æœ‰æ•°æ®ï¼‰

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-12-11
ç‰ˆæœ¬: v1.0.0
"""

import argparse
import logging
import os
import sys
import json
from datetime import datetime, date, timedelta
from decimal import Decimal
from typing import Optional, List, Dict, Any

import numpy as np
import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv
from tqdm import tqdm

# ============================================================
# é…ç½®
# ============================================================
load_dotenv()

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“è¿æ¥
DB_HOST = os.getenv("DB_HOST", "192.168.182.128")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "db_20251106_analysis_a")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"


# ============================================================
# é…ç½®åŠ è½½å™¨
# ============================================================
class ConfigLoader:
    """ä» system_configs è¡¨åŠ è½½é…ç½®"""
    
    def __init__(self, engine):
        self.engine = engine
        self._cache: Dict[str, Any] = {}
        self._load_board_configs()
        self._load_blacklist()
    
    def _load_board_configs(self):
        """åŠ è½½ board ç±»åˆ«çš„é…ç½®"""
        sql = "SELECT config_key, config_value, config_type FROM system_configs WHERE category = 'board'"
        with self.engine.connect() as conn:
            result = conn.execute(text(sql))
            for row in result:
                key, value, vtype = row[0], row[1], row[2]
                if vtype == 'float':
                    self._cache[key] = float(value)
                elif vtype == 'int':
                    self._cache[key] = int(value)
                elif vtype == 'bool':
                    self._cache[key] = value.lower() in ('true', '1', 'yes')
                else:
                    self._cache[key] = value
        logger.info(f"å·²åŠ è½½ {len(self._cache)} ä¸ªæ¿å—é…ç½®é¡¹")

    def _load_blacklist(self):
        """åŠ è½½é»‘åå•/ç°åå•é…ç½®"""
        self.blacklist = {} # keyword -> level (BLACK/GREY)
        sql = "SELECT keyword, level FROM board_blacklist WHERE is_active = true"
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(sql))
                for row in result:
                    self.blacklist[row[0]] = row[1]
            logger.info(f"å·²åŠ è½½ {len(self.blacklist)} æ¡é»‘/ç°åå•è§„åˆ™")
        except Exception as e:
            logger.warning(f"åŠ è½½é»‘åå•å¤±è´¥ (å¯èƒ½æ˜¯è¡¨æœªåˆ›å»º): {e}")
            # é»˜è®¤ç¡¬ç¼–ç ä¸€äº›
            self.blacklist = {
                'èèµ„èåˆ¸': 'BLACK', 'è½¬èé€š': 'BLACK', 'æ·±è‚¡é€š': 'BLACK', 'æ²ªè‚¡é€š': 'BLACK',
                'å¤§ç›˜è‚¡': 'GREY', 'ä¸­ç›˜è‚¡': 'GREY', 'åˆ›ä¸šæ¿ç»¼': 'GREY', 'ä¸Šè¯180': 'GREY'
            }
    
    def get_blacklist_level(self, board_name: str) -> Optional[str]:
        """æ£€æŸ¥æ¿å—æ˜¯å¦å‘½ä¸­é»‘/ç°åå•ï¼Œè¿”å›çº§åˆ«"""
        for keyword, level in self.blacklist.items():
            if keyword in board_name:
                return level
        return None
    
    def get(self, key: str, default: Any = None) -> Any:
        return self._cache.get(key, default)
    
    # å¿«æ·å±æ€§
    @property
    def w_industry(self) -> float:
        return self.get('board_w_industry', 1.0)
    
    @property
    def w_concept(self) -> float:
        return self.get('board_w_concept', 0.7)
    
    @property
    def w_region(self) -> float:
        return self.get('board_w_region', 0.5)
    
    @property
    def heat_alpha(self) -> float:
        return self.get('board_heat_alpha', 0.4)
    
    @property
    def heat_beta(self) -> float:
        return self.get('board_heat_beta', 0.2)
    
    @property
    def heat_gamma(self) -> float:
        return self.get('board_heat_gamma', 0.3)
    
    @property
    def heat_delta(self) -> float:
        return self.get('board_heat_delta', 0.1)


# ============================================================
# æ¿å—çƒ­åº¦è®¡ç®—å™¨
# ============================================================
class BoardHeatCalculator:
    """æ¿å—çƒ­åº¦è®¡ç®—å™¨"""
    
    def __init__(self, engine, config: ConfigLoader, allow_latest_snap_fallback: bool = False):
        self.engine = engine
        self.config = config
        self.allow_latest_snap_fallback = allow_latest_snap_fallback
        self.Session = sessionmaker(bind=engine)
    
    def get_available_dates(self) -> List[date]:
        """è·å– daily_stock_data ä¸­æœ‰æ•°æ®çš„æ—¥æœŸï¼ˆä¸å†è¦æ±‚ä¸ snap äº¤é›†ï¼‰"""
        sql = """
        SELECT DISTINCT date FROM daily_stock_data 
        WHERE rank IS NOT NULL 
        ORDER BY date DESC
        """
        with self.engine.connect() as conn:
            result = conn.execute(text(sql))
            return [row[0] for row in result]
    
    def get_snap_dates(self) -> List[date]:
        """è·å–æ‰€æœ‰å¿«ç…§æ—¥æœŸï¼ˆç¨€ç–ï¼‰"""
        sql = "SELECT DISTINCT date FROM ext_board_daily_snap ORDER BY date DESC"
        with self.engine.connect() as conn:
            result = conn.execute(text(sql))
            return [row[0] for row in result]
    
    def find_nearest_snap_date(self, target_date: date) -> Optional[date]:
        """
        ã€å¯»å€ã€‘æ‰¾åˆ°ç¦» target_date æœ€è¿‘çš„å¿«ç…§æ—¥æœŸ
        é€»è¾‘ï¼šæ‰¾ä¸€ä¸ª <= ç›®æ ‡æ—¥æœŸçš„æœ€å¤§æ—¥æœŸ
        """
        sql = """
        SELECT MAX(date) FROM ext_board_daily_snap WHERE date <= :d
        """
        with self.engine.connect() as conn:
            result = conn.execute(text(sql), {"d": target_date})
            row = result.fetchone()
            nearest = row[0] if row and row[0] else None

            if nearest:
                return nearest

            # ä¸´æ—¶å…œåº•ï¼šå¦‚æœç›®æ ‡æ—¥æœŸä¹‹å‰æ²¡æœ‰ä»»ä½•å¿«ç…§ï¼Œåˆ™ç›´æ¥å€Ÿç”¨æœ€æ–°å¿«ç…§
            if not self.allow_latest_snap_fallback:
                return None

            latest_row = conn.execute(text("SELECT MAX(date) FROM ext_board_daily_snap")).fetchone()
            latest = latest_row[0] if latest_row and latest_row[0] else None
            if latest:
                logger.warning(f"âš ï¸ {target_date} ä¹‹å‰æ— ä»»ä½•æ¿å—å…³ç³»å¿«ç…§ï¼Œä¸´æ—¶å€Ÿç”¨æœ€æ–°å¿«ç…§ {latest} (å¯èƒ½å­˜åœ¨æœªæ¥å…³ç³»åå·®)")
            return latest
    
    def get_latest_date(self) -> Optional[date]:
        """è·å–æœ€æ–°çš„å¯è®¡ç®—æ—¥æœŸ"""
        dates = self.get_available_dates()
        return dates[0] if dates else None
    
    def check_existing(self, trade_date: date) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å·²æœ‰è®¡ç®—ç»“æœ"""
        sql = "SELECT COUNT(*) FROM ext_board_heat_daily WHERE trade_date = :d"
        with self.engine.connect() as conn:
            result = conn.execute(text(sql), {"d": trade_date})
            return result.scalar() > 0
    
    def delete_existing(self, trade_date: date):
        """åˆ é™¤æŒ‡å®šæ—¥æœŸçš„å·²æœ‰ç»“æœ"""
        sql = "DELETE FROM ext_board_heat_daily WHERE trade_date = :d"
        with self.engine.connect() as conn:
            conn.execute(text(sql), {"d": trade_date})
            conn.commit()
        logger.info(f"å·²åˆ é™¤ {trade_date} çš„æ—§æ•°æ®")
    
    def calculate(self, trade_date: date, force: bool = False) -> int:
        """
        è®¡ç®—æŒ‡å®šæ—¥æœŸçš„æ¿å—çƒ­åº¦
        
        ã€ç¨€ç–å¿«ç…§ã€‘ä½¿ç”¨"å¯»å€"é€»è¾‘ï¼š
        - ä¸ªè‚¡æ•°æ®ç”¨ trade_dateï¼ˆå®æ—¶ï¼‰
        - æ¿å—å…³ç³»ç”¨æœ€è¿‘çš„å¿«ç…§æ—¥æœŸï¼ˆç¨€ç–ï¼‰
        
        Returns:
            å†™å…¥çš„è®°å½•æ•°
        """
        try:
            logger.info(f"å¼€å§‹è®¡ç®— {trade_date} çš„æ¿å—çƒ­åº¦...")
            sys.stdout.flush()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.check_existing(trade_date) and not force:
                logger.warning(f"{trade_date} å·²æœ‰è®¡ç®—ç»“æœï¼Œè·³è¿‡ï¼ˆä½¿ç”¨ --force å¼ºåˆ¶é‡ç®—ï¼‰")
                return 0
            
            if force and self.check_existing(trade_date):
                self.delete_existing(trade_date)
            
            # ã€å¯»å€ã€‘æ‰¾åˆ°æœ€è¿‘çš„å¿«ç…§æ—¥æœŸ
            snap_date = self.find_nearest_snap_date(trade_date)
            if not snap_date:
                logger.error(f"æˆªæ­¢åˆ° {trade_date} æ²¡æœ‰ä»»ä½•æ¿å—å…³ç³»æ•°æ®ï¼Œæ— æ³•è®¡ç®—ï¼")
                return 0
            
            if snap_date != trade_date:
                logger.info(f"  ğŸ“… è®¡ç®—æ—¥æœŸ: {trade_date} | ğŸ”— å€Ÿç”¨å…³ç³»: {snap_date} (å¤ç”¨å†å²å¿«ç…§)")
            
            # Step 1: åŠ è½½åŸå§‹æ•°æ®ï¼ˆæ··åˆæ—¥æœŸï¼‰
            df_snap, df_stock, df_board = self._load_data(trade_date, snap_date)
            if df_snap.empty:
                logger.warning(f"{snap_date} æ— å¿«ç…§æ•°æ®")
                return 0
            if df_stock.empty:
                logger.warning(f"{trade_date} æ— ä¸ªè‚¡æ•°æ®")
                return 0
            
            logger.info(f"  å¿«ç…§æ•°æ®: {len(df_snap)} æ¡ ({snap_date}), ä¸ªè‚¡æ•°æ®: {len(df_stock)} æ¡ ({trade_date}), æ¿å—: {len(df_board)} ä¸ª")
            sys.stdout.flush()
            
            # Step 2: è®¡ç®—åˆ†æ‘Šæƒé‡ share_ij (å«é»‘åå•/ç°åå•é€»è¾‘)
            df_share = self._calc_share_weight(df_snap, df_board)
            logger.info(f"  åˆ†æ‘Šæƒé‡è®¡ç®—å®Œæˆ: {len(df_share)} æ¡")
            
            # Step 3: åˆå¹¶ä¸ªè‚¡æ•°æ®
            df_merged = df_share.merge(
                df_stock[['stock_code', 'rank', 'total_score', 'turnover_rate_percent', 'volume_days']], 
                on='stock_code', 
                how='inner'
            )
            logger.info(f"  åˆå¹¶å: {len(df_merged)} æ¡ï¼ˆäº¤é›†ï¼‰")
            
            if df_merged.empty:
                logger.warning(f"{trade_date} å¿«ç…§ä¸ä¸ªè‚¡æ•°æ®æ— äº¤é›†")
                return 0
                
            # Step 3.5: è®¡ç®— Contribution Score å¹¶æ›´æ–°å› ext_board_daily_snap
            self._update_contribution_score(df_merged, snap_date)
            
            # Step 4: èšåˆ B/C æŒ‡æ ‡
            df_heat = self._aggregate_bc(df_merged, trade_date)
            logger.info(f"  èšåˆå®Œæˆ: {len(df_heat)} ä¸ªæ¿å—")
            
            # Step 5: è®¡ç®—ç»¼åˆçƒ­åº¦å’Œåˆ†ä½
            df_heat = self._calc_heat_pct(df_heat)
            
            # Step 6: å†™å…¥æ•°æ®åº“
            count = self._save_results(df_heat, trade_date)
            logger.info(f"âœ… {trade_date} å†™å…¥ {count} æ¡æ¿å—çƒ­åº¦æ•°æ®")
            sys.stdout.flush()
            
            # Step 7: è®¡ç®—å¹¶å†™å…¥ä¸ªè‚¡æ¿å—ä¿¡å·ï¼ˆå«å…¨å¸‚åœºåˆ†ä½ + DNAï¼‰
            signal_count = self._calc_and_save_stock_signals(
                trade_date, snap_date, df_merged, df_heat, df_stock
            )
            logger.info(f"âœ… {trade_date} å†™å…¥ {signal_count} æ¡ä¸ªè‚¡ä¿¡å·æ•°æ®")
            
            return count

        except Exception as e:
            logger.error(f"âŒ è®¡ç®—è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            sys.stdout.flush()
            return 0
    
    def _load_data(self, trade_date: date, snap_date: date):
        """
        åŠ è½½åŸå§‹æ•°æ®ï¼ˆæ··åˆæ—¥æœŸï¼‰
        """
        # å¿«ç…§æ•°æ® - ç”¨ snap_date
        sql_snap = """
        SELECT stock_code, board_id, board_rank, weight
        FROM ext_board_daily_snap
        WHERE date = :snap_d
        """
        
        # ä¸ªè‚¡æ•°æ® - ç”¨ trade_dateï¼ˆå®æ—¶ï¼‰
        # ã€V4.0ã€‘å¢åŠ  turnover_rate_percent, volume_days ç”¨äºè®¡ç®—ä¸ªè‚¡ç¡¬å®åŠ›
        sql_stock = """
        SELECT stock_code, rank, total_score, turnover_rate_percent, volume_days
        FROM daily_stock_data
        WHERE date = :trade_d AND rank IS NOT NULL AND total_score IS NOT NULL
        """
        
        # æ¿å—åˆ—è¡¨ï¼ˆè·å–ç±»å‹ï¼‰
        sql_board = """
        SELECT id, board_name, board_type FROM ext_board_list WHERE is_active = true
        """
        
        with self.engine.connect() as conn:
            df_snap = pd.read_sql(text(sql_snap), conn, params={"snap_d": snap_date})
            df_stock = pd.read_sql(text(sql_stock), conn, params={"trade_d": trade_date})
            df_board = pd.read_sql(text(sql_board), conn)
        
        return df_snap, df_stock, df_board
    
    def _calc_share_weight(self, df_snap: pd.DataFrame, df_board: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—åˆ†æ‘Šæƒé‡ share_ijï¼ˆçƒ­åº¦å®ˆæ’ï¼‰
        ã€V4.0 å‡çº§ã€‘ï¼šé»‘åå•/ç°åå•é€»è¾‘
        """
        # åˆå¹¶æ¿å—ç±»å‹å’Œåç§°
        df = df_snap.merge(df_board, left_on='board_id', right_on='id', how='left')
        
        # åˆ†é…ç±»å‹æƒé‡
        type_weights = {
            'industry': self.config.w_industry,
            'concept': self.config.w_concept,
            'region': self.config.w_region
        }
        df['type_weight'] = df['board_type'].map(type_weights).fillna(self.config.w_concept)
        
        # ã€V4.0ã€‘é»‘åå•/ç°åå•æƒé‡è°ƒæ•´
        def apply_blacklist_penalty(row):
            level = self.config.get_blacklist_level(row['board_name'])
            if level == 'BLACK':
                return 0.01 # æ¥è¿‘0ä½†ä¿ç•™ï¼Œé¿å…é™¤é›¶é”™è¯¯æˆ–å®Œå…¨ä¸¢å¤±
            elif level == 'GREY':
                return 0.1  # ç°åå•ï¼Œä¿ç•™åº•è‰²
            return 1.0
        
        df['penalty_factor'] = df.apply(apply_blacklist_penalty, axis=1)
        df['type_weight'] = df['type_weight'] * df['penalty_factor']
        
        # æ ‡è®°æ˜¯å¦é»‘åå•ï¼ˆç”¨äºåç»­é€»è¾‘ï¼‰
        df['blacklist_level'] = df['board_name'].apply(self.config.get_blacklist_level)
        
        # è®¡ç®—æ¯åªè‚¡ç¥¨çš„æƒé‡æ€»å’Œ
        stock_weight_sum = df.groupby('stock_code')['type_weight'].transform('sum')
        
        # è®¡ç®—åˆ†æ‘Šæƒé‡
        # é¿å…åˆ†æ¯ä¸º0
        df['share_ij'] = df.apply(
            lambda row: row['type_weight'] / stock_weight_sum[row.name] if stock_weight_sum[row.name] > 0 else 0, 
            axis=1
        )
        
        return df[['stock_code', 'board_id', 'board_type', 'board_name', 'share_ij', 'blacklist_level', 'type_weight']]
    
    def _update_contribution_score(self, df_merged: pd.DataFrame, snap_date: date):
        """
        ã€V4.0ã€‘è®¡ç®—å¹¶æ›´æ–° Contribution Score åˆ° ext_board_daily_snap
        contribution_score = share_ij * total_score
        """
        logger.info("  æ›´æ–° Contribution Score åˆ°ç‰©ç†è¡¨...")
        
        df_merged['contribution_score'] = df_merged['share_ij'] * df_merged['total_score']
        
        # æ‰¹é‡æ›´æ–°
        # ç”±äº sqlalchemy ä¸æ”¯æŒæ‰¹é‡ update from values è¿™ç§é«˜æ•ˆè¯­æ³•ï¼ˆå–å†³äºDBï¼‰ï¼Œ
        # è¿™é‡Œä½¿ç”¨ä¸´æ—¶è¡¨æ–¹å¼æˆ–é€æ¡æ›´æ–°ã€‚ä¸ºäº†æ€§èƒ½ï¼Œä½¿ç”¨ä¸´æ—¶è¡¨ + UPDATE FROM
        
        # æå–éœ€è¦æ›´æ–°çš„æ•°æ®
        update_data = df_merged[['stock_code', 'board_id', 'contribution_score']].to_dict('records')
        if not update_data:
            return

        # åˆ›å»ºä¸´æ—¶è¡¨å¹¶æ‰¹é‡æ›´æ–°
        try:
            with self.engine.begin() as conn: # Transaction
                # 1. åˆ›å»ºä¸´æ—¶è¡¨
                conn.execute(text("""
                    CREATE TEMP TABLE temp_contrib_update (
                        stock_code VARCHAR(10),
                        board_id INT,
                        contribution_score DECIMAL(20,8)
                    ) ON COMMIT DROP
                """))
                
                # 2. æ’å…¥æ•°æ®
                conn.execute(
                    text("INSERT INTO temp_contrib_update (stock_code, board_id, contribution_score) VALUES (:stock_code, :board_id, :contribution_score)"),
                    update_data
                )
                
                # 3. æ‰§è¡Œæ›´æ–°
                conn.execute(text("""
                    UPDATE ext_board_daily_snap t
                    SET contribution_score = temp.contribution_score
                    FROM temp_contrib_update temp
                    WHERE t.stock_code = temp.stock_code 
                      AND t.board_id = temp.board_id
                      AND t.date = :snap_date
                """), {"snap_date": snap_date})
                
            logger.info(f"  å·²æ›´æ–° {len(update_data)} æ¡ contribution_score")
        except Exception as e:
            logger.error(f"æ›´æ–° contribution_score å¤±è´¥: {e}")

    def _aggregate_bc(self, df: pd.DataFrame, trade_date: date) -> pd.DataFrame:
        """
        èšåˆ B/C æŒ‡æ ‡ + ã€V4.0ã€‘æ ‡å‡†å·®
        """
        k = 1.0  # æ’åæŒ‡æ•°
        
        # è®¡ç®—åŠ æƒè´¡çŒ®
        df['b1_contrib'] = df['share_ij'] * (1.0 / np.power(df['rank'].clip(lower=1), k))
        df['c1_contrib'] = df['share_ij'] * df['total_score']
        
        # æŒ‰æ¿å—èšåˆ
        agg = df.groupby('board_id').agg(
            stock_count=('stock_code', 'count'),
            b1_rank_sum=('b1_contrib', 'sum'),
            c1_score_sum=('c1_contrib', 'sum'),
            # ã€V4.0ã€‘åˆ†æ­§ç›‘æµ‹ï¼šè®¡ç®— total_score çš„æ ‡å‡†å·®
            score_stddev=('total_score', 'std')
        ).reset_index()
        
        # è®¡ç®—å¯†åº¦æŒ‡æ ‡
        agg['b2_rank_avg'] = agg['b1_rank_sum'] / agg['stock_count'].clip(lower=1)
        agg['c2_score_avg'] = agg['c1_score_sum'] / agg['stock_count'].clip(lower=1)
        
        agg['trade_date'] = trade_date
        
        return agg
    
    def _calc_heat_pct(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—ç»¼åˆçƒ­åº¦å’Œåˆ†ä½å€¼
        """
        # æ ‡å‡†åŒ–å„æŒ‡æ ‡ï¼ˆMin-Maxï¼‰
        def normalize(series):
            min_val, max_val = series.min(), series.max()
            if max_val - min_val < 1e-10:
                return pd.Series([0.5] * len(series), index=series.index)
            return (series - min_val) / (max_val - min_val)
        
        norm_b1 = normalize(df['b1_rank_sum'])
        norm_b2 = normalize(df['b2_rank_avg'])
        norm_c2 = normalize(df['c2_score_avg'])
        
        alpha = self.config.heat_alpha
        beta = self.config.heat_beta
        gamma = self.config.heat_gamma
        
        total = alpha + beta + gamma
        alpha, beta, gamma = alpha/total, beta/total, gamma/total
        
        df['heat_raw'] = alpha * norm_b1 + beta * norm_b2 + gamma * norm_c2
        
        # è®¡ç®—åˆ†ä½å€¼
        df['heat_pct'] = df['heat_raw'].rank(pct=True)
        
        return df
    
    def _save_results(self, df: pd.DataFrame, trade_date: date) -> int:
        """ä¿å­˜ç»“æœåˆ°æ•°æ®åº“"""
        records = []
        for _, row in df.iterrows():
            records.append({
                'trade_date': trade_date,
                'board_id': int(row['board_id']),
                'stock_count': int(row['stock_count']),
                'b1_rank_sum': float(row['b1_rank_sum']),
                'b2_rank_avg': float(row['b2_rank_avg']),
                'c1_score_sum': float(row['c1_score_sum']),
                'c2_score_avg': float(row['c2_score_avg']),
                'heat_raw': float(row['heat_raw']),
                'heat_pct': float(row['heat_pct']),
                'score_stddev': float(row['score_stddev']) if not pd.isna(row['score_stddev']) else 0.0
            })
        
        if not records:
            return 0
        
        # æ‰¹é‡æ’å…¥
        sql = """
        INSERT INTO ext_board_heat_daily 
            (trade_date, board_id, stock_count, b1_rank_sum, b2_rank_avg, 
             c1_score_sum, c2_score_avg, heat_raw, heat_pct, score_stddev)
        VALUES 
            (:trade_date, :board_id, :stock_count, :b1_rank_sum, :b2_rank_avg,
             :c1_score_sum, :c2_score_avg, :heat_raw, :heat_pct, :score_stddev)
        """
        
        with self.engine.connect() as conn:
            for record in records:
                conn.execute(text(sql), record)
            conn.commit()
        
        return len(records)
    
    # ==================== ä¸ªè‚¡æ¿å—ä¿¡å·è®¡ç®— ====================
    
    def _calc_and_save_stock_signals(
        self, 
        trade_date: date, 
        snap_date: date,
        df_merged: pd.DataFrame, 
        df_heat: pd.DataFrame,
        df_stock: pd.DataFrame
    ) -> int:
        """
        è®¡ç®—å¹¶ä¿å­˜ä¸ªè‚¡æ¿å—ä¿¡å·
        ã€V4.0 å‡çº§ã€‘ï¼šå…¨å¸‚åœºåˆ†ä½ + DNA JSON
        """
        logger.info("  å¼€å§‹è®¡ç®—ä¸ªè‚¡æ¿å—ä¿¡å·...")
        
        industry_map = self._load_stock_industries()
        board_info = self._load_board_info(df_heat)
        stock_names = self._load_stock_names()
        
        stock_signals = []
        grouped = df_merged.groupby('stock_code')
        
        w_stock = self.config.get('board_w_stock', 1.5) # V4.0 æé«˜ä¸ªè‚¡æƒé‡
        w_exposure = self.config.get('board_w_exposure', 0.5)
        w_max_concept = self.config.get('board_w_max_concept', 0.3)
        penalty_unsafe = self.config.get('board_penalty_unsafe', 0.5)
        safe_pct = self.config.get('board_safe_pct', 0.3)
        
        # ä¸´æ—¶å­˜å‚¨ï¼Œç”¨äºè®¡ç®—åˆ†ä½
        temp_results = []
        
        logger.info(f"å¾…å¤„ç†ä¸ªè‚¡æ•°é‡: {len(grouped)}")
        processed_count = 0

        try:
            for stock_code, group in grouped:
                processed_count += 1
                if processed_count % 1000 == 0:
                    logger.info(f"å·²å¤„ç† {processed_count} åªè‚¡ç¥¨...")
                    
                try:
                    stock_row = df_stock[df_stock['stock_code'] == stock_code].iloc[0]
                    market_rank = int(stock_row['rank'])
                    total_score = float(stock_row['total_score'])
                    stock_name = stock_names.get(stock_code, '')
                    
                    board_ids = group['board_id'].tolist()
                    shares = group['share_ij'].tolist()
                    blacklist_levels = group['blacklist_level'].tolist()
                    
                    # è®¡ç®— Exposure å’Œæ”¶é›†æ¿å—ä¿¡æ¯
                    exposure = 0.0
                    boards_details = []
                    
                    for bid, share, bl_level in zip(board_ids, shares, blacklist_levels):
                        info = board_info.get(bid, {})
                        heat_pct = info.get('heat_pct', 0)
                        exposure += share * heat_pct
                        
                        boards_details.append({
                            'id': bid,
                            'name': info.get('name', ''),
                            'type': info.get('type', ''),
                            'heat_pct': heat_pct,
                            'share': share,
                            'blacklist_level': bl_level
                        })
                    
                    # Fallback ç­–ç•¥é€‰æ‹©æœ€ä½³é©±åŠ¨
                    # ä¼˜å…ˆçº§: éé»‘åå•S/Aæ¦‚å¿µ > éé»‘åå•S/Aè¡Œä¸š > éé»‘åå•B > å…¶å®ƒ
                    max_driver = self._select_best_driver(boards_details)
                    max_heat = max_driver.get('heat_pct', 0)
                    
                    # è¡Œä¸šå®‰å…¨
                    industry_id = industry_map.get(stock_code)
                    industry_info = board_info.get(industry_id, {}) if industry_id else {}
                    industry_heat_pct = industry_info.get('heat_pct', 0)
                    industry_safe = industry_heat_pct >= safe_pct if industry_id else True
                    
                    # è®¡ç®—åˆæˆåˆ† (Final Score)
                    final_score = (
                        w_stock * (total_score / 100) +
                        w_exposure * exposure +
                        w_max_concept * max_heat
                    )
                    
                    if not industry_safe:
                        final_score *= penalty_unsafe
                    
                    # ã€V4.0ã€‘è®¡ç®—ä¸ªè‚¡ç¡¬å®åŠ›å­åˆ†æ•° (0-100å½’ä¸€åŒ–)
                    # C2: é‡èƒ½çˆ†å‘ (volume_days) - å‡è®¾ >0 ä¸ºå¼ºï¼Œæ˜ å°„ -20~20 -> 0~100
                    raw_vol = float(stock_row.get('volume_days', 0) or 0)
                    vol_score = min(100, max(0, 50 + raw_vol * 2.5))
                    
                    # C1: èµ„é‡‘å¼ºåº¦ (turnover) - 20%æ¢æ‰‹ = 100åˆ†
                    raw_turnover = float(stock_row.get('turnover_rate_percent', 0) or 0)
                    turnover_score = min(100, max(0, raw_turnover * 5))
                    
                    # B2: è¶‹åŠ¿å½¢æ€ (total_score) - å‡è®¾ -50~50 -> 0~100
                    trend_score = min(100, max(0, 50 + total_score))
                    
                    # ç”Ÿæˆ DNA JSON
                    stock_detail_json = {
                         "vol_score": round(vol_score, 1),
                         "turnover_score": round(turnover_score, 1),
                         "trend_score": round(trend_score, 1),
                         "rank": market_rank,
                         "contribution_score": group['contribution_score'].max() if 'contribution_score' in group.columns else 0
                    }
                    
                    dna_data = {
                        "score_breakdown": {
                            "stock": round(w_stock * (total_score / 100), 4),
                            "exposure": round(w_exposure * exposure, 4),
                            "driver": round(w_max_concept * max_heat, 4),
                            "formula": f"{w_stock}*Stock + {w_exposure}*Expo + {w_max_concept}*Driver"
                        },
                        "boards": sorted(boards_details, key=lambda x: x['share'], reverse=True),
                        "fallback_selected": max_driver.get('name', 'None'),
                        "max_concept_name": max_driver.get('name', ''),
                        "max_concept_heat": max_heat,
                        "industry_name": industry_info.get('name', ''),
                        "industry_heat": industry_heat_pct,
                        "industry_safe": industry_safe,
                        "stock_details": stock_detail_json
                    }
                    
                    temp_results.append({
                        'trade_date': trade_date,
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'market_rank': market_rank,
                        'total_score': total_score,
                        'final_score': final_score,
                        'max_driver': max_driver,
                        'industry_id': industry_id,
                        'industry_name': industry_info.get('name', ''),
                        'industry_heat_pct': industry_heat_pct,
                        'industry_safe': industry_safe,
                        'exposure': exposure,
                        'board_count': len(board_ids),
                        'top_boards_json': json.dumps([
                            {k: v for k, v in b.items() if k in ['id', 'name', 'type', 'heat_pct']} 
                            for b in sorted(boards_details, key=lambda x: x['heat_pct'], reverse=True)[:5]
                        ], ensure_ascii=False),
                        'dna_json': json.dumps(dna_data, ensure_ascii=False),
                        'snap_date': snap_date
                    })
                except Exception as inner_e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_code} å¤±è´¥: {inner_e}")
                    continue

        except Exception as e:
            logger.error(f"è®¡ç®—ä¸ªè‚¡ä¿¡å·å¾ªç¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
            
        # ã€V4.0ã€‘å…¨å¸‚åœºåˆ†ä½è®¡ç®—
        df_res = pd.DataFrame(temp_results)
        if not df_res.empty:
            df_res['final_score_pct'] = df_res['final_score'].rank(pct=True)
            
            s_pct = self.config.get('board_signal_S_pct', 0.97)
            a_pct = self.config.get('board_signal_A_pct', 0.90)
            b_pct = self.config.get('board_signal_B_pct', 0.80)
            
            def get_level(pct):
                if pct >= s_pct: return 'S'
                if pct >= a_pct: return 'A'
                if pct >= b_pct: return 'B'
                return 'NONE'
            
            df_res['signal_level'] = df_res['final_score_pct'].apply(get_level)
            
            # è½¬å› dict åˆ—è¡¨
            stock_signals = df_res.to_dict('records')
            
            # 5. æ‰¹é‡å†™å…¥
            count = self._save_stock_signals(stock_signals, trade_date)
            return count
        return 0

    def _select_best_driver(self, boards: list) -> dict:
        """
        ã€V3.5/4.0ã€‘Fallback ç­–ç•¥é€‰æ‹©æœ€ä½³é©±åŠ¨
        """
        # è¿‡æ»¤æ‰ BLACK åå• (ä½† GREY ä¿ç•™)
        candidates = [b for b in boards if b.get('blacklist_level') != 'BLACK']
        if not candidates:
            return {}
        
        s_pct = self.config.get('board_signal_S_pct', 0.97)
        a_pct = self.config.get('board_signal_A_pct', 0.90)
        
        # 1. éé»‘åå• S/A çº§æ¦‚å¿µ
        tier1 = [b for b in candidates if b['type'] == 'concept' and b['heat_pct'] >= a_pct]
        if tier1: return max(tier1, key=lambda x: x['heat_pct'])
        
        # 2. éé»‘åå• S/A çº§è¡Œä¸š
        tier2 = [b for b in candidates if b['type'] == 'industry' and b['heat_pct'] >= a_pct]
        if tier2: return max(tier2, key=lambda x: x['heat_pct'])
        
        # 3. å…¶å®ƒ (å–çƒ­åº¦æœ€é«˜)
        return max(candidates, key=lambda x: x['heat_pct'])

    def _load_stock_industries(self) -> Dict[str, int]:
        """åŠ è½½ä¸ªè‚¡ä¸»è¥è¡Œä¸šæ˜ å°„ {stock_code: industry_board_id}"""
        sql = """
        SELECT s.stock_code, b.id as industry_board_id
        FROM stocks s
        JOIN ext_board_list b ON b.board_name = TRIM(BOTH '[]''' FROM s.industry)
            AND b.board_type = 'industry'
        WHERE s.industry IS NOT NULL 
            AND s.industry != '[]'
            AND s.industry != ''
        """
        result = {}
        with self.engine.connect() as conn:
            rows = conn.execute(text(sql))
            for row in rows:
                result[row[0]] = row[1]
        return result
    
    def _load_board_info(self, df_heat: pd.DataFrame) -> Dict[int, Dict]:
        """æ„å»ºæ¿å—ä¿¡æ¯å­—å…¸ {board_id: {name, type, heat_pct}}"""
        sql = """
        SELECT id, board_name, board_type FROM ext_board_list WHERE is_active = true
        """
        board_info = {}
        with self.engine.connect() as conn:
            rows = conn.execute(text(sql))
            for row in rows:
                board_info[row[0]] = {
                    'name': row[1],
                    'type': row[2],
                    'heat_pct': 0
                }
        
        # å¡«å……çƒ­åº¦åˆ†ä½
        for _, row in df_heat.iterrows():
            bid = int(row['board_id'])
            if bid in board_info:
                board_info[bid]['heat_pct'] = float(row['heat_pct'])
        
        return board_info
    
    def _load_stock_names(self) -> Dict[str, str]:
        """åŠ è½½è‚¡ç¥¨åç§°æ˜ å°„"""
        sql = "SELECT stock_code, stock_name FROM stocks"
        result = {}
        with self.engine.connect() as conn:
            rows = conn.execute(text(sql))
            for row in rows:
                result[row[0]] = row[1]
        return result
    
    def _save_stock_signals(self, signals: list, trade_date: date) -> int:
        """æ‰¹é‡ä¿å­˜ä¸ªè‚¡ä¿¡å·åˆ°ç¼“å­˜è¡¨ (åˆ†æ‰¹å†™å…¥ + æ•°æ®æ¸…æ´—)"""
        if not signals:
            return 0
        
        # å…ˆåˆ é™¤å·²æœ‰æ•°æ®
        delete_sql = "DELETE FROM cache_stock_board_signal WHERE trade_date = :d"
        
        insert_sql = """
        INSERT INTO cache_stock_board_signal (
            trade_date, stock_code, stock_name, market_rank, total_score,
            signal_level, final_score,
            max_driver_board_id, max_driver_name, max_driver_type, max_driver_heat_pct,
            primary_industry_id, primary_industry_name, primary_industry_heat_pct, industry_safe,
            board_exposure, board_count, top_boards_json, snap_date,
            dna_json, final_score_pct, fallback_reason
        ) VALUES (
            :trade_date, :stock_code, :stock_name, :market_rank, :total_score,
            :signal_level, :final_score,
            :max_driver_board_id, :max_driver_name, :max_driver_type, :max_driver_heat_pct,
            :industry_id, :industry_name, :industry_heat_pct, :industry_safe,
            :board_exposure, :board_count, :top_boards_json, :snap_date,
            :dna_json, :final_score_pct, :fallback_reason
        )
        """
        
        def safe_int(val):
            """Convert to int64-safe value; return None on NaN/None/overflow."""
            try:
                if pd.isna(val) or val is None:
                    return None
                v = float(val)
                if np.isnan(v) or np.isinf(v):
                    return None
                v_int = int(v)
                # guard bigint overflow
                int64_max = 2**63 - 1
                int64_min = -2**63
                if v_int > int64_max or v_int < int64_min:
                    logger.warning(f"  safe_int overflow drop: {v_int}")
                    return None
                return v_int
            except Exception:
                return None

        def safe_float(val):
            try:
                if pd.isna(val) or val is None or np.isinf(val): return None
                return float(val)
            except:
                return None

        total_inserted = 0
        batch_size = 1000
        
        try:
            with self.engine.connect() as conn:
                conn.execute(text(delete_sql), {"d": trade_date})
                conn.commit()
                
                # åˆ†æ‰¹å¤„ç†
                for i in range(0, len(signals), batch_size):
                    batch = signals[i:i+batch_size]
                    final_records = []
                    
                    for sig in batch:
                        md = sig.get('max_driver', {})
                        
                        rec = {
                            'trade_date': sig['trade_date'],
                            'stock_code': sig['stock_code'],
                            'stock_name': sig['stock_name'],
                            'market_rank': safe_int(sig['market_rank']),
                            'total_score': safe_float(sig['total_score']),
                            'signal_level': sig['signal_level'],
                            'final_score': safe_float(sig.get('final_score', 0)),
                            'max_driver_board_id': safe_int(md.get('id')),
                            'max_driver_name': md.get('name', ''),
                            'max_driver_type': md.get('type', ''),
                            'max_driver_heat_pct': safe_float(md.get('heat_pct', 0)),
                            'industry_id': safe_int(sig['industry_id']),
                            'industry_name': sig['industry_name'],
                            'industry_heat_pct': safe_float(sig['industry_heat_pct']),
                            'industry_safe': bool(sig['industry_safe']),
                            'board_exposure': safe_float(sig['exposure']),
                            'board_count': safe_int(sig['board_count']),
                            'top_boards_json': sig['top_boards_json'],
                            'snap_date': sig['snap_date'],
                            'dna_json': sig['dna_json'],
                            'final_score_pct': safe_float(sig.get('final_score_pct', 0)),
                            'fallback_reason': sig.get('fallback_reason', '')
                        }
                        final_records.append(rec)

                    try:
                        conn.execute(text(insert_sql), final_records)
                        conn.commit()
                        total_inserted += len(final_records)
                        logger.info(f"  å·²å†™å…¥ {total_inserted}/{len(signals)} æ¡ä¿¡å·æ•°æ®")
                    except Exception as e:
                        logger.error(f"  âŒ æ‰¹é‡å†™å…¥å¤±è´¥ (Batch {i//batch_size}): {e}")
                        logger.error(f"  Sample Record: {final_records[0] if final_records else 'Empty'}")
                        # Don't raise, try next batch? No, data integrity matters.
                        raise e
                        
        except Exception as e:
            logger.error(f"ä¿å­˜ä¿¡å·æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return total_inserted
        
        return total_inserted


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    parser = argparse.ArgumentParser(description='æ¿å—çƒ­åº¦è®¡ç®— ETL')
    parser.add_argument('--date', type=str, help='æŒ‡å®šè®¡ç®—æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--all', action='store_true', help='è®¡ç®—æ‰€æœ‰å¯ç”¨æ—¥æœŸ')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡ç®—ï¼ˆè¦†ç›–å·²æœ‰æ•°æ®ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='åªæ˜¾ç¤ºå¯ç”¨æ—¥æœŸï¼Œä¸æ‰§è¡Œè®¡ç®—')
    parser.add_argument('--allow-latest-snap-fallback', action='store_true', help='å½“ç›®æ ‡æ—¥æœŸä¹‹å‰æ— ä»»ä½•å¿«ç…§æ—¶ï¼Œå…è®¸å€Ÿç”¨æœ€æ–°å¿«ç…§ï¼ˆé»˜è®¤å…³é—­ï¼‰')
    args = parser.parse_args()
    
    # åˆ›å»ºå¼•æ“
    engine = create_engine(DATABASE_URL)
    
    # åŠ è½½é…ç½®
    config = ConfigLoader(engine)
    
    # åˆ›å»ºè®¡ç®—å™¨
    calc = BoardHeatCalculator(engine, config, allow_latest_snap_fallback=args.allow_latest_snap_fallback)
    
    # è·å–å¯ç”¨æ—¥æœŸ
    available_dates = calc.get_available_dates()
    if not available_dates:
        logger.error("æ²¡æœ‰å¯ç”¨çš„è®¡ç®—æ—¥æœŸï¼ˆsnap å’Œ daily_stock_data æ— äº¤é›†ï¼‰")
        return 1
    
    logger.info(f"å¯ç”¨æ—¥æœŸèŒƒå›´: {available_dates[-1]} ~ {available_dates[0]}ï¼Œå…± {len(available_dates)} å¤©")
    
    if args.dry_run:
        logger.info("--dry-run æ¨¡å¼ï¼Œä¸æ‰§è¡Œè®¡ç®—")
        for d in available_dates[:10]:
            exists = "âœ“" if calc.check_existing(d) else "â—‹"
            logger.info(f"  {exists} {d}")
        if len(available_dates) > 10:
            logger.info(f"  ... è¿˜æœ‰ {len(available_dates) - 10} å¤©")
        return 0
    
    # ç¡®å®šè¦è®¡ç®—çš„æ—¥æœŸ
    if args.all:
        # --all ä¼˜å…ˆï¼šè®¡ç®—æ‰€æœ‰å¯ç”¨æ—¥æœŸ
        dates_to_calc = available_dates
        logger.info(f"ğŸ“… è®¡ç®—å…¨éƒ¨æ—¥æœŸæ¨¡å¼ï¼Œå…± {len(dates_to_calc)} å¤©")
    elif args.date:
        requested_date = datetime.strptime(args.date, '%Y-%m-%d').date()
        
        if requested_date in available_dates:
            target_date = requested_date
        else:
            target_date = None
            for d in available_dates:
                if d <= requested_date:
                    target_date = d
                    break
            
            if not target_date:
                logger.error(f"{requested_date} ä¸åœ¨å¯ç”¨æ—¥æœŸèŒƒå›´å†…")
                return 1

            logger.info(f"ğŸ“… {requested_date} éäº¤æ˜“æ—¥/æ— æ•°æ®ï¼Œå›é€€åˆ°æœ€è¿‘äº¤æ˜“æ—¥ {target_date}")
        
        dates_to_calc = [target_date]
    else:
        # é»˜è®¤åªè®¡ç®—æœ€æ–°æ—¥æœŸ
        dates_to_calc = [available_dates[0]]
    
    # æ‰§è¡Œè®¡ç®—
    total_count = 0
    for d in tqdm(dates_to_calc, desc="è®¡ç®—æ¿å—çƒ­åº¦"):
        count = calc.calculate(d, force=args.force)
        total_count += count
    
    logger.info(f"ğŸ‰ å®Œæˆï¼å…±è®¡ç®— {len(dates_to_calc)} å¤©ï¼Œå†™å…¥ {total_count} æ¡è®°å½•")
    return 0


if __name__ == '__main__':
    sys.exit(main())
