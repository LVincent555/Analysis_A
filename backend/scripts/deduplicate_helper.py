"""
Excelæ•°æ®å»é‡è¾…åŠ©å·¥å…·
ä¸“é—¨å¤„ç†ï¼šåŒä¸€è‚¡ç¥¨ä»£ç å‡ºç°å¤šæ¬¡ï¼Œä¸”è¯„åˆ†å¼‚å¸¸çš„æƒ…å†µ

ç­–ç•¥ï¼š
1. æ£€æµ‹é‡å¤çš„è‚¡ç¥¨ä»£ç 
2. æ¯”è¾ƒtotal_scoreï¼ˆç»¼åˆè¯„åˆ†ï¼‰
3. ä¿ç•™è¯„åˆ†æ›´åˆç†çš„é‚£æ¡ï¼ˆåŸºäºæ’åä¸Šä¸‹æ–‡ï¼‰
4. ä¸¥æ ¼æ¡ä»¶ï¼šåªåœ¨æ˜ç¡®å¼‚å¸¸æ—¶æ‰å»é‡
"""
import pandas as pd
import logging
from typing import Tuple, List

logger = logging.getLogger(__name__)


class DataDeduplicator:
    """æ•°æ®å»é‡å™¨"""
    
    def __init__(self):
        self.removed_count = 0
        self.removed_details = []
    
    def deduplicate_stock_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
        """
        å»é‡è‚¡ç¥¨æ•°æ®
        
        ç­–ç•¥ï¼š
        1. æ£€æµ‹é‡å¤çš„è‚¡ç¥¨ä»£ç 
        2. å¯¹äºé‡å¤è®°å½•ï¼Œè®¡ç®—æ¯æ¡çš„"åˆç†æ€§åˆ†æ•°"
        3. ä¿ç•™æœ€åˆç†çš„é‚£æ¡ï¼Œåˆ é™¤å…¶ä»–
        
        Args:
            df: åŸå§‹DataFrame
        
        Returns:
            (å»é‡åçš„DataFrame, ç»Ÿè®¡ä¿¡æ¯)
        """
        self.removed_count = 0
        self.removed_details = []
        
        # 0. æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
        score_col = None
        for col_name in ['æ€»åˆ†', 'ç»¼åˆè¯„åˆ†', 'score']:
            if col_name in df.columns:
                score_col = col_name
                break
        
        if score_col is None:
            logger.warning(f"âš ï¸  Excelç¼ºå°‘è¯„åˆ†åˆ—ï¼ˆæ€»åˆ†/ç»¼åˆè¯„åˆ†/scoreï¼‰ï¼Œè·³è¿‡æ™ºèƒ½å»é‡")
            logger.warning(f"   å¯ç”¨åˆ—: {', '.join(df.columns.tolist()[:10])}")
            return df, {
                'has_duplicates': False,
                'removed_count': 0,
                'removed_details': [],
                'duplicate_codes': [],
                'skip_reason': 'missing_score_column'
            }
        
        # ä¿å­˜åˆ—åä¾›åç»­ä½¿ç”¨
        self.score_column = score_col
        logger.info(f"ä½¿ç”¨è¯„åˆ†åˆ—: {score_col}")
        
        # 1. æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç åˆ—
        df['ä»£ç _normalized'] = df['ä»£ç '].astype(str).str.strip()
        
        # 2. æ£€æµ‹é‡å¤
        duplicates = df[df.duplicated(subset=['ä»£ç _normalized'], keep=False)]
        
        if duplicates.empty:
            # æ¸…ç†ä¸´æ—¶åˆ—
            df = df.drop(columns=['ä»£ç _normalized'])
            return df, {
                'has_duplicates': False,
                'removed_count': 0,
                'removed_details': [],
                'duplicate_codes': []
            }
        
        # 3. å¤„ç†æ‰€æœ‰é‡å¤ï¼ˆæ–°ç­–ç•¥ï¼šä¸€æ¬¡æ€§å¤„ç†å®Œï¼‰
        dup_codes = duplicates['ä»£ç _normalized'].unique()
        logger.warning(f"âš ï¸  æ£€æµ‹åˆ° {len(dup_codes)} ä¸ªé‡å¤çš„è‚¡ç¥¨ä»£ç : {', '.join(dup_codes)}")
        
        # æ˜¾ç¤ºé‡å¤è¯¦æƒ…
        for code in dup_codes:
            dup_rows = df[df['ä»£ç _normalized'] == code]
            rows_info = ', '.join([str(idx + 2) for idx in dup_rows.index])
            logger.warning(f"   è‚¡ç¥¨ {code} å‡ºç°äº† {len(dup_rows)} æ¬¡ï¼Œåœ¨è¡Œ: [{rows_info}]")
        
        # ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰é‡å¤
        indices_to_remove = []
        for code in dup_codes:
            dup_rows = df[df['ä»£ç _normalized'] == code]
            
            if len(dup_rows) > 1:
                # ä¿ç•™æœ€æ¥è¿‘å…¨å±€å‡å€¼çš„ï¼Œåˆ é™¤å…¶ä»–
                removed_idx = self._select_rows_to_remove(df, dup_rows, code)
                indices_to_remove.extend(removed_idx)
        
        # 4. åˆ é™¤å¼‚å¸¸è¡Œ
        if indices_to_remove:
            df_cleaned = df.drop(indices_to_remove).reset_index(drop=True)
            logger.warning(f"âœ… æ™ºèƒ½å»é‡å®Œæˆï¼šåˆ é™¤ {len(indices_to_remove)} æ¡å¼‚å¸¸è®°å½•")
        else:
            df_cleaned = df
            logger.warning(f"âš ï¸  å»é‡æ¡ä»¶ä¸æ»¡è¶³ï¼šä¿ç•™æ‰€æœ‰é‡å¤è®°å½•ï¼ˆå°†åœ¨åç»­ä¸¥æ ¼æ£€æŸ¥ä¸­æŠ¥é”™ï¼‰")
        
        # 5. æ¸…ç†ä¸´æ—¶åˆ—
        if 'ä»£ç _normalized' in df_cleaned.columns:
            df_cleaned = df_cleaned.drop(columns=['ä»£ç _normalized'])
        
        return df_cleaned, {
            'has_duplicates': True,
            'duplicate_codes': list(dup_codes),
            'removed_count': self.removed_count,
            'removed_details': self.removed_details
        }
    
    def _select_rows_to_remove(
        self, 
        df: pd.DataFrame, 
        dup_rows: pd.DataFrame, 
        code: str
    ) -> List[int]:
        """
        é€‰æ‹©è¦åˆ é™¤çš„è¡Œï¼ˆåŸºäºå…¨å±€ç¦»ç¾¤å€¼æ£€æµ‹ï¼‰
        
        ç­–ç•¥ï¼š
        1. è®¡ç®—å…¨å±€åˆ†æ•°çš„å‡å€¼å’Œæ ‡å‡†å·®
        2. å¯¹äºé‡å¤çš„è®°å½•ï¼Œä¿ç•™æœ€æ¥è¿‘å…¨å±€å‡å€¼çš„é‚£æ¡
        3. åˆ é™¤å…¶ä»–æ˜æ˜¾åç¦»çš„
        
        Args:
            df: å®Œæ•´DataFrame
            dup_rows: é‡å¤çš„è¡Œ
            code: è‚¡ç¥¨ä»£ç 
        
        Returns:
            è¦åˆ é™¤çš„ç´¢å¼•åˆ—è¡¨
        """
        indices_to_remove = []
        score_col = self.score_column  # ä½¿ç”¨ä¹‹å‰æ£€æµ‹åˆ°çš„åˆ—å
        
        # === è®¡ç®—å…¨å±€ç»Ÿè®¡ ===
        global_scores = df[score_col].dropna()
        if len(global_scores) == 0:
            return indices_to_remove
        
        global_mean = global_scores.mean()
        global_std = global_scores.std()
        
        # === è·å–é‡å¤è¡Œçš„ä¿¡æ¯ ===
        dup_info = []
        for idx, row in dup_rows.iterrows():
            rank = idx + 1  # Excelè¡Œå·ï¼ˆ1-basedï¼‰
            total_score = row.get(score_col, None)
            name = row.get('åç§°', 'N/A')
            
            # è®¡ç®—ä¸å…¨å±€å‡å€¼çš„è·ç¦»
            if pd.notna(total_score):
                distance_from_mean = abs(total_score - global_mean)
                # Z-scoreï¼šæ ‡å‡†åŒ–åç¦»åº¦
                z_score = distance_from_mean / global_std if global_std > 0 else 0
            else:
                distance_from_mean = float('inf')
                z_score = float('inf')
            
            dup_info.append({
                'index': idx,
                'rank': rank,
                'total_score': total_score,
                'name': name,
                'distance_from_mean': distance_from_mean,
                'z_score': z_score
            })
        
        # === æ–°ç­–ç•¥ï¼šä¿ç•™æœ€æ¥è¿‘å…¨å±€å‡å€¼çš„ï¼Œåˆ é™¤å…¶ä»– ===
        # 1. æ£€æŸ¥åˆ†æ•°æ˜¯å¦å­˜åœ¨
        scores = [d['total_score'] for d in dup_info if pd.notna(d['total_score'])]
        if len(scores) < 2:
            # æ— æ³•æ¯”è¾ƒï¼Œä¿ç•™æ‰€æœ‰ï¼ˆäº¤ç»™æ•°æ®åº“æŠ¥é”™ï¼‰
            logger.warning(f"  è‚¡ç¥¨ {code} é‡å¤ä½†ç¼ºå°‘åˆ†æ•°æ•°æ®ï¼Œä¿ç•™æ‰€æœ‰è¡Œï¼ˆå°†è§¦å‘æ•°æ®åº“ERRORï¼‰")
            return indices_to_remove
        
        # 2. æ‰¾å‡ºæœ€æ¥è¿‘å…¨å±€å‡å€¼çš„è®°å½•ï¼ˆä¿ç•™å®ƒï¼‰
        closest_to_mean = min(dup_info, key=lambda x: x['distance_from_mean'])
        
        # 3. åˆ é™¤å…¶ä»–æ‰€æœ‰è®°å½•
        for info in dup_info:
            if info['index'] != closest_to_mean['index']:
                indices_to_remove.append(info['index'])
                
                self.removed_count += 1
                detail = {
                    'code': code,
                    'name': info['name'],
                    'rank': info['rank'],
                    'total_score': info['total_score'],
                    'global_mean': global_mean,
                    'distance_from_mean': info['distance_from_mean'],
                    'z_score': info['z_score'],
                    'reason': f'é‡å¤è®°å½•ä¸­åç¦»å…¨å±€å‡å€¼è¾ƒè¿œï¼ˆä¿ç•™æœ€æ¥è¿‘å‡å€¼çš„ï¼‰'
                }
                self.removed_details.append(detail)
                
                logger.warning(
                    f"  [å»é‡] è‚¡ç¥¨ {code}({info['name']}) "
                    f"è¡Œ{info['rank']} "
                    f"åˆ†æ•°={info['total_score']:.2f} "
                    f"å…¨å±€å‡å€¼={global_mean:.2f} "
                    f"è·ç¦»={info['distance_from_mean']:.2f} "
                    f"Z-score={info['z_score']:.2f} â†’ åˆ é™¤ï¼ˆä¿ç•™è¡Œ{closest_to_mean['rank']}ï¼‰"
                )
        
        return indices_to_remove


def print_dedup_summary(dedup_stats: dict):
    """æ‰“å°å»é‡æ‘˜è¦"""
    if not dedup_stats.get('has_duplicates'):
        return
    
    removed_count = dedup_stats.get('removed_count', 0)
    
    if removed_count == 0:
        logger.warning("ğŸ“Š æ•°æ®å»é‡æ‘˜è¦")
        logger.warning("  æœªåˆ é™¤ä»»ä½•è®°å½•ï¼ˆå»é‡æ¡ä»¶ä¸æ»¡è¶³ï¼‰")
        return
    
    logger.warning("")
    logger.warning("=" * 70)
    logger.warning("ğŸ“Š æ•°æ®å»é‡æ‘˜è¦ï¼ˆåŸºäºå…¨å±€ç¦»ç¾¤å€¼æ£€æµ‹ï¼‰")
    logger.warning("=" * 70)
    logger.warning(f"å»é‡æ¡æ•°: {removed_count}")
    
    for detail in dedup_stats.get('removed_details', []):
        logger.warning(
            f"  â€¢ è‚¡ç¥¨ {detail['code']}({detail['name']}) "
            f"è¡Œ{detail['rank']} åŸå› : {detail['reason']}"
        )
        
        # å…¼å®¹æ–°æ—§æ ¼å¼
        if 'global_mean' in detail:
            # æ–°æ ¼å¼ï¼šå…¨å±€å‡å€¼
            logger.warning(
                f"    åˆ†æ•°={detail['total_score']:.2f}, "
                f"å…¨å±€å‡å€¼={detail['global_mean']:.2f}, "
                f"è·ç¦»={detail['distance_from_mean']:.2f}, "
                f"Z-score={detail['z_score']:.2f}"
            )
        else:
            # æ—§æ ¼å¼ï¼šå‘¨å›´å‡å€¼ï¼ˆå‘åå…¼å®¹ï¼‰
            logger.warning(
                f"    åˆ†æ•°={detail['total_score']:.2f}, "
                f"å‘¨å›´å‡å€¼={detail.get('context_mean', 'N/A'):.2f}, "
                f"åç¦»={detail.get('deviation', 'N/A'):.2f}Ïƒ"
            )
    
    logger.warning("=" * 70)
    logger.warning("")
