#!/usr/bin/env python3
"""
å¤–éƒ¨æ¿å—æ•°æ®åŒæ­¥è„šæœ¬
============================
ä» AKShare è·å–ä¸œæ–¹è´¢å¯Œ(EM)å’ŒåŒèŠ±é¡º(THS)çš„æ¿å—æ•°æ®ï¼Œå†™å…¥ ext_* è¡¨

åŠŸèƒ½:
1. åŒæ­¥æ¿å—åˆ—è¡¨ -> ext_board_list
2. åŒæ­¥æ¿å—æˆåˆ†è‚¡ -> ext_board_daily_snap
3. è‡ªåŠ¨æ˜ å°„åˆ°æœ¬åœ°æ¿å— -> ext_board_local_map
4. çŠ¶æ€ç®¡ç†å’Œå¹‚ç­‰æ£€æŸ¥

ä½¿ç”¨æ–¹æ³•:
    python sync_ext_boards.py                    # åŒæ­¥ä»Šå¤©æ•°æ®
    python sync_ext_boards.py --date 2025-01-10  # åŒæ­¥æŒ‡å®šæ—¥æœŸ
    python sync_ext_boards.py --provider em      # åªåŒæ­¥ä¸œè´¢
    python sync_ext_boards.py --provider ths     # åªåŒæ­¥åŒèŠ±é¡º
    python sync_ext_boards.py --skip-cons        # åªåŒæ­¥æ¿å—åˆ—è¡¨ï¼Œè·³è¿‡æˆåˆ†è‚¡
    python sync_ext_boards.py --force            # å¼ºåˆ¶åŒæ­¥ï¼ˆå¿½ç•¥å¹‚ç­‰æ£€æŸ¥ï¼‰

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-12-10
ç‰ˆæœ¬: v1.1.0 (æ·»åŠ çŠ¶æ€ç®¡ç†)
"""

import argparse
import json
import logging
import os
import sys
import time
from datetime import datetime, date
from pathlib import Path
from typing import Optional, List, Dict, Any, Callable

import random
import requests
import akshare as ak
import pandas as pd
import pywencai  # åŒèŠ±é¡ºé—®è´¢ APIï¼ˆæ›¿ä»£å·²å¤±æ•ˆçš„ AKShare æ¥å£ï¼‰
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, scoped_session
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ============================================================
# 91HTTP ä»£ç†ç®¡ç†å™¨
# ============================================================
class ProxyManager:
    """91HTTP éš§é“ä»£ç†ç®¡ç†å™¨ï¼Œè‡ªåŠ¨è·å–å’Œè½®æ¢ä»£ç† IPï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    
    API_URL = "http://api.91http.com/v1/get-ip"
    
    def __init__(self, trade_no: str = None, secret: str = None, max_ips: int = 200):
        self.trade_no = trade_no or "B200987778609"
        self.secret = secret or "7GJdQkL6G93PcR9o"
        self.max_ips = max_ips  # IP ä½¿ç”¨ä¸Šé™
        self._lock = threading.Lock()  # çº¿ç¨‹é”
        self._proxy_pool: Dict[int, Dict] = {}  # çº¿ç¨‹ID -> ä»£ç†ä¿¡æ¯
        self._total_ips_used: int = 0  # ç»Ÿè®¡ç”¨äº†å¤šå°‘ä¸ª IP
        self._exhausted: bool = False  # æ˜¯å¦å·²è€—å°½é…é¢
    
    def get_proxy(self, force_refresh: bool = False) -> Optional[Dict[str, str]]:
        """è·å–ä»£ç†ï¼Œæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹çš„ä»£ç†ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if self._exhausted:
            return None
        
        thread_id = threading.get_ident()
        now = time.time()
        
        with self._lock:
            # æ£€æŸ¥å½“å‰çº¿ç¨‹æ˜¯å¦æœ‰ä»£ç†
            if thread_id in self._proxy_pool:
                proxy_info = self._proxy_pool[thread_id]
                if not force_refresh and now < proxy_info['expire_time']:
                    return proxy_info['proxy']
            
            # éœ€è¦æ–°ä»£ç†ï¼Œæ£€æŸ¥é…é¢
            if self._total_ips_used >= self.max_ips:
                if not self._exhausted:
                    logger.warning(f"âš ï¸ å·²è¾¾åˆ° IP ä½¿ç”¨ä¸Šé™ ({self.max_ips})ï¼Œåœæ­¢è·å–æ–°ä»£ç†")
                    self._exhausted = True
                return None
            
            # è·å–æ–°ä»£ç†
            proxy = self._fetch_new_proxy()
            if proxy:
                self._proxy_pool[thread_id] = {
                    'proxy': proxy,
                    'expire_time': now + 50  # 50ç§’æœ‰æ•ˆæœŸ
                }
                return proxy
            return None
    
    def _fetch_new_proxy(self) -> Optional[Dict[str, str]]:
        """ä» 91HTTP è·å–æ–°çš„ä»£ç† IPï¼ˆå†…éƒ¨è°ƒç”¨ï¼Œå·²åŠ é”ï¼‰
        
        åŸºäºæ¼æ¡¶åŸç†çš„æµé‡æ•´å½¢ (Traffic Shaping)
        å…¬å¼: WaitTime = Max(0, MinInterval - (Now - LastTime))
        MIN_INTERVAL=2.0ç§’ æä¾› 300% å†—ä½™ï¼Œå½»åº•é¿å…ä»£ç†å•†é™æµ
        """
        MIN_INTERVAL = 2.0  # æ™ºèƒ½å†·å´é—´éš”ï¼ˆç§’ï¼‰
        
        now = time.time()
        last_time = getattr(self, '_last_fetch_time', 0)
        elapsed = now - last_time
        wait_time = MIN_INTERVAL - elapsed
        
        if wait_time > 0:
            # è¿˜æ²¡å†·é€ï¼Œå¼ºåˆ¶ç¡å¤Ÿæ—¶é—´
            time.sleep(wait_time)
        
        # æ›´æ–°æœ€åæå–æ—¶é—´
        self._last_fetch_time = time.time()
        
        try:
            params = {
                'trade_no': self.trade_no,
                'secret': self.secret,
                'num': 1,
                'protocol': 1,
                'format': 'text',
                'sep': 1,
                'filter': 1,
                'auto_white': 1
            }
            r = requests.get(self.API_URL, params=params, timeout=10)
            proxy_ip = r.text.strip()
            
            if proxy_ip and ':' in proxy_ip and not proxy_ip.startswith('{'):
                self._total_ips_used += 1
                logger.info(f"è·å–æ–°ä»£ç† #{self._total_ips_used}/{self.max_ips}: {proxy_ip}")
                return {
                    'http': f'http://{proxy_ip}',
                    'https': f'http://{proxy_ip}'
                }
            else:
                logger.warning(f"è·å–ä»£ç†å¤±è´¥: {proxy_ip}")
                return None
        except Exception as e:
            logger.error(f"è·å–ä»£ç†å¼‚å¸¸: {e}")
            return None
    
    def mark_failed(self):
        """æ ‡è®°å½“å‰çº¿ç¨‹çš„ä»£ç†å¤±è´¥ï¼Œå¼ºåˆ¶åˆ·æ–°"""
        thread_id = threading.get_ident()
        with self._lock:
            if thread_id in self._proxy_pool:
                del self._proxy_pool[thread_id]
    
    def is_exhausted(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è€—å°½é…é¢"""
        return self._exhausted
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_ips_used': self._total_ips_used,
            'max_ips': self.max_ips,
            'exhausted': self._exhausted,
            'active_threads': len(self._proxy_pool)
        }

# å…¨å±€ä»£ç†ç®¡ç†å™¨
_proxy_manager: Optional[ProxyManager] = None

def get_proxy_manager() -> Optional[ProxyManager]:
    return _proxy_manager

def set_proxy_manager(manager: ProxyManager):
    global _proxy_manager
    _proxy_manager = manager

# ============================================================
# ç›´æ¥è°ƒç”¨ä¸œè´¢ APIï¼ˆç»•è¿‡ AKShare çš„é‡å¤è¯·æ±‚ï¼‰
# ============================================================
class EastMoneyAPI:
    """ä¸œæ–¹è´¢å¯Œç›´è¿ APIï¼Œæ”¯æŒä»£ç†"""
    
    BASE_URL = "https://push2.eastmoney.com/api/qt/clist/get"
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://quote.eastmoney.com/center/gridlist.html",
        "Accept": "*/*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Connection": "close",
    }
    
    def __init__(self):
        self._industry_code_map: Dict[str, str] = {}
        self._concept_code_map: Dict[str, str] = {}
    
    def cache_board_codes(self, board_type: str, df: pd.DataFrame):
        """ç¼“å­˜æ¿å—ä»£ç æ˜ å°„"""
        code_map = {}
        for _, row in df.iterrows():
            name = str(row.get('æ¿å—åç§°', row.get('åç§°', row.get('name', ''))))
            code = str(row.get('æ¿å—ä»£ç ', row.get('ä»£ç ', row.get('code', ''))))
            if name and code:
                code_map[name] = code
        
        if board_type == 'industry':
            self._industry_code_map = code_map
        else:
            self._concept_code_map = code_map
        
        return len(code_map)
    
    def fetch_board_list(self, board_type: str = 'industry') -> Optional[pd.DataFrame]:
        """ç›´æ¥è¯·æ±‚ä¸œè´¢ API è·å–æ¿å—åˆ—è¡¨ï¼ˆèµ°ä»£ç†ï¼Œæ”¯æŒåˆ†é¡µï¼Œå¸¦é‡è¯•ï¼‰"""
        # è¡Œä¸šæ¿å—å’Œæ¦‚å¿µæ¿å—çš„ API å‚æ•°ä¸åŒ
        if board_type == 'industry':
            fs = "m:90+t:2+f:!50"
        else:
            fs = "m:90+t:3+f:!50"
        
        all_records = []
        page = 1
        page_size = 100  # æ¯é¡µ100æ¡ï¼Œç¡®ä¿è·å–å®Œæ•´æ•°æ®
        
        try:
            proxy_mgr = get_proxy_manager()
            
            while True:
                # æ¯é¡µè¯·æ±‚å¸¦é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
                page_success = False
                for retry in range(3):
                    try:
                        # æ¯æ¬¡é‡è¯•éƒ½è·å–æ–°ä»£ç†
                        proxies = proxy_mgr.get_proxy() if proxy_mgr else None
                        
                        params = {
                            "pn": str(page),
                            "pz": str(page_size),
                            "po": "1",
                            "np": "1",
                            "ut": "bd1d9ddb04089700cf9c27f6f7426281",
                            "fltt": "2",
                            "invt": "2",
                            "fid": "f3",
                            "fs": fs,
                            "fields": "f12,f14,f3"
                        }
                        
                        with requests.Session() as s:
                            s.trust_env = False
                            r = s.get(self.BASE_URL, params=params, headers=self.HEADERS, 
                                     proxies=proxies, timeout=15)
                            r.raise_for_status()
                            data = r.json()
                        
                        diff = data.get("data", {}).get("diff") or []
                        if not diff:
                            page_success = True
                            break
                        
                        for item in diff:
                            all_records.append({
                                "æ¿å—ä»£ç ": item.get("f12", ""),
                                "æ¿å—åç§°": item.get("f14", ""),
                                "æ¶¨è·Œå¹…": item.get("f3", 0),
                            })
                        
                        page_success = True
                        break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        
                    except Exception as e:
                        if proxy_mgr:
                            proxy_mgr.mark_failed()
                        logger.warning(f"è·å–{board_type}æ¿å—ç¬¬{page}é¡µå¤±è´¥(é‡è¯•{retry+1}/3): {str(e)[:50]}")
                        time.sleep(2 + retry)  # é€’å¢å»¶è¿Ÿ
                
                if not page_success:
                    logger.error(f"è·å–{board_type}æ¿å—ç¬¬{page}é¡µå¤±è´¥ï¼Œè·³è¿‡åç»­é¡µé¢")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                total = data.get("data", {}).get("total", 0)
                if page * page_size >= total:
                    break
                
                page += 1
                time.sleep(1)  # åˆ†é¡µé—´å»¶è¿Ÿå¢åŠ åˆ°1ç§’
            
            if not all_records:
                return pd.DataFrame()
            
            df = pd.DataFrame(all_records)
            # è‡ªåŠ¨ç¼“å­˜
            self.cache_board_codes(board_type, df)
            logger.info(f"è·å–åˆ° {len(df)} ä¸ª{board_type}æ¿å—")
            return df
            
        except Exception as e:
            proxy_mgr = get_proxy_manager()
            if proxy_mgr:
                proxy_mgr.mark_failed()
            raise Exception(f"è·å–æ¿å—åˆ—è¡¨å¤±è´¥: {e}")
    
    def fetch_board_cons(self, board_name: str, board_type: str = 'industry') -> Optional[pd.DataFrame]:
        """ç›´æ¥è¯·æ±‚ä¸œè´¢ API è·å–æ¿å—æˆåˆ†è‚¡ï¼ˆæ”¯æŒåˆ†é¡µï¼Œçªç ´100æ¡é™åˆ¶ï¼‰"""
        code_map = self._industry_code_map if board_type == 'industry' else self._concept_code_map
        bk_code = code_map.get(board_name)
        
        if not bk_code:
            return None
        
        all_records = []
        page = 1
        page_size = 100  # ä¸œè´¢ API æ¯é¡µæœ€å¤šè¿”å› 100 æ¡
        
        try:
            proxy_mgr = get_proxy_manager()
            
            while True:
                params = {
                    "pn": str(page),
                    "pz": str(page_size),
                    "po": "1",
                    "np": "1",
                    "ut": "bd1d9ddb04089700cf9c27f6f7426281",
                    "fltt": "2",
                    "invt": "2",
                    "fid": "f3",
                    "fs": f"b:{bk_code}+f:!50",
                    "fields": "f12,f14,f2,f3,f4,f5,f6,f7"
                }
                
                proxies = proxy_mgr.get_proxy() if proxy_mgr else None
                
                with requests.Session() as s:
                    s.trust_env = False
                    r = s.get(self.BASE_URL, params=params, headers=self.HEADERS, 
                             proxies=proxies, timeout=15)
                    r.raise_for_status()
                    data = r.json()
                
                diff = data.get("data", {}).get("diff") or []
                if not diff:
                    break  # æ²¡æœ‰æ›´å¤šæ•°æ®
                
                for item in diff:
                    all_records.append({
                        "ä»£ç ": item.get("f12", ""),
                        "åç§°": item.get("f14", ""),
                        "æœ€æ–°ä»·": item.get("f2", 0),
                        "æ¶¨è·Œå¹…": item.get("f3", 0),
                        "æ¶¨è·Œé¢": item.get("f4", 0),
                        "æˆäº¤é‡": item.get("f5", 0),
                        "æˆäº¤é¢": item.get("f6", 0),
                        "æŒ¯å¹…": item.get("f7", 0),
                    })
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                total = data.get("data", {}).get("total", 0)
                if page * page_size >= total:
                    break  # å·²è·å–å…¨éƒ¨æ•°æ®
                
                page += 1
                time.sleep(0.1)  # åˆ†é¡µè¯·æ±‚é—´çŸ­æš‚å»¶è¿Ÿ
            
            if not all_records:
                return pd.DataFrame()
            
            return pd.DataFrame(all_records)
            
        except Exception as e:
            # æ ‡è®°ä»£ç†å¤±è´¥
            if proxy_mgr and ("RemoteDisconnected" in str(e) or "Connection" in str(e)):
                proxy_mgr.mark_failed()
            raise Exception(f"ä¸œè´¢APIè¯·æ±‚å¤±è´¥: {e}")

# å…¨å±€ä¸œè´¢ API å®ä¾‹
_em_api = EastMoneyAPI()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“è¿æ¥ (ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–)
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

# å°è¯•ä» app.database è·å–æ•°æ®åº“ URL
DATABASE_URL = None
try:
    # æ·»åŠ  backend ç›®å½•åˆ°è·¯å¾„
    backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if backend_dir not in sys.path:
        sys.path.insert(0, backend_dir)
    from app.database import DATABASE_URL as APP_DB_URL
    DATABASE_URL = APP_DB_URL
    logger.info("ä½¿ç”¨ app.database ä¸­çš„æ•°æ®åº“é…ç½®")
except ImportError:
    pass

# å¦‚æœæ²¡æœ‰ä» app è·å–åˆ°ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡æ„å»º
if not DATABASE_URL:
    DATABASE_URL = os.getenv('DATABASE_URL') or os.getenv('DB_URL')
    
    if not DATABASE_URL:
        db_host = os.getenv('DB_HOST', 'localhost')
        db_port = os.getenv('DB_PORT', '5432')
        db_name = os.getenv('DB_NAME', 'stock_analysis')
        db_user = os.getenv('DB_USER', 'postgres')
        db_password = os.getenv('DB_PASSWORD', 'password')
        DATABASE_URL = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'

# æ•°æ®ç›®å½•
try:
    from app.config import DATA_DIR
except ImportError:
    DATA_DIR = os.path.join(os.path.dirname(__file__), '..', '..', 'data')


def find_latest_trade_date(engine, target_date: date) -> Optional[date]:
    sql = """
    SELECT MAX(date) FROM daily_stock_data
    WHERE rank IS NOT NULL AND date <= :d
    """
    with engine.connect() as conn:
        row = conn.execute(text(sql), {"d": target_date}).fetchone()
        return row[0] if row and row[0] else None


# ============================================================
# çŠ¶æ€ç®¡ç†å™¨
# ============================================================

class ExtBoardStateManager:
    """å¤–éƒ¨æ¿å—åŒæ­¥çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self, state_file: str = "ext_board_sync_state.json"):
        self.state_file = Path(DATA_DIR) / state_file
        self.state = self._load_state()
    
    def _load_state(self) -> Dict:
        """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"çŠ¶æ€æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {e}")
        return self._create_empty_state()
    
    def _create_empty_state(self) -> Dict:
        """åˆ›å»ºç©ºçŠ¶æ€"""
        return {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "syncs": {},
            "config": {
                "auto_sync_on_import": True,
                "sync_providers": ["em", "ths"],
                "skip_cons_if_slow": False
            }
        }
    
    def _save_state(self):
        """ä¿å­˜çŠ¶æ€"""
        self.state["last_updated"] = datetime.now().isoformat()
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, ensure_ascii=False, indent=2)
    
    def is_synced_today(self, date_str: str, provider: str = None) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å·²åŒæ­¥æˆåŠŸ
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            provider: å¯é€‰ï¼ŒæŒ‡å®šæ•°æ®æºã€‚å¦‚æœæŒ‡å®šï¼Œåˆ™åªæ£€æŸ¥è¯¥ provider çš„çŠ¶æ€
        """
        sync_info = self.state["syncs"].get(date_str)
        if sync_info is None:
            return False
        
        if provider:
            # æ£€æŸ¥æŒ‡å®š provider çš„çŠ¶æ€
            provider_info = sync_info.get("providers", {}).get(provider)
            return provider_info is not None and provider_info.get("status") == "success"
        else:
            # æ£€æŸ¥æ•´ä½“çŠ¶æ€
            return sync_info.get("status") == "success"
    
    def start_sync(self, date_str: str, provider: str = None):
        """å¼€å§‹åŒæ­¥ï¼ˆä¸è¦†ç›–å·²æœ‰ provider æ•°æ®ï¼‰"""
        if date_str not in self.state["syncs"]:
            # æ–°æ—¥æœŸï¼Œåˆ›å»ºæ–°è®°å½•
            self.state["syncs"][date_str] = {
                "status": "in_progress",
                "start_time": datetime.now().isoformat(),
                "providers": {},
                "metrics": {}
            }
        else:
            # å·²æœ‰è®°å½•ï¼Œåªæ›´æ–°çŠ¶æ€ï¼Œä¿ç•™ providers æ•°æ®
            self.state["syncs"][date_str]["status"] = "in_progress"
            # ä¸è¦†ç›– start_time å’Œ providers
        self._save_state()
    
    def update_provider_status(self, date_str: str, provider: str, 
                                board_count: int, cons_count: int, 
                                status: str, duration: float,
                                failed_boards: List[str] = None):
        """æ›´æ–°æ•°æ®æºçŠ¶æ€"""
        if date_str in self.state["syncs"]:
            self.state["syncs"][date_str]["providers"][provider] = {
                "board_count": board_count,
                "cons_count": cons_count,
                "status": status,
                "duration_seconds": round(duration, 2),
                "failed_boards": failed_boards or []
            }
            self._save_state()
    
    def mark_success(self, date_str: str, metrics: Dict):
        """æ ‡è®°åŒæ­¥æˆåŠŸ"""
        if date_str in self.state["syncs"]:
            self.state["syncs"][date_str].update({
                "status": "success",
                "end_time": datetime.now().isoformat(),
                "metrics": metrics
            })
            # è®¡ç®—æ€»è€—æ—¶
            start = datetime.fromisoformat(self.state["syncs"][date_str]["start_time"])
            end = datetime.fromisoformat(self.state["syncs"][date_str]["end_time"])
            self.state["syncs"][date_str]["duration_seconds"] = round((end - start).total_seconds(), 2)
            self._save_state()
    
    def mark_failed(self, date_str: str, error: str):
        """æ ‡è®°åŒæ­¥å¤±è´¥"""
        if date_str in self.state["syncs"]:
            self.state["syncs"][date_str].update({
                "status": "failed",
                "end_time": datetime.now().isoformat(),
                "error": error
            })
            self._save_state()
    
    def get_sync_info(self, date_str: str) -> Optional[Dict]:
        """è·å–åŒæ­¥ä¿¡æ¯"""
        return self.state["syncs"].get(date_str)


class ExtBoardSyncer:
    """å¤–éƒ¨æ¿å—æ•°æ®åŒæ­¥å™¨"""
    
    def __init__(self, db_url: str = DATABASE_URL, delay: float = 1.5):
        self.engine = create_engine(db_url)
        Session = sessionmaker(bind=self.engine)
        self.session = Session()
        
        # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.delay = delay
        self.delay_ths = delay * 1.5  # åŒèŠ±é¡ºé™åˆ¶æ›´ä¸¥æ ¼
        
        # ç¼“å­˜ provider_id
        self._provider_cache: Dict[str, int] = {}
        self._load_providers()
        
        # ç¼“å­˜å·²æœ‰è‚¡ç¥¨ä»£ç 
        self._stock_codes: set = set()
        self._load_stock_codes()
    
    def _load_providers(self):
        """åŠ è½½æ•°æ®æº ID"""
        result = self.session.execute(text("SELECT id, code FROM ext_providers"))
        for row in result:
            self._provider_cache[row.code] = row.id
        logger.info(f"å·²åŠ è½½æ•°æ®æº: {self._provider_cache}")
    
    def _load_stock_codes(self):
        """åŠ è½½å·²æœ‰è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè¿‡æ»¤æ— æ•ˆæˆåˆ†è‚¡ï¼‰"""
        result = self.session.execute(text("SELECT stock_code FROM stocks"))
        self._stock_codes = {row.stock_code for row in result}
        logger.info(f"å·²åŠ è½½ {len(self._stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
    
    def normalize_stock_code(self, raw_code: str) -> Optional[str]:
        """
        æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼Œä½¿å…¶ä¸ stocks è¡¨ä¸€è‡´
        
        è¾“å…¥æ ¼å¼å¯èƒ½æ˜¯:
        - 000001 (çº¯æ•°å­—)
        - 000001.SZ / 000001.SH (å¸¦åç¼€)
        - sz000001 / sh000001 (å¸¦å‰ç¼€)
        - S00001 (å¸¦Så‰ç¼€çš„æ—§æ ¼å¼)
        
        è¾“å‡ºæ ¼å¼: ä¸ stocks.stock_code ä¸€è‡´
        """
        if not raw_code:
            return None
        
        code = str(raw_code).strip().upper()
        
        # ç§»é™¤å¸¸è§å‰åç¼€
        for suffix in ['.SZ', '.SH', '.BJ']:
            if code.endswith(suffix):
                code = code[:-3]
                break
        
        for prefix in ['SZ', 'SH', 'BJ']:
            if code.startswith(prefix):
                code = code[2:]
                break
        
        # å¤„ç† S å‰ç¼€ (æ—§æ ¼å¼å¦‚ S00001)
        if code.startswith('S') and len(code) == 6:
            code = code[1:]
        
        # ç¡®ä¿æ˜¯6ä½æ•°å­—
        if len(code) == 6 and code.isdigit():
            # æ£€æŸ¥æ˜¯å¦åœ¨å·²çŸ¥è‚¡ç¥¨åˆ—è¡¨ä¸­
            # å°è¯•å¤šç§æ ¼å¼åŒ¹é…
            for fmt in [code, f'S{code[1:]}', f's{code[1:]}']:
                if fmt in self._stock_codes:
                    return fmt
            
            # å¦‚æœéƒ½æ²¡åŒ¹é…ä¸Šï¼Œè¿”å›åŸå§‹6ä½ä»£ç ï¼ˆå¯èƒ½æ˜¯æ–°è‚¡ï¼‰
            return code
        
        return None
    
    # ==================== ä¸œæ–¹è´¢å¯Œ (EM) ====================
    
    def sync_em_boards(self, target_date: date, skip_cons: bool = False):
        """åŒæ­¥ä¸œæ–¹è´¢å¯Œæ¿å—æ•°æ®"""
        provider_id = self._provider_cache.get('em')
        if not provider_id:
            logger.error("æœªæ‰¾åˆ° em æ•°æ®æºé…ç½®")
            return
        
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŒæ­¥ä¸œæ–¹è´¢å¯Œæ¿å—æ•°æ®...")
        
        # 1. åŒæ­¥è¡Œä¸šæ¿å—
        self._sync_em_industry_boards(provider_id)
        
        # 2. åŒæ­¥æ¦‚å¿µæ¿å—
        self._sync_em_concept_boards(provider_id)
        
        # 3. åŒæ­¥æˆåˆ†è‚¡
        if not skip_cons:
            self._sync_em_board_cons(provider_id, target_date, 'industry')
            self._sync_em_board_cons(provider_id, target_date, 'concept')
        
        self.session.commit()
        logger.info("ä¸œæ–¹è´¢å¯Œæ¿å—æ•°æ®åŒæ­¥å®Œæˆ")
    
    def sync_em_boards_with_metrics(self, target_date: date, skip_cons: bool = False) -> tuple:
        """åŒæ­¥ä¸œæ–¹è´¢å¯Œæ¿å—æ•°æ®ï¼ˆå¸¦è¿ç»´æŒ‡æ ‡ï¼‰
        
        Returns:
            tuple: (æ¿å—æ•°, æˆåˆ†è‚¡æ•°, å¤±è´¥æ¿å—åˆ—è¡¨)
        """
        provider_id = self._provider_cache.get('em')
        if not provider_id:
            logger.error("æœªæ‰¾åˆ° em æ•°æ®æºé…ç½®")
            return 0, 0, []
        
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŒæ­¥ä¸œæ–¹è´¢å¯Œæ¿å—æ•°æ®...")
        
        total_boards = 0
        total_cons = 0
        all_failed = []
        
        # 1. åŒæ­¥è¡Œä¸šæ¿å—
        industry_count = self._sync_em_industry_boards_with_count(provider_id)
        total_boards += industry_count
        
        # 2. åŒæ­¥æ¦‚å¿µæ¿å—
        concept_count = self._sync_em_concept_boards_with_count(provider_id)
        total_boards += concept_count
        
        # 3. åŒæ­¥æˆåˆ†è‚¡ï¼ˆæ”¯æŒ board_type å’Œ limit å‚æ•°ï¼‰
        if not skip_cons:
            board_type = getattr(self, '_board_type', 'all')
            limit = getattr(self, '_limit', None)
            
            # å¹¶å‘æ¨¡å¼å‚æ•°
            use_concurrent = getattr(self, '_concurrent', False)
            workers = getattr(self, '_workers', 10)
            ip_ttl = getattr(self, '_ip_ttl', 50.0)  # IP å¤ç”¨æ—¶é—´ï¼ˆç§’ï¼‰
            req_delay_min = getattr(self, '_req_delay_min', 1.0)
            req_delay_max = getattr(self, '_req_delay_max', 3.0)
            
            if board_type in ('industry', 'all'):
                if use_concurrent:
                    cons1, failed1 = self._sync_em_board_cons_concurrent(
                        provider_id, target_date, 'industry', limit, workers, ip_ttl,
                        req_delay_min, req_delay_max
                    )
                else:
                    cons1, failed1 = self._sync_em_board_cons_with_metrics(provider_id, target_date, 'industry', limit)
                total_cons += cons1
                all_failed.extend(failed1)
            
            if board_type in ('concept', 'all'):
                if use_concurrent:
                    cons2, failed2 = self._sync_em_board_cons_concurrent(
                        provider_id, target_date, 'concept', limit, workers, ip_ttl,
                        req_delay_min, req_delay_max
                    )
                else:
                    cons2, failed2 = self._sync_em_board_cons_with_metrics(provider_id, target_date, 'concept', limit)
                total_cons += cons2
                all_failed.extend(failed2)
        
        self.session.commit()
        logger.info("ä¸œæ–¹è´¢å¯Œæ¿å—æ•°æ®åŒæ­¥å®Œæˆ")
        
        return total_boards, total_cons, all_failed
    
    def _sync_em_industry_boards_with_count(self, provider_id: int) -> int:
        """åŒæ­¥ä¸œè´¢è¡Œä¸šæ¿å—åˆ—è¡¨ï¼ˆè¿”å›æ•°é‡ï¼‰"""
        logger.info("è·å–ä¸œè´¢è¡Œä¸šæ¿å—åˆ—è¡¨...")
        try:
            # ä¼˜å…ˆä½¿ç”¨ä»£ç†ç›´è¿ API
            proxy_mgr = get_proxy_manager()
            if proxy_mgr:
                df = _em_api.fetch_board_list('industry')
                logger.info(f"å·²é€šè¿‡ä»£ç†è·å–å¹¶ç¼“å­˜ {len(_em_api._industry_code_map)} ä¸ªè¡Œä¸šæ¿å—ä»£ç ")
            else:
                df = ak.stock_board_industry_name_em()
                if df is not None and not df.empty:
                    cached = _em_api.cache_board_codes('industry', df)
                    logger.info(f"å·²ç¼“å­˜ {cached} ä¸ªè¡Œä¸šæ¿å—ä»£ç ")
            
            if df is None or df.empty:
                logger.warning("ä¸œè´¢è¡Œä¸šæ¿å—åˆ—è¡¨ä¸ºç©º")
                return 0
            
            count = 0
            for _, row in df.iterrows():
                board_code = str(row.get('æ¿å—ä»£ç ', row.get('ä»£ç ', '')))
                board_name = str(row.get('æ¿å—åç§°', row.get('åç§°', '')))
                
                if not board_code or not board_name:
                    continue
                
                self._upsert_board(provider_id, board_code, board_name, 'industry')
                count += 1
            
            self.session.commit()
            logger.info(f"åŒæ­¥ä¸œè´¢è¡Œä¸šæ¿å—: {count} ä¸ª")
            return count
        except Exception as e:
            logger.error(f"åŒæ­¥ä¸œè´¢è¡Œä¸šæ¿å—å¤±è´¥: {e}")
            return 0
    
    def _sync_em_concept_boards_with_count(self, provider_id: int) -> int:
        """åŒæ­¥ä¸œè´¢æ¦‚å¿µæ¿å—åˆ—è¡¨ï¼ˆè¿”å›æ•°é‡ï¼‰"""
        logger.info("è·å–ä¸œè´¢æ¦‚å¿µæ¿å—åˆ—è¡¨...")
        try:
            # ä¼˜å…ˆä½¿ç”¨ä»£ç†ç›´è¿ API
            proxy_mgr = get_proxy_manager()
            if proxy_mgr:
                df = _em_api.fetch_board_list('concept')
                logger.info(f"å·²é€šè¿‡ä»£ç†è·å–å¹¶ç¼“å­˜ {len(_em_api._concept_code_map)} ä¸ªæ¦‚å¿µæ¿å—ä»£ç ")
            else:
                df = ak.stock_board_concept_name_em()
                if df is not None and not df.empty:
                    cached = _em_api.cache_board_codes('concept', df)
                    logger.info(f"å·²ç¼“å­˜ {cached} ä¸ªæ¦‚å¿µæ¿å—ä»£ç ")
            
            if df is None or df.empty:
                logger.warning("ä¸œè´¢æ¦‚å¿µæ¿å—åˆ—è¡¨ä¸ºç©º")
                return 0
            
            count = 0
            for _, row in df.iterrows():
                board_code = str(row.get('æ¿å—ä»£ç ', row.get('ä»£ç ', '')))
                board_name = str(row.get('æ¿å—åç§°', row.get('åç§°', '')))
                
                if not board_code or not board_name:
                    continue
                
                self._upsert_board(provider_id, board_code, board_name, 'concept')
                count += 1
            
            self.session.commit()
            logger.info(f"åŒæ­¥ä¸œè´¢æ¦‚å¿µæ¿å—: {count} ä¸ª")
            return count
        except Exception as e:
            import traceback
            logger.error(f"åŒæ­¥ä¸œè´¢æ¦‚å¿µæ¿å—å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return 0
    
    def _sync_em_board_cons_with_metrics(self, provider_id: int, target_date: date, board_type: str, limit: int = None) -> tuple:
        """åŒæ­¥ä¸œè´¢æ¿å—æˆåˆ†è‚¡ï¼ˆå¸¦è¿ç»´æŒ‡æ ‡ï¼‰
        
        ä½¿ç”¨ç›´è¿ä¸œè´¢ APIï¼Œé¿å… AKShare å†…éƒ¨é‡å¤è¯·æ±‚æ¿å—åˆ—è¡¨
        
        Args:
            limit: é™åˆ¶åŒæ­¥æ¿å—æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
        
        Returns:
            tuple: (æˆåˆ†è‚¡æ•°, å¤±è´¥æ¿å—åˆ—è¡¨)
        """
        logger.info(f"è·å–ä¸œè´¢{board_type}æ¿å—æˆåˆ†è‚¡ï¼ˆç›´è¿APIï¼‰...")
        
        boards = self._get_boards(provider_id, board_type)
        
        # é™åˆ¶æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
        if limit and limit > 0:
            boards = boards[:limit]
            logger.info(f"é™åˆ¶åŒæ­¥å‰ {limit} ä¸ªæ¿å—ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        
        logger.info(f"å…± {len(boards)} ä¸ª{board_type}æ¿å—éœ€è¦åŒæ­¥æˆåˆ†è‚¡")
        
        total_cons = 0
        failed_boards = []
        max_retries = 2
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        pbar = tqdm(boards, desc=f"ä¸œè´¢{board_type}æˆåˆ†è‚¡", unit="æ¿å—", ncols=100)
        
        for board_id, board_name in pbar:
            success = False
            
            for attempt in range(max_retries):
                try:
                    # ä½¿ç”¨ç›´è¿ APIï¼ˆå•æ¬¡ HTTP è¯·æ±‚ï¼Œä¸ä¼šé‡å¤è·å–æ¿å—åˆ—è¡¨ï¼‰
                    df = _em_api.fetch_board_cons(board_name, board_type)
                    
                    if df is None or df.empty:
                        success = True
                        break
                    
                    cons_count = self._save_board_cons(board_id, target_date, df)
                    total_cons += cons_count
                    success = True
                    
                    # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    proxy_mgr = get_proxy_manager()
                    proxy_info = f"IP#{proxy_mgr.get_stats()['total_ips_used']}" if proxy_mgr else ""
                    pbar.set_postfix({"æˆåˆ†è‚¡": total_cons, "å¤±è´¥": len(failed_boards), "ä»£ç†": proxy_info})
                    break
                    
                except Exception as e:
                    error_msg = str(e)
                    # å¤±è´¥æ—¶æ¢ IP é‡è¯•
                    proxy_mgr = get_proxy_manager()
                    if proxy_mgr:
                        proxy_mgr.mark_failed()  # ç«‹å³æ¢ IP
                    
                    if attempt < max_retries - 1:
                        wait_time = 1  # æ¢ IP ååªç­‰ 1 ç§’
                        pbar.set_postfix({"é‡è¯•": f"{attempt+1}/{max_retries}", "æ¢IPä¸­": "..."})
                        time.sleep(wait_time)
                    else:
                        failed_boards.append(board_name)
                        if len(failed_boards) <= 5:
                            logger.warning(f"æ¿å— {board_name} å¤±è´¥: {error_msg[:60]}")
            
            # æ­£å¸¸è¯·æ±‚é—´éš” + éšæœºæŠ–åŠ¨
            jitter = random.random() * 0.3
            time.sleep(self.delay + jitter)
            
            # æ¯50ä¸ªæäº¤ä¸€æ¬¡
            if pbar.n > 0 and pbar.n % 50 == 0:
                self.session.commit()
        
        pbar.close()
        self.session.commit()
        logger.info(f"åŒæ­¥ä¸œè´¢{board_type}æ¿å—æˆåˆ†è‚¡å®Œæˆ: {total_cons} æ¡, å¤±è´¥ {len(failed_boards)} ä¸ª")
        
        return total_cons, failed_boards
    
    def _sync_em_board_cons_concurrent(self, provider_id: int, target_date: date, board_type: str, 
                                        limit: int = None, workers: int = 6, 
                                        ip_ttl: float = 45.0,
                                        req_delay_min: float = 1.0, req_delay_max: float = 2.0) -> tuple:
        """
        [ç»ˆæç‰ˆ] å•é˜Ÿåˆ— + é‡è¯•è®¡æ•° + ç»Ÿä¸€é”™è¯¯å¤„ç† + æ­»ç£•åˆ°100%
        
        æ¶æ„è®¾è®¡ï¼š
        - å•é˜Ÿåˆ—å¾ªç¯ï¼šä»»åŠ¡ç»“æ„ (retry_count, board_id, board_name)
        - å¤±è´¥å›ç‚‰ï¼šretry_count + 1ï¼Œæ”¾å›é˜Ÿåˆ—å°¾éƒ¨
        - è¶…è¿‡ max_retry æ‰è®°ä¸ºæ°¸ä¹…å¤±è´¥
        - 6 çº¿ç¨‹ï¼ˆåˆ©ç‰¹å°”å®šå¾‹ç®—å‡ºçš„é»„é‡‘åˆ†å‰²ç‚¹ï¼‰
        - 2ç§’å†·å´ï¼ˆ300% IP ä¾›ç»™å†—ä½™ï¼‰
        """
        import queue
        
        MAX_RETRY = 10  # å•ä¸ªä»»åŠ¡æœ€å¤§é‡è¯•æ¬¡æ•°
        
        logger.info(f"ğŸš€ å¯åŠ¨æ­»ç£•æ¨¡å¼: {workers}çº¿ç¨‹, TTL={ip_ttl}s, æœ€å¤§é‡è¯•={MAX_RETRY}æ¬¡")
        
        # ============ 1. ä»»åŠ¡è£…è½½ ============
        boards = self._get_boards(provider_id, board_type)
        if limit and limit > 0:
            boards = boards[:limit]
        
        total_tasks = len(boards)
        if total_tasks == 0:
            return 0, []
        
        # ä»»åŠ¡é˜Ÿåˆ—ï¼š(retry_count, board_id, board_name)
        task_queue = queue.Queue()
        result_queue = queue.Queue()
        
        seen = set()
        for b_id, b_name in boards:
            if b_id not in seen:
                seen.add(b_id)
                task_queue.put((0, b_id, b_name))  # åˆå§‹é‡è¯•æ¬¡æ•°ä¸º 0
        
        logger.info(f"å…± {total_tasks} ä¸ª{board_type}æ¿å—ï¼Œç›®æ ‡: 100% æˆåŠŸ")
        
        # ============ 2. ç»Ÿä¸€è¯·æ±‚å‡½æ•°ï¼ˆé›†ä¸­å¤„ç†æ‰€æœ‰é”™è¯¯ï¼‰ ============
        def fetch_board_members(session, board_name: str) -> dict:
            """
            ç»Ÿä¸€çš„è¯·æ±‚+æ ¡éªŒ+è§£æå‡½æ•°
            è¿”å›: {'ok': bool, 'data': list or None, 'error': str or None, 'error_type': str}
            """
            code_map = _em_api._industry_code_map if board_type == 'industry' else _em_api._concept_code_map
            
            # é”™è¯¯ç±»å‹åˆ†ç±»
            if code_map is None:
                return {'ok': False, 'data': None, 'error': 'ä»£ç æ˜ å°„è¡¨æœªåˆå§‹åŒ–', 'error_type': 'init_error'}
            
            bk_code = code_map.get(board_name)
            if not bk_code:
                return {'ok': False, 'data': None, 'error': f'æ— æ¿å—ä»£ç : {board_name}', 'error_type': 'no_code'}
            
            try:
                all_records = []
                page = 1
                
                while True:
                    params = {
                        "pn": str(page), "pz": "100", "po": "1", "np": "1",
                        "ut": "bd1d9ddb04089700cf9c27f6f7426281",
                        "fltt": "2", "invt": "2", "fid": "f3",
                        "fs": f"b:{bk_code}+f:!50", "fields": "f12,f14"
                    }
                    
                    resp = session.get(
                        "https://push2.eastmoney.com/api/qt/clist/get",
                        params=params, timeout=12
                    )
                    resp.raise_for_status()
                    
                    # å®‰å…¨è§£æ JSON
                    try:
                        j = resp.json()
                    except ValueError:
                        return {'ok': False, 'data': None, 'error': 'JSONè§£æå¤±è´¥', 'error_type': 'invalid_json'}
                    
                    data = j.get("data")
                    if not data:
                        return {'ok': False, 'data': None, 'error': 'è¿”å›æ•°æ®ä¸ºç©º', 'error_type': 'empty_data'}
                    
                    diff = data.get("diff")
                    if not diff:
                        break  # æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                    
                    for item in diff:
                        all_records.append({
                            "ä»£ç ": item.get("f12", ""),
                            "åç§°": item.get("f14", "")
                        })
                    
                    total = data.get("total", 0)
                    if page * 100 >= total:
                        break
                    page += 1
                    time.sleep(0.05)
                
                return {'ok': True, 'data': all_records, 'error': None, 'error_type': None}
                
            except requests.exceptions.Timeout:
                return {'ok': False, 'data': None, 'error': 'è¯·æ±‚è¶…æ—¶', 'error_type': 'timeout'}
            except requests.exceptions.ConnectionError as e:
                return {'ok': False, 'data': None, 'error': f'è¿æ¥é”™è¯¯: {str(e)[:30]}', 'error_type': 'connection'}
            except Exception as e:
                return {'ok': False, 'data': None, 'error': f'{str(e)[:40]}', 'error_type': 'unknown'}
        
        # ============ 3. Worker å®šä¹‰ ============
        def worker_routine(worker_id: int):
            t_name = f"W{worker_id}"
            
            # é”™å³°å¯åŠ¨
            time.sleep(random.uniform(0, 5))
            
            session = None
            proxy_start_time = 0
            current_ip = "æ— "
            
            proxy_mgr = get_proxy_manager()
            
            while True:
                # 1. å–ä»»åŠ¡
                try:
                    item = task_queue.get(timeout=5)
                except queue.Empty:
                    continue
                
                if item is None:  # æ¯’ä¸¸
                    task_queue.task_done()
                    break
                
                retry_count, board_id, board_name = item
                
                # 2. è¯·æ±‚å‰ç¡çœ ï¼ˆJust-In-Timeï¼Œä¸æµªè´¹ IPï¼‰
                time.sleep(random.uniform(req_delay_min, req_delay_max))
                
                # 3. IP ç»´æŠ¤
                now = time.time()
                current_ttl = ip_ttl + random.uniform(-5, 5)
                is_expired = (session is None) or (now - proxy_start_time > current_ttl)
                
                if is_expired:
                    if session:
                        try: session.close()
                        except: pass
                        session = None
                    
                    # é¡½å¼ºè·å– IP
                    if proxy_mgr:
                        for p_retry in range(5):
                            if proxy_mgr.is_exhausted():
                                result_queue.put({'type': 'FATAL', 'msg': 'IPé…é¢è€—å°½'})
                                task_queue.task_done()
                                return
                            
                            try:
                                proxies = proxy_mgr.get_proxy()
                                if proxies:
                                    session = requests.Session()
                                    session.proxies = proxies
                                    session.trust_env = False
                                    session.headers.update({
                                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                                        "Connection": "keep-alive"
                                    })
                                    proxy_start_time = time.time()
                                    current_ip = proxies['http'].split('//')[1] if proxies else "æ— "
                                    break
                            except Exception:
                                pass
                            
                            # åŠ¨æ€ç­‰å¾…ï¼š2s, 5s, 5s...
                            time.sleep(2.0 if p_retry == 0 else 5.0)
                    
                    # å¦‚æœå®åœ¨æ‹¿ä¸åˆ° IPï¼Œä»»åŠ¡å›ç‚‰ï¼Œçº¿ç¨‹ä¼‘æ¯
                    if session is None:
                        task_queue.put((retry_count, board_id, board_name))  # åŸæ ·æ”¾å›
                        task_queue.task_done()
                        time.sleep(10)  # ä¼‘æ¯ä¹…ä¸€ç‚¹
                        continue
                
                # 4. æ‰§è¡Œè¯·æ±‚ï¼ˆä½¿ç”¨ç»Ÿä¸€å‡½æ•°ï¼‰
                result = fetch_board_members(session, board_name)
                
                if result['ok']:
                    # æˆåŠŸï¼
                    result_queue.put({
                        'type': 'SUCCESS',
                        'board_id': board_id,
                        'board_name': board_name,
                        'data': result['data'],
                        'ip': current_ip
                    })
                else:
                    # å¤±è´¥å¤„ç†
                    error_type = result['error_type']
                    
                    # é”€æ¯å¯èƒ½åæ‰çš„ session
                    if error_type in ('timeout', 'connection', 'invalid_json', 'unknown', 'empty_data'):
                        if session:
                            session.close()
                        session = None
                        
                        # ã€æ ¸å¿ƒä¿®å¤ã€‘æ˜¾å¼æ ‡è®°å½“å‰ IP å·²å¤±æ•ˆï¼Œé¿å…åƒµå°¸ IP å¾ªç¯ï¼
                        if proxy_mgr:
                            proxy_mgr.mark_failed()
                    
                    if retry_count >= MAX_RETRY:
                        # å½»åº•æ”¾å¼ƒ
                        result_queue.put({
                            'type': 'FAIL_PERM',
                            'board_name': board_name,
                            'error': result['error'],
                            'error_type': error_type,
                            'retries': retry_count
                        })
                    else:
                        # ä»»åŠ¡å›ç‚‰
                        task_queue.put((retry_count + 1, board_id, board_name))
                        result_queue.put({
                            'type': 'RETRY',
                            'board_name': board_name,
                            'retry_count': retry_count + 1,
                            'error_type': error_type
                        })
                
                task_queue.task_done()
            
            if session:
                session.close()
        
        # ============ 4. å¯åŠ¨çº¿ç¨‹ ============
        threads = []
        for i in range(workers):
            t = threading.Thread(target=worker_routine, args=(i,), daemon=True)
            t.start()
            threads.append(t)
        
        # ============ 5. ä¸»çº¿ç¨‹ç›‘æ§ ============
        success_count = 0
        perm_fail_count = 0
        total_cons = 0
        retry_stats = {}  # ç»Ÿè®¡å„ç±»é”™è¯¯
        perm_failed_boards = []
        
        pbar = tqdm(total=total_tasks, desc=f"ä¸œè´¢{board_type}", unit="æ¿å—", ncols=100)
        
        while success_count + perm_fail_count < total_tasks:
            try:
                res = result_queue.get(timeout=2)
                
                if res['type'] == 'SUCCESS':
                    if res['data']:
                        df = pd.DataFrame(res['data'])
                        cons = self._save_board_cons(res['board_id'], target_date, df)
                        total_cons += cons
                    
                    success_count += 1
                    pbar.update(1)
                    
                    proxy_mgr = get_proxy_manager()
                    ip_used = proxy_mgr.get_stats()['total_ips_used'] if proxy_mgr else 0
                    pbar.set_postfix({"æˆåŠŸ": success_count, "IP": ip_used, "é‡è¯•ä¸­": task_queue.qsize()})
                    
                    if success_count % 50 == 0:
                        self.session.commit()
                
                elif res['type'] == 'FAIL_PERM':
                    perm_fail_count += 1
                    perm_failed_boards.append(res['board_name'])
                    pbar.update(1)
                    logger.error(f"âŒ {res['board_name']} å½»åº•å¤±è´¥({res['retries']}æ¬¡): {res['error']}")
                    
                    # ç»Ÿè®¡é”™è¯¯ç±»å‹
                    et = res.get('error_type', 'unknown')
                    retry_stats[et] = retry_stats.get(et, 0) + 1
                
                elif res['type'] == 'FATAL':
                    logger.error(f"ğŸš¨ ç³»ç»Ÿç†”æ–­: {res['msg']}")
                    break
                
                elif res['type'] == 'RETRY':
                    # ç»Ÿè®¡é‡è¯•
                    et = res.get('error_type', 'unknown')
                    retry_stats[et] = retry_stats.get(et, 0) + 1
                    
            except queue.Empty:
                if not any(t.is_alive() for t in threads):
                    logger.warning("æ‰€æœ‰çº¿ç¨‹æ„å¤–é€€å‡º")
                    break
        
        pbar.close()
        
        # æ¸…ç†
        for _ in threads:
            task_queue.put(None)
        for t in threads:
            t.join(timeout=2)
        
        self.session.commit()
        
        # ç»Ÿè®¡æŠ¥å‘Š
        proxy_mgr = get_proxy_manager()
        ip_stats = proxy_mgr.get_stats() if proxy_mgr else {}
        
        logger.info("=" * 50)
        logger.info(f"ğŸ {board_type} åŒæ­¥å®Œæˆ:")
        logger.info(f"   æˆåŠŸ: {success_count}/{total_tasks} ({100*success_count/total_tasks:.1f}%)")
        logger.info(f"   æ°¸ä¹…å¤±è´¥: {perm_fail_count}")
        logger.info(f"   æˆåˆ†è‚¡: {total_cons} æ¡")
        logger.info(f"   IPæ¶ˆè€—: {ip_stats.get('total_ips_used', 0)}/{ip_stats.get('max_ips', 0)}")
        if retry_stats:
            logger.info(f"   é”™è¯¯åˆ†å¸ƒ: {retry_stats}")
        logger.info("=" * 50)
        
        return total_cons, perm_failed_boards
    
    def _save_board_cons_with_session(self, session, board_id: int, target_date: date, df: pd.DataFrame) -> int:
        """ä½¿ç”¨æŒ‡å®šä¼šè¯ä¿å­˜æ¿å—æˆåˆ†è‚¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if df is None or df.empty:
            return 0
        
        # å…ˆåˆ é™¤å½“å¤©æ•°æ®
        session.execute(text("""
            DELETE FROM ext_board_daily_snap 
            WHERE board_id = :board_id AND snap_date = :snap_date
        """), {"board_id": board_id, "snap_date": target_date})
        
        count = 0
        for _, row in df.iterrows():
            raw_code = str(row.get('ä»£ç ', ''))
            stock_code = self._normalize_stock_code(raw_code)
            
            if not stock_code:
                continue
            
            if stock_code not in self.stock_codes:
                continue
            
            session.execute(text("""
                INSERT INTO ext_board_daily_snap (board_id, stock_code, snap_date, created_at)
                VALUES (:board_id, :stock_code, :snap_date, NOW())
            """), {
                "board_id": board_id,
                "stock_code": stock_code,
                "snap_date": target_date
            })
            count += 1
        
        return count
    
    def _sync_em_industry_boards(self, provider_id: int):
        """åŒæ­¥ä¸œè´¢è¡Œä¸šæ¿å—åˆ—è¡¨"""
        logger.info("è·å–ä¸œè´¢è¡Œä¸šæ¿å—åˆ—è¡¨...")
        try:
            df = ak.stock_board_industry_name_em()
            if df is None or df.empty:
                logger.warning("ä¸œè´¢è¡Œä¸šæ¿å—åˆ—è¡¨ä¸ºç©º")
                return
            
            count = 0
            for _, row in df.iterrows():
                board_code = str(row.get('æ¿å—ä»£ç ', row.get('ä»£ç ', '')))
                board_name = str(row.get('æ¿å—åç§°', row.get('åç§°', '')))
                
                if not board_code or not board_name:
                    continue
                
                self._upsert_board(provider_id, board_code, board_name, 'industry')
                count += 1
            
            logger.info(f"åŒæ­¥ä¸œè´¢è¡Œä¸šæ¿å—: {count} ä¸ª")
        except Exception as e:
            logger.error(f"åŒæ­¥ä¸œè´¢è¡Œä¸šæ¿å—å¤±è´¥: {e}")
    
    def _sync_em_concept_boards(self, provider_id: int):
        """åŒæ­¥ä¸œè´¢æ¦‚å¿µæ¿å—åˆ—è¡¨"""
        logger.info("è·å–ä¸œè´¢æ¦‚å¿µæ¿å—åˆ—è¡¨...")
        try:
            df = ak.stock_board_concept_name_em()
            if df is None or df.empty:
                logger.warning("ä¸œè´¢æ¦‚å¿µæ¿å—åˆ—è¡¨ä¸ºç©º")
                return
            
            count = 0
            for _, row in df.iterrows():
                board_code = str(row.get('æ¿å—ä»£ç ', row.get('ä»£ç ', '')))
                board_name = str(row.get('æ¿å—åç§°', row.get('åç§°', '')))
                
                if not board_code or not board_name:
                    continue
                
                self._upsert_board(provider_id, board_code, board_name, 'concept')
                count += 1
            
            logger.info(f"åŒæ­¥ä¸œè´¢æ¦‚å¿µæ¿å—: {count} ä¸ª")
        except Exception as e:
            logger.error(f"åŒæ­¥ä¸œè´¢æ¦‚å¿µæ¿å—å¤±è´¥: {e}")
    
    def _sync_em_board_cons(self, provider_id: int, target_date: date, board_type: str):
        """åŒæ­¥ä¸œè´¢æ¿å—æˆåˆ†è‚¡"""
        logger.info(f"è·å–ä¸œè´¢{board_type}æ¿å—æˆåˆ†è‚¡...")
        
        # è·å–è¯¥ç±»å‹çš„æ‰€æœ‰æ¿å—
        boards = self._get_boards(provider_id, board_type)
        logger.info(f"å…± {len(boards)} ä¸ª{board_type}æ¿å—éœ€è¦åŒæ­¥æˆåˆ†è‚¡")
        
        total_cons = 0
        failed_boards = []
        
        for i, (board_id, board_name) in enumerate(boards):
            try:
                if board_type == 'industry':
                    df = ak.stock_board_industry_cons_em(symbol=board_name)
                else:
                    df = ak.stock_board_concept_cons_em(symbol=board_name)
                
                if df is None or df.empty:
                    continue
                
                cons_count = self._save_board_cons(board_id, target_date, df)
                total_cons += cons_count
                
                if (i + 1) % 50 == 0:
                    logger.info(f"è¿›åº¦: {i+1}/{len(boards)}, å·²åŒæ­¥ {total_cons} æ¡æˆåˆ†è‚¡")
                    self.session.commit()
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.1)
                
            except Exception as e:
                failed_boards.append(board_name)
                if "è¯·æ±‚" in str(e) or "é¢‘ç¹" in str(e):
                    logger.warning(f"è¯·æ±‚é¢‘ç¹ï¼Œæš‚åœ5ç§’...")
                    time.sleep(5)
        
        self.session.commit()
        logger.info(f"åŒæ­¥ä¸œè´¢{board_type}æ¿å—æˆåˆ†è‚¡å®Œæˆ: {total_cons} æ¡")
        if failed_boards:
            logger.warning(f"å¤±è´¥æ¿å— ({len(failed_boards)}): {failed_boards[:10]}...")
    
    # ==================== åŒèŠ±é¡º (THS) ====================
    
    def sync_ths_boards(self, target_date: date, skip_cons: bool = False):
        """åŒæ­¥åŒèŠ±é¡ºæ¿å—æ•°æ®"""
        provider_id = self._provider_cache.get('ths')
        if not provider_id:
            logger.error("æœªæ‰¾åˆ° ths æ•°æ®æºé…ç½®")
            return
        
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŒæ­¥åŒèŠ±é¡ºæ¿å—æ•°æ®...")
        
        # 1. åŒæ­¥æ¦‚å¿µæ¿å—
        self._sync_ths_concept_boards(provider_id)
        
        # 2. åŒæ­¥æˆåˆ†è‚¡
        if not skip_cons:
            self._sync_ths_board_cons(provider_id, target_date)
        
        self.session.commit()
        logger.info("åŒèŠ±é¡ºæ¿å—æ•°æ®åŒæ­¥å®Œæˆ")
    
    def sync_ths_boards_with_metrics(self, target_date: date, skip_cons: bool = False) -> tuple:
        """åŒæ­¥åŒèŠ±é¡ºæ¿å—æ•°æ®ï¼ˆå¸¦è¿ç»´æŒ‡æ ‡ï¼‰
        
        Returns:
            tuple: (æ¿å—æ•°, æˆåˆ†è‚¡æ•°, å¤±è´¥æ¿å—åˆ—è¡¨)
        """
        provider_id = self._provider_cache.get('ths')
        if not provider_id:
            logger.error("æœªæ‰¾åˆ° ths æ•°æ®æºé…ç½®")
            return 0, 0, []
        
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŒæ­¥åŒèŠ±é¡ºæ¿å—æ•°æ®...")
        
        total_boards = 0
        total_cons = 0
        all_failed = []
        
        # 1. åŒæ­¥æ¦‚å¿µæ¿å—
        board_count = self._sync_ths_concept_boards_with_count(provider_id)
        total_boards = board_count
        
        # 2. åŒæ­¥æˆåˆ†è‚¡
        if not skip_cons:
            cons, failed = self._sync_ths_board_cons_with_metrics(provider_id, target_date)
            total_cons = cons
            all_failed = failed
        
        self.session.commit()
        logger.info("åŒèŠ±é¡ºæ¿å—æ•°æ®åŒæ­¥å®Œæˆ")
        
        return total_boards, total_cons, all_failed
    
    def _sync_ths_concept_boards_with_count(self, provider_id: int) -> int:
        """åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—åˆ—è¡¨ï¼ˆè¿”å›æ•°é‡ï¼‰"""
        logger.info("è·å–åŒèŠ±é¡ºæ¦‚å¿µæ¿å—åˆ—è¡¨...")
        try:
            df = ak.stock_board_concept_name_ths()
            if df is None or df.empty:
                logger.warning("åŒèŠ±é¡ºæ¦‚å¿µæ¿å—åˆ—è¡¨ä¸ºç©º")
                return 0
            
            logger.info(f"åŒèŠ±é¡º API è¿”å›åˆ—å: {df.columns.tolist()}")
            
            count = 0
            for _, row in df.iterrows():
                # åŒèŠ±é¡ºè¿”å›çš„åˆ—åæ˜¯ 'name' å’Œ 'code'
                board_code = str(row.get('code', row.get('ä»£ç ', row.get('æ¿å—ä»£ç ', f'THS_{count}'))))
                board_name = str(row.get('name', row.get('æ¦‚å¿µåç§°', row.get('åç§°', ''))))
                
                if not board_name:
                    continue
                
                self._upsert_board(provider_id, board_code, board_name, 'concept')
                count += 1
            
            self.session.commit()
            logger.info(f"åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—: {count} ä¸ª")
            return count
        except Exception as e:
            logger.error(f"åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—å¤±è´¥: {e}")
            return 0
    
    def _sync_ths_board_cons_with_metrics(self, provider_id: int, target_date: date) -> tuple:
        """åŒæ­¥åŒèŠ±é¡ºæ¿å—æˆåˆ†è‚¡ï¼ˆä½¿ç”¨é—®è´¢ APIï¼‰
        
        Returns:
            tuple: (æˆåˆ†è‚¡æ•°, å¤±è´¥æ¿å—åˆ—è¡¨)
        """
        logger.info("è·å–åŒèŠ±é¡ºæ¿å—æˆåˆ†è‚¡ï¼ˆä½¿ç”¨é—®è´¢ APIï¼‰...")
        
        boards = self._get_boards(provider_id, 'concept')
        
        # æ”¯æŒ limit å‚æ•°
        limit = getattr(self, '_limit', None)
        if limit and limit > 0:
            boards = boards[:limit]
            logger.info(f"é™åˆ¶åŒæ­¥å‰ {limit} ä¸ªæ¿å—ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        
        logger.info(f"å…± {len(boards)} ä¸ªåŒèŠ±é¡ºæ¿å—éœ€è¦åŒæ­¥æˆåˆ†è‚¡")
        
        total_cons = 0
        failed_boards = []
        max_retries = 2
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        pbar = tqdm(boards, desc="åŒèŠ±é¡ºæˆåˆ†è‚¡(é—®è´¢)", unit="æ¿å—", ncols=100)
        
        for board_id, board_name in pbar:
            success = False
            
            for attempt in range(max_retries):
                try:
                    # ä½¿ç”¨é—®è´¢ API è·å–æˆåˆ†è‚¡
                    query = f"{board_name}æ¦‚å¿µæˆåˆ†è‚¡"
                    df = pywencai.get(question=query, loop=True)
                    
                    if df is None or df.empty:
                        success = True
                        break
                    
                    # é—®è´¢è¿”å›çš„åˆ—åä¸åŒï¼Œéœ€è¦è½¬æ¢
                    # é—®è´¢çš„è‚¡ç¥¨ä»£ç æ ¼å¼: 601933.SH -> éœ€è¦æå–çº¯æ•°å­—
                    if 'è‚¡ç¥¨ä»£ç ' in df.columns:
                        df['ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].astype(str).str.extract(r'(\d{6})')[0]
                    elif 'code' in df.columns:
                        df['ä»£ç '] = df['code'].astype(str).str.zfill(6)
                    
                    if 'è‚¡ç¥¨ç®€ç§°' in df.columns:
                        df['åç§°'] = df['è‚¡ç¥¨ç®€ç§°']
                    
                    cons_count = self._save_board_cons(board_id, target_date, df)
                    total_cons += cons_count
                    success = True
                    
                    pbar.set_postfix({"æˆåˆ†è‚¡": total_cons, "å¤±è´¥": len(failed_boards)})
                    break
                    
                except Exception as e:
                    error_msg = str(e)
                    if attempt < max_retries - 1:
                        wait_time = 3  # é—®è´¢ API ä¸éœ€è¦å¤ªé•¿ç­‰å¾…
                        pbar.set_postfix({"é‡è¯•": f"{attempt+1}/{max_retries}", "ç­‰å¾…": f"{wait_time}s"})
                        time.sleep(wait_time)
                    else:
                        failed_boards.append(board_name)
                        if len(failed_boards) <= 5:
                            logger.warning(f"æ¿å— {board_name} å¤±è´¥: {error_msg[:50]}")
            
            # é—®è´¢ API å»¶è¿Ÿï¼ˆä¸éœ€è¦å¤ªé•¿ï¼‰
            jitter = random.random() * 0.5
            time.sleep(self.delay + jitter)
            
            # æ¯50ä¸ªæäº¤ä¸€æ¬¡
            if pbar.n % 50 == 0:
                self.session.commit()
        
        pbar.close()
        self.session.commit()
        logger.info(f"åŒæ­¥åŒèŠ±é¡ºæ¿å—æˆåˆ†è‚¡å®Œæˆ: {total_cons} æ¡, å¤±è´¥ {len(failed_boards)} ä¸ª")
        
        return total_cons, failed_boards
    
    def _sync_ths_concept_boards(self, provider_id: int):
        """åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—åˆ—è¡¨"""
        logger.info("è·å–åŒèŠ±é¡ºæ¦‚å¿µæ¿å—åˆ—è¡¨...")
        try:
            df = ak.stock_board_concept_name_ths()
            if df is None or df.empty:
                logger.warning("åŒèŠ±é¡ºæ¦‚å¿µæ¿å—åˆ—è¡¨ä¸ºç©º")
                return
            
            count = 0
            for _, row in df.iterrows():
                # åŒèŠ±é¡ºè¿”å›çš„åˆ—åæ˜¯ 'name' å’Œ 'code'
                board_code = str(row.get('code', row.get('ä»£ç ', f'THS_{count}')))
                board_name = str(row.get('name', row.get('æ¦‚å¿µåç§°', '')))
                
                if not board_name:
                    continue
                
                self._upsert_board(provider_id, board_code, board_name, 'concept')
                count += 1
            
            logger.info(f"åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—: {count} ä¸ª")
        except Exception as e:
            logger.error(f"åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—å¤±è´¥: {e}")
    
    def _sync_ths_board_cons(self, provider_id: int, target_date: date):
        """åŒæ­¥åŒèŠ±é¡ºæ¿å—æˆåˆ†è‚¡"""
        logger.info("è·å–åŒèŠ±é¡ºæ¿å—æˆåˆ†è‚¡...")
        
        boards = self._get_boards(provider_id, 'concept')
        logger.info(f"å…± {len(boards)} ä¸ªåŒèŠ±é¡ºæ¿å—éœ€è¦åŒæ­¥æˆåˆ†è‚¡")
        
        total_cons = 0
        failed_boards = []
        
        for i, (board_id, board_name) in enumerate(boards):
            try:
                df = ak.stock_board_concept_cons_ths(symbol=board_name)
                
                if df is None or df.empty:
                    continue
                
                cons_count = self._save_board_cons(board_id, target_date, df)
                total_cons += cons_count
                
                if (i + 1) % 50 == 0:
                    logger.info(f"è¿›åº¦: {i+1}/{len(boards)}, å·²åŒæ­¥ {total_cons} æ¡æˆåˆ†è‚¡")
                    self.session.commit()
                
                time.sleep(0.2)  # åŒèŠ±é¡ºé™åˆ¶æ›´ä¸¥æ ¼
                
            except Exception as e:
                failed_boards.append(board_name)
                if "è¯·æ±‚" in str(e) or "é¢‘ç¹" in str(e) or "é™åˆ¶" in str(e):
                    logger.warning(f"è¯·æ±‚é¢‘ç¹ï¼Œæš‚åœ10ç§’...")
                    time.sleep(10)
        
        self.session.commit()
        logger.info(f"åŒæ­¥åŒèŠ±é¡ºæ¿å—æˆåˆ†è‚¡å®Œæˆ: {total_cons} æ¡")
        if failed_boards:
            logger.warning(f"å¤±è´¥æ¿å— ({len(failed_boards)}): {failed_boards[:10]}...")
    
    # ==================== é€šç”¨æ–¹æ³• ====================
    
    def _upsert_board(self, provider_id: int, board_code: str, board_name: str, board_type: str):
        """æ’å…¥æˆ–æ›´æ–°æ¿å—"""
        sql = text("""
            INSERT INTO ext_board_list (provider_id, board_code, board_name, board_type, updated_at)
            VALUES (:provider_id, :board_code, :board_name, :board_type, NOW())
            ON CONFLICT (provider_id, board_code) 
            DO UPDATE SET 
                board_name = EXCLUDED.board_name,
                board_type = EXCLUDED.board_type,
                updated_at = NOW()
        """)
        self.session.execute(sql, {
            'provider_id': provider_id,
            'board_code': board_code,
            'board_name': board_name,
            'board_type': board_type
        })
    
    def _get_boards(self, provider_id: int, board_type: str) -> List[tuple]:
        """è·å–æŒ‡å®šç±»å‹çš„æ¿å—åˆ—è¡¨"""
        sql = text("""
            SELECT id, board_name FROM ext_board_list 
            WHERE provider_id = :provider_id AND board_type = :board_type AND is_active = true
        """)
        result = self.session.execute(sql, {'provider_id': provider_id, 'board_type': board_type})
        return [(row.id, row.board_name) for row in result]
    
    def _save_board_cons(self, board_id: int, target_date: date, df: pd.DataFrame) -> int:
        """ä¿å­˜æ¿å—æˆåˆ†è‚¡"""
        count = 0
        
        # å°è¯•å¤šç§åˆ—å
        code_col = None
        for col in ['ä»£ç ', 'è‚¡ç¥¨ä»£ç ', 'code']:
            if col in df.columns:
                code_col = col
                break
        
        if not code_col:
            return 0
        
        for _, row in df.iterrows():
            raw_code = str(row[code_col])
            stock_code = self.normalize_stock_code(raw_code)
            
            if not stock_code or stock_code not in self._stock_codes:
                continue
            
            try:
                sql = text("""
                    INSERT INTO ext_board_daily_snap (stock_code, board_id, date)
                    VALUES (:stock_code, :board_id, :date)
                    ON CONFLICT (stock_code, board_id, date) DO NOTHING
                """)
                self.session.execute(sql, {
                    'stock_code': stock_code,
                    'board_id': board_id,
                    'date': target_date
                })
                count += 1
            except Exception as e:
                # è·³è¿‡å¤–é”®çº¦æŸé”™è¯¯ï¼ˆè‚¡ç¥¨ä¸å­˜åœ¨ï¼‰
                pass
        
        return count
    
    def update_board_stock_count(self):
        """æ›´æ–°æ¿å—æˆåˆ†è‚¡æ•°é‡"""
        logger.info("æ›´æ–°æ¿å—æˆåˆ†è‚¡æ•°é‡...")
        sql = text("""
            UPDATE ext_board_list b
            SET stock_count = (
                SELECT COUNT(DISTINCT stock_code) 
                FROM ext_board_daily_snap s 
                WHERE s.board_id = b.id
            )
        """)
        self.session.execute(sql)
        self.session.commit()
        logger.info("æ¿å—æˆåˆ†è‚¡æ•°é‡æ›´æ–°å®Œæˆ")
    
    def auto_map_local_sectors(self):
        """è‡ªåŠ¨æ˜ å°„å¤–éƒ¨æ¿å—åˆ°æœ¬åœ° sectors"""
        logger.info("è‡ªåŠ¨æ˜ å°„å¤–éƒ¨æ¿å—åˆ°æœ¬åœ° sectors...")
        
        # ç²¾ç¡®åŒ¹é…
        sql = text("""
            INSERT INTO ext_board_local_map (ext_board_id, local_sector_id, match_type, confidence)
            SELECT b.id, s.id, 'auto', 100.00
            FROM ext_board_list b
            JOIN ext_providers p ON p.id = b.provider_id AND p.code = 'em'
            JOIN sectors s ON s.sector_name = b.board_name
            ON CONFLICT DO NOTHING
        """)
        result = self.session.execute(sql)
        self.session.commit()
        
        # ç»Ÿè®¡æ˜ å°„æ•°é‡
        count_sql = text("SELECT COUNT(*) FROM ext_board_local_map")
        count = self.session.execute(count_sql).scalar()
        logger.info(f"è‡ªåŠ¨æ˜ å°„å®Œæˆï¼Œå…± {count} æ¡æ˜ å°„å…³ç³»")
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.session.close()
        self.engine.dispose()


def main():
    parser = argparse.ArgumentParser(description='åŒæ­¥å¤–éƒ¨æ¿å—æ•°æ®')
    parser.add_argument('--date', type=str, help='ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä»Šå¤©')
    parser.add_argument('--provider', type=str, choices=['em', 'ths', 'all'], default='all',
                        help='æ•°æ®æº: em(ä¸œè´¢), ths(åŒèŠ±é¡º), all(å…¨éƒ¨)')
    parser.add_argument('--skip-cons', action='store_true', help='è·³è¿‡æˆåˆ†è‚¡åŒæ­¥')
    parser.add_argument('--skip-map', action='store_true', help='è·³è¿‡è‡ªåŠ¨æ˜ å°„')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶åŒæ­¥ï¼ˆå¿½ç•¥å¹‚ç­‰æ£€æŸ¥ï¼‰')
    parser.add_argument('--db-url', type=str, help='æ•°æ®åº“è¿æ¥URL (è¦†ç›–ç¯å¢ƒå˜é‡)')
    parser.add_argument('--delay', type=float, default=1.5, help='è¯·æ±‚é—´éš”ç§’æ•° (é»˜è®¤1.5ç§’ï¼Œè¢«å°ç¦åå»ºè®®3-5ç§’)')
    parser.add_argument('--proxy', action='store_true', help='å¯ç”¨91HTTPéš§é“ä»£ç†')
    parser.add_argument('--proxy-trade-no', type=str, help='91HTTPä¸šåŠ¡ç¼–å·')
    parser.add_argument('--proxy-secret', type=str, help='91HTTPå¯†é’¥')
    parser.add_argument('--board-type', type=str, choices=['industry', 'concept', 'all'], default='all',
                        help='æ¿å—ç±»å‹: industry(è¡Œä¸š), concept(æ¦‚å¿µ), all(å…¨éƒ¨)')
    parser.add_argument('--limit', type=int, help='é™åˆ¶åŒæ­¥æ¿å—æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    
    # å¹¶å‘æ¨¡å¼å‚æ•° (Manager-Worker æ¶æ„)
    parser.add_argument('--concurrent', action='store_true', help='å¯ç”¨å¹¶å‘æ¨¡å¼ï¼ˆManager-Worker æ¶æ„ï¼‰')
    parser.add_argument('--workers', type=int, default=10, help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤10)')
    parser.add_argument('--max-ips', type=int, default=200, help='æœ€å¤§ä»£ç†IPæ•°é‡ (é»˜è®¤200ï¼Œé˜²æ­¢æ‰“ç©¿å·æ± )')
    parser.add_argument('--ip-ttl', type=float, default=50.0, help='å•ä¸ªIPå¤ç”¨æ—¶é—´ç§’æ•° (é»˜è®¤50ï¼Œç•™10ç§’ç¼“å†²)')
    parser.add_argument('--req-delay-min', type=float, default=1.0, help='è¯·æ±‚é—´æœ€å°å»¶è¿Ÿç§’æ•° (é»˜è®¤1)')
    parser.add_argument('--req-delay-max', type=float, default=3.0, help='è¯·æ±‚é—´æœ€å¤§å»¶è¿Ÿç§’æ•° (é»˜è®¤3)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ä»£ç†
    if args.proxy:
        proxy_mgr = ProxyManager(
            trade_no=args.proxy_trade_no,
            secret=args.proxy_secret,
            max_ips=args.max_ips
        )
        set_proxy_manager(proxy_mgr)
        logger.info(f"å·²å¯ç”¨91HTTPéš§é“ä»£ç† (IPä¸Šé™: {args.max_ips})")
    
    # æ•°æ®åº“ URL
    db_url = args.db_url or DATABASE_URL
    logger.info(f"æ•°æ®åº“: {db_url.split('@')[-1] if '@' in db_url else db_url}")  # éšè—å¯†ç 
    
    # è§£ææ—¥æœŸ
    if args.date:
        requested_date = datetime.strptime(args.date, '%Y-%m-%d').date()
    else:
        requested_date = date.today()

    trade_date = None
    try:
        _cal_engine = create_engine(db_url)
        trade_date = find_latest_trade_date(_cal_engine, requested_date)
    except Exception as e:
        logger.warning(f"âš ï¸ äº¤æ˜“æ—¥å¯»å€å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸ {requested_date}: {e}")

    if trade_date and trade_date != requested_date:
        logger.info(f"ğŸ“… {requested_date} éäº¤æ˜“æ—¥/æ— æ•°æ®ï¼Œä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥ {trade_date} ä½œä¸ºåŒæ­¥æ—¥æœŸ")
        target_date = trade_date
    else:
        target_date = requested_date
    
    date_str = target_date.isoformat()
    
    logger.info("=" * 60)
    logger.info(f"å¤–éƒ¨æ¿å—æ•°æ®åŒæ­¥ v1.1.0")
    logger.info("=" * 60)
    logger.info(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
    logger.info(f"æ•°æ®æº: {args.provider}")
    logger.info(f"è·³è¿‡æˆåˆ†è‚¡: {args.skip_cons}")
    logger.info(f"å¼ºåˆ¶åŒæ­¥: {args.force}")
    
    # çŠ¶æ€ç®¡ç†å™¨
    state_manager = ExtBoardStateManager()
    
    # å¹‚ç­‰æ£€æŸ¥ï¼ˆæŒ‰ provider åˆ†åˆ«æ£€æŸ¥ï¼‰
    providers_to_sync = []
    if args.provider in ['em', 'all']:
        if args.force or not state_manager.is_synced_today(date_str, 'em'):
            providers_to_sync.append('em')
        else:
            logger.info(f"â­ï¸ ä¸œè´¢(em) {date_str} å·²åŒæ­¥æˆåŠŸï¼Œè·³è¿‡")
    
    if args.provider in ['ths', 'all']:
        if args.force or not state_manager.is_synced_today(date_str, 'ths'):
            providers_to_sync.append('ths')
        else:
            logger.info(f"â­ï¸ åŒèŠ±é¡º(ths) {date_str} å·²åŒæ­¥æˆåŠŸï¼Œè·³è¿‡")
    
    if not providers_to_sync:
        logger.info("æ‰€æœ‰æ•°æ®æºéƒ½å·²åŒæ­¥ï¼Œæ— éœ€é‡å¤æ‰§è¡Œï¼ˆä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°åŒæ­¥ï¼‰")
        return
    
    logger.info(f"å¾…åŒæ­¥æ•°æ®æº: {providers_to_sync}")
    
    # å¼€å§‹åŒæ­¥ï¼ˆä¸è¦†ç›–å·²æœ‰ provider æ•°æ®ï¼‰
    state_manager.start_sync(date_str)
    
    syncer = ExtBoardSyncer(db_url, delay=args.delay)
    syncer._board_type = args.board_type  # æ¿å—ç±»å‹è¿‡æ»¤
    syncer._limit = args.limit  # æ•°é‡é™åˆ¶
    
    # å¹¶å‘æ¨¡å¼å‚æ•°
    syncer._concurrent = args.concurrent
    syncer._workers = args.workers
    syncer._ip_ttl = args.ip_ttl
    syncer._req_delay_min = args.req_delay_min
    syncer._req_delay_max = args.req_delay_max
    
    if args.concurrent:
        logger.info(f"ğŸš€ å¹¶å‘æ¨¡å¼ (Manager-Worker): {args.workers}çº¿ç¨‹, IPå¤ç”¨{args.ip_ttl}ç§’, è¯·æ±‚é—´éš”{args.req_delay_min}-{args.req_delay_max}ç§’, IPä¸Šé™{args.max_ips}")
    else:
        logger.info(f"è¯·æ±‚é—´éš”: {args.delay}ç§’ (åŒèŠ±é¡º: {args.delay * 1.5}ç§’)")
    
    if args.board_type != 'all':
        logger.info(f"æ¿å—ç±»å‹: {args.board_type}")
    if args.limit:
        logger.info(f"æ•°é‡é™åˆ¶: {args.limit} ä¸ªæ¿å—")
    
    # è¿ç»´æ•°æ®
    metrics = {
        "total_boards": 0,
        "total_cons": 0,
        "matched_stocks": 0,
        "unmatched_stocks": 0,
        "api_success_rate": 0,
        "providers": {}
    }
    
    try:
        total_start = time.time()
        
        # åŒæ­¥ä¸œæ–¹è´¢å¯Œï¼ˆåªåœ¨éœ€è¦æ—¶æ‰§è¡Œï¼‰
        if 'em' in providers_to_sync:
            em_start = time.time()
            em_boards, em_cons, em_failed = syncer.sync_em_boards_with_metrics(
                target_date, skip_cons=args.skip_cons
            )
            em_duration = time.time() - em_start
            
            metrics["providers"]["em"] = {
                "board_count": em_boards,
                "cons_count": em_cons,
                "duration_seconds": round(em_duration, 2),
                "failed_count": len(em_failed)
            }
            metrics["total_boards"] += em_boards
            metrics["total_cons"] += em_cons
            
            state_manager.update_provider_status(
                date_str, "em", em_boards, em_cons,
                "success" if len(em_failed) == 0 else "partial",
                em_duration, em_failed
            )
            
            logger.info(f"ğŸ“Š ä¸œè´¢åŒæ­¥å®Œæˆ: {em_boards} æ¿å—, {em_cons} æˆåˆ†è‚¡, è€—æ—¶ {em_duration:.1f}ç§’")
        
        # åŒæ­¥åŒèŠ±é¡ºï¼ˆåªåœ¨éœ€è¦æ—¶æ‰§è¡Œï¼‰
        if 'ths' in providers_to_sync:
            ths_start = time.time()
            ths_boards, ths_cons, ths_failed = syncer.sync_ths_boards_with_metrics(
                target_date, skip_cons=args.skip_cons
            )
            ths_duration = time.time() - ths_start
            
            metrics["providers"]["ths"] = {
                "board_count": ths_boards,
                "cons_count": ths_cons,
                "duration_seconds": round(ths_duration, 2),
                "failed_count": len(ths_failed)
            }
            metrics["total_boards"] += ths_boards
            metrics["total_cons"] += ths_cons
            
            state_manager.update_provider_status(
                date_str, "ths", ths_boards, ths_cons,
                "success" if len(ths_failed) == 0 else "partial",
                ths_duration, ths_failed
            )
            
            logger.info(f"ğŸ“Š åŒèŠ±é¡ºåŒæ­¥å®Œæˆ: {ths_boards} æ¿å—, {ths_cons} æˆåˆ†è‚¡, è€—æ—¶ {ths_duration:.1f}ç§’")
        
        # æ›´æ–°ç»Ÿè®¡
        syncer.update_board_stock_count()
        
        # è‡ªåŠ¨æ˜ å°„
        if not args.skip_map:
            syncer.auto_map_local_sectors()
        
        total_duration = time.time() - total_start
        metrics["total_duration_seconds"] = round(total_duration, 2)
        
        # æ ‡è®°æˆåŠŸ
        state_manager.mark_success(date_str, metrics)
        
        logger.info("=" * 60)
        logger.info("âœ… å…¨éƒ¨åŒæ­¥å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"ğŸ“ˆ è¿ç»´æ•°æ®æ±‡æ€»:")
        logger.info(f"   æ€»æ¿å—æ•°: {metrics['total_boards']}")
        logger.info(f"   æ€»æˆåˆ†è‚¡è®°å½•: {metrics['total_cons']}")
        logger.info(f"   æ€»è€—æ—¶: {total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)")
        for provider, pdata in metrics["providers"].items():
            logger.info(f"   {provider}: {pdata['board_count']} æ¿å—, {pdata['cons_count']} æˆåˆ†è‚¡, "
                       f"è€—æ—¶ {pdata['duration_seconds']:.1f}ç§’, å¤±è´¥ {pdata['failed_count']} ä¸ª")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­")
        state_manager.mark_failed(date_str, "ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"åŒæ­¥å¤±è´¥: {e}")
        state_manager.mark_failed(date_str, str(e))
        raise
    finally:
        syncer.close()


if __name__ == '__main__':
    main()
