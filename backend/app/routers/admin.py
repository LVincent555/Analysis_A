"""
ç®¡ç†å‘˜è·¯ç”± - æ–‡ä»¶ä¸Šä¼ å’Œæ•°æ®å¯¼å…¥
ä»… admin è§’è‰²å¯è®¿é—®

v0.5.0: æ•°æ®åˆ é™¤æ—¶æ¸…ç†ç»Ÿä¸€ç¼“å­˜
"""
import os
import base64
import logging
import threading
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel

from ..auth.dependencies import get_current_user
from ..db_models import User
from ..core.caching import cache  # v0.5.0: ç»Ÿä¸€ç¼“å­˜
from ..services.hot_spots_cache import HotSpotsCache  # v0.5.0: çƒ­ç‚¹æ¦œç¼“å­˜

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/admin", tags=["admin"])

# æ•°æ®ç›®å½•ï¼ˆä½¿ç”¨ config ä¸­çš„ç»Ÿä¸€é…ç½®ï¼‰
from ..config import DATA_DIR

# å¯¼å…¥çŠ¶æ€å­˜å‚¨ï¼ˆç®€å•çš„å†…å­˜å­˜å‚¨ï¼Œé‡å¯åæ¸…ç©ºï¼‰
import_status = {
    "is_importing": False,
    "current_file": None,
    "progress": 0,
    "last_import": None,
    "last_result": None,
    "history": [],
    "logs": []  # å®æ—¶æ—¥å¿—
}

# æœ€å¤§æ—¥å¿—æ¡æ•°
MAX_LOGS = 100

def add_log(message: str, level: str = "info"):
    """æ·»åŠ æ—¥å¿—åˆ°çŠ¶æ€"""
    from datetime import datetime
    log_entry = {
        "time": datetime.now().strftime("%H:%M:%S"),
        "level": level,
        "message": message
    }
    import_status["logs"].append(log_entry)
    # ä¿æŒæ—¥å¿—æ•°é‡é™åˆ¶
    if len(import_status["logs"]) > MAX_LOGS:
        import_status["logs"] = import_status["logs"][-MAX_LOGS:]
    # åŒæ—¶è¾“å‡ºåˆ°æœåŠ¡å™¨æ—¥å¿—
    if level == "error":
        logger.error(message)
    else:
        logger.info(message)


def require_admin(current_user: User = Depends(get_current_user)) -> User:
    """è¦æ±‚ç®¡ç†å‘˜æƒé™"""
    if current_user.role != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="éœ€è¦ç®¡ç†å‘˜æƒé™"
        )
    return current_user


class FileUploadRequest(BaseModel):
    """æ–‡ä»¶ä¸Šä¼ è¯·æ±‚"""
    filename: str  # æ–‡ä»¶å
    content: str   # Base64 ç¼–ç çš„æ–‡ä»¶å†…å®¹
    

class ImportRequest(BaseModel):
    """å¯¼å…¥è¯·æ±‚"""
    date: Optional[str] = None  # å¯é€‰çš„æ—¥æœŸå‚æ•° YYYYMMDD


class UploadResponse(BaseModel):
    """ä¸Šä¼ å“åº”"""
    success: bool
    message: str
    filepath: Optional[str] = None


@router.post("/upload", response_model=UploadResponse)
async def upload_file(
    file_data: FileUploadRequest,
    current_user: User = Depends(require_admin)
):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨ data ç›®å½•
    æ–‡ä»¶å†…å®¹éœ€è¦ Base64 ç¼–ç 
    """
    try:
        # éªŒè¯æ–‡ä»¶å
        filename = file_data.filename
        if not filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        
        # åªå…è®¸ xlsx å’Œ xls æ–‡ä»¶
        if not filename.lower().endswith(('.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒ Excel æ–‡ä»¶ (.xlsx, .xls)")
        
        # è§£ç  Base64 å†…å®¹
        try:
            file_content = base64.b64decode(file_data.content)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Base64 è§£ç å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ 10MBï¼‰
        if len(file_content) > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡ 10MB")
        
        # ç¡®ä¿ data ç›®å½•å­˜åœ¨
        os.makedirs(DATA_DIR, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        filepath = os.path.join(DATA_DIR, filename)
        with open(filepath, 'wb') as f:
            f.write(file_content)
        
        logger.info(f"ç®¡ç†å‘˜ {current_user.username} ä¸Šä¼ æ–‡ä»¶: {filename}")
        
        return UploadResponse(
            success=True,
            message=f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}",
            filepath=filepath
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")


def _do_import_task(username: str):
    """åå°æ‰§è¡Œå¯¼å…¥ä»»åŠ¡"""
    global import_status
    from pathlib import Path
    
    try:
        data_dir = Path(DATA_DIR)
        add_log(f"ğŸ“‚ æ‰«ææ•°æ®ç›®å½•: {data_dir}")
        
        # åˆ†ç±»æ–‡ä»¶
        stock_files = list(data_dir.glob("*_data_sma_feature_color.xlsx"))
        sector_files = list(data_dir.glob("*_allbk_sma_feature_color.xlsx"))
        
        all_files = stock_files + sector_files
        
        add_log(f"ğŸ“Š æ‰¾åˆ° {len(stock_files)} ä¸ªè‚¡ç¥¨æ•°æ®æ–‡ä»¶")
        add_log(f"ğŸ“Š æ‰¾åˆ° {len(sector_files)} ä¸ªæ¿å—æ•°æ®æ–‡ä»¶")
        
        if not all_files:
            import_status["is_importing"] = False
            add_log("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¾…å¯¼å…¥çš„æ–‡ä»¶", "warning")
            return
        
        total_files = len(all_files)
        imported_count = 0
        errors = []
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(msg: str, progress_pct: int = None):
            """å¯¼å…¥è¿›åº¦å›è°ƒ"""
            if progress_pct is not None:
                import_status["progress"] = progress_pct
            if msg:
                add_log(msg)
        
        # å¯¼å…¥è‚¡ç¥¨æ•°æ®
        if stock_files:
            import_status["current_file"] = "å¯¼å…¥è‚¡ç¥¨æ•°æ®..."
            add_log(f"ğŸ“ˆ å¼€å§‹å¯¼å…¥è‚¡ç¥¨æ•°æ® ({len(stock_files)} ä¸ªæ–‡ä»¶)")
            
            from scripts.import_data_robust import import_excel_file as import_stock_file
            from scripts.import_state_manager import get_state_manager
            
            state_manager = get_state_manager()
            
            for i, filepath in enumerate(stock_files):
                filename = os.path.basename(str(filepath))
                import_status["current_file"] = f"[è‚¡ç¥¨] {filename}"
                base_progress = int((i / total_files) * 50)
                import_status["progress"] = base_progress
                
                try:
                    # ä¼ é€’è¿›åº¦å›è°ƒ
                    result = import_stock_file(filepath, state_manager, progress_callback=lambda msg, pct=None: progress_callback(msg, base_progress + int((pct or 0) * 0.5 / len(stock_files)) if pct else None))
                    if result[2]:  # success flag
                        imported_count += 1
                        add_log(f"âœ… [è‚¡ç¥¨] {filename} - å¯¼å…¥æˆåŠŸ ({result[0]} æ¡)")
                    else:
                        if result[1] > 0:  # skipped
                            add_log(f"â­ï¸ [è‚¡ç¥¨] {filename} - å·²å­˜åœ¨ï¼Œè·³è¿‡")
                            imported_count += 1
                        else:
                            add_log(f"âŒ [è‚¡ç¥¨] {filename} - å¯¼å…¥å¤±è´¥", "error")
                            errors.append(f"{filename}: å¯¼å…¥è¿”å›å¤±è´¥")
                except Exception as e:
                    errors.append(f"[è‚¡ç¥¨] {filename}: {str(e)}")
                    add_log(f"âŒ [è‚¡ç¥¨] {filename} - é”™è¯¯: {str(e)}", "error")
        
        # å¯¼å…¥æ¿å—æ•°æ®
        if sector_files:
            import_status["current_file"] = "å¯¼å…¥æ¿å—æ•°æ®..."
            add_log(f"ğŸ“Š å¼€å§‹å¯¼å…¥æ¿å—æ•°æ® ({len(sector_files)} ä¸ªæ–‡ä»¶)")
            
            from scripts.import_sectors_robust import import_sector_excel_file as import_sector_file
            from scripts.import_state_manager import ImportStateManager
            
            sector_state_manager = ImportStateManager("sector_import_state.json")
            
            for i, filepath in enumerate(sector_files):
                filename = os.path.basename(str(filepath))
                import_status["current_file"] = f"[æ¿å—] {filename}"
                base_progress = int(50 + (i / total_files) * 50)
                import_status["progress"] = base_progress
                
                try:
                    # ä¼ é€’è¿›åº¦å›è°ƒ
                    result = import_sector_file(filepath, sector_state_manager, progress_callback=lambda msg, pct=None: progress_callback(msg, base_progress + int((pct or 0) * 0.5 / len(sector_files)) if pct else None))
                    if result[2]:  # success flag
                        imported_count += 1
                        add_log(f"âœ… [æ¿å—] {filename} - å¯¼å…¥æˆåŠŸ ({result[0]} æ¡)")
                    else:
                        if result[1] > 0:  # skipped
                            add_log(f"â­ï¸ [æ¿å—] {filename} - å·²å­˜åœ¨ï¼Œè·³è¿‡")
                            imported_count += 1
                        else:
                            add_log(f"âŒ [æ¿å—] {filename} - å¯¼å…¥å¤±è´¥", "error")
                            errors.append(f"{filename}: å¯¼å…¥è¿”å›å¤±è´¥")
                except Exception as e:
                    errors.append(f"[æ¿å—] {filename}: {str(e)}")
                    add_log(f"âŒ [æ¿å—] {filename} - é”™è¯¯: {str(e)}", "error")
        
        # è®°å½•ç»“æœ
        result = {
            "success": imported_count > 0,
            "message": f"å¯¼å…¥å®Œæˆ: {imported_count}/{total_files} ä¸ªæ–‡ä»¶æˆåŠŸ",
            "imported": imported_count,
            "total": total_files,
            "errors": errors if errors else None
        }
        
        import_status["last_import"] = datetime.now().isoformat()
        import_status["last_result"] = result
        import_status["history"].insert(0, {
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "result": result
        })
        
        # åªä¿ç•™æœ€è¿‘ 10 æ¡è®°å½•
        import_status["history"] = import_status["history"][:10]
        
        add_log(f"âœ… {result['message']}")
        add_log(f"ğŸ‘¤ æ“ä½œç”¨æˆ·: {username}")
        
        # é‡è½½å†…å­˜ç¼“å­˜ï¼ˆç¡®ä¿æœ€æ–°æ•°æ®ç«‹å³å¯ç”¨ï¼‰
        add_log("ğŸ”„ æ­£åœ¨é‡è½½å†…å­˜ç¼“å­˜...")
        import_status["current_file"] = "é‡è½½ç¼“å­˜..."
        import_status["progress"] = 90
        try:
            from ..core.startup import preload_cache
            preload_cache()
            # v0.5.0: æ¸…ç†ç»Ÿä¸€ç¼“å­˜ç³»ç»Ÿçš„ API ç¼“å­˜å’Œçƒ­ç‚¹æ¦œç¼“å­˜
            cache.clear_api_cache()
            HotSpotsCache.clear_cache()
            add_log("âœ… å†…å­˜ç¼“å­˜é‡è½½å®Œæˆï¼æ–°æ•°æ®å·²ç”Ÿæ•ˆ (å«ç»Ÿä¸€ç¼“å­˜)")
        except Exception as cache_error:
            add_log(f"âš ï¸ ç¼“å­˜é‡è½½å¤±è´¥: {str(cache_error)}", "warning")
        
    except Exception as e:
        add_log(f"âŒ å¯¼å…¥å¤±è´¥: {str(e)}", "error")
    finally:
        import_status["is_importing"] = False
        import_status["progress"] = 100
        import_status["current_file"] = None


@router.post("/import")
async def trigger_import(
    import_params: ImportRequest = None,
    current_user: User = Depends(require_admin)
):
    """
    è§¦å‘æ•°æ®å¯¼å…¥ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ï¼‰
    å‰ç«¯é€šè¿‡ /admin/import-status è½®è¯¢è¿›åº¦
    """
    global import_status
    
    if import_status["is_importing"]:
        raise HTTPException(status_code=400, detail="æ­£åœ¨å¯¼å…¥ä¸­ï¼Œè¯·ç¨åå†è¯•")
    
    # åˆå§‹åŒ–çŠ¶æ€
    import_status["is_importing"] = True
    import_status["progress"] = 0
    import_status["current_file"] = "æ­£åœ¨å‡†å¤‡å¯¼å…¥..."
    import_status["logs"] = []
    
    add_log("ğŸš€ å¼€å§‹æ•°æ®å¯¼å…¥ä»»åŠ¡ï¼ˆåå°æ‰§è¡Œï¼‰")
    
    # ä½¿ç”¨ threading åœ¨åå°æ‰§è¡Œï¼ˆåŠ å¯†ç½‘å…³ä¸æ”¯æŒ BackgroundTasksï¼‰
    thread = threading.Thread(target=_do_import_task, args=(current_user.username,))
    thread.daemon = True
    thread.start()
    
    return {
        "success": True,
        "message": "å¯¼å…¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ /admin/import-status æŸ¥çœ‹è¿›åº¦",
        "status": "started"
    }


@router.get("/import-status")
async def get_import_status(current_user: User = Depends(require_admin)):
    """
    è·å–å¯¼å…¥çŠ¶æ€
    """
    return {
        "is_importing": import_status["is_importing"],
        "current_file": import_status["current_file"],
        "progress": import_status["progress"],
        "last_import": import_status["last_import"],
        "last_result": import_status["last_result"],
        "history": import_status["history"],
        "logs": import_status["logs"]  # å®æ—¶æ—¥å¿—
    }


@router.get("/data-files")
async def list_data_files(current_user: User = Depends(require_admin)):
    """
    åˆ—å‡º data ç›®å½•ä¸­çš„æ–‡ä»¶
    """
    try:
        if not os.path.exists(DATA_DIR):
            return {"files": []}
        
        files = []
        for filename in os.listdir(DATA_DIR):
            filepath = os.path.join(DATA_DIR, filename)
            if os.path.isfile(filepath) and filename.lower().endswith(('.xlsx', '.xls')):
                stat = os.stat(filepath)
                files.append({
                    "name": filename,
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
                })
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åº
        files.sort(key=lambda x: x["modified"], reverse=True)
        
        return {"files": files}
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}")


@router.delete("/data-files/{filename}")
async def delete_data_file(
    filename: str,
    current_user: User = Depends(require_admin)
):
    """
    åˆ é™¤ data ç›®å½•ä¸­çš„æ–‡ä»¶
    """
    try:
        # é˜²æ­¢è·¯å¾„éå†æ”»å‡»
        if ".." in filename or "/" in filename or "\\" in filename:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶å")
        
        filepath = os.path.join(DATA_DIR, filename)
        
        if not os.path.exists(filepath):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        os.remove(filepath)
        logger.info(f"ç®¡ç†å‘˜ {current_user.username} åˆ é™¤æ–‡ä»¶: {filename}")
        
        return {"success": True, "message": f"æ–‡ä»¶å·²åˆ é™¤: {filename}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.get("/dates")
async def get_imported_dates(current_user: User = Depends(require_admin)):
    """
    è·å–å·²å¯¼å…¥æ•°æ®çš„æ—¥æœŸåˆ—è¡¨
    """
    try:
        from ..database import SessionLocal
        from sqlalchemy import text
        
        db = SessionLocal()
        try:
            # æŸ¥è¯¢å·²å¯¼å…¥çš„æ—¥æœŸ
            result = db.execute(text("""
                SELECT DISTINCT date 
                FROM daily_stock_data 
                ORDER BY date DESC 
                LIMIT 30
            """))
            dates = [row[0].strftime("%Y-%m-%d") if hasattr(row[0], 'strftime') else str(row[0]) for row in result]
            
            return {"dates": dates}
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"è·å–æ—¥æœŸåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ—¥æœŸåˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/login-history")
async def get_login_history(current_user: User = Depends(require_admin)):
    """
    è·å–ç”¨æˆ·ç™»å½•å†å²å’Œæ´»è·ƒä¼šè¯
    ä»…ç®¡ç†å‘˜å¯è®¿é—®
    """
    try:
        from ..database import SessionLocal
        from ..db_models import User as UserModel, UserSession
        from sqlalchemy import func
        from datetime import datetime, timedelta
        
        db = SessionLocal()
        try:
            # 1. è·å–æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯
            users_query = db.query(
                UserModel.id,
                UserModel.username,
                UserModel.role,
                UserModel.is_active,
                UserModel.created_at,
                UserModel.last_login,
                func.count(UserSession.id).label('session_count')
            ).outerjoin(
                UserSession, UserModel.id == UserSession.user_id
            ).group_by(
                UserModel.id
            ).order_by(
                UserModel.last_login.desc().nullslast()
            ).all()
            
            users = []
            for u in users_query:
                users.append({
                    'id': u.id,
                    'username': u.username,
                    'role': u.role,
                    'is_active': u.is_active,
                    'created_at': u.created_at.isoformat() if u.created_at else None,
                    'last_login': u.last_login.isoformat() if u.last_login else None,
                    'session_count': u.session_count or 0
                })
            
            # 2. è·å–æ‰€æœ‰æ´»è·ƒä¼šè¯
            sessions_query = db.query(
                UserSession.id,
                UserSession.user_id,
                UserSession.device_id,
                UserSession.device_name,
                UserSession.created_at,
                UserSession.expires_at,
                UserSession.last_active,
                UserModel.username,
                UserModel.role
            ).join(
                UserModel, UserSession.user_id == UserModel.id
            ).order_by(
                UserSession.last_active.desc().nullslast()
            ).all()
            
            sessions = []
            for s in sessions_query:
                sessions.append({
                    'id': s.id,
                    'user_id': s.user_id,
                    'username': s.username,
                    'role': s.role,
                    'device_id': s.device_id,
                    'device_name': s.device_name,
                    'created_at': s.created_at.isoformat() if s.created_at else None,
                    'expires_at': s.expires_at.isoformat() if s.expires_at else None,
                    'last_active': s.last_active.isoformat() if s.last_active else None
                })
            
            # 3. è®¡ç®—ç»Ÿè®¡æ•°æ®
            now = datetime.now()
            active_threshold = now - timedelta(hours=24)
            session_active_threshold = now - timedelta(hours=1)
            
            total_users = len(users)
            active_users = sum(1 for u in users if u['last_login'] and 
                             datetime.fromisoformat(u['last_login']) > active_threshold)
            total_sessions = len(sessions)
            active_sessions = sum(1 for s in sessions if s['last_active'] and 
                                datetime.fromisoformat(s['last_active']) > session_active_threshold)
            
            return {
                'success': True,
                'data': {
                    'users': users,
                    'sessions': sessions,
                    'stats': {
                        'totalUsers': total_users,
                        'activeUsers': active_users,
                        'totalSessions': total_sessions,
                        'activeSessions': active_sessions
                    }
                }
            }
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"è·å–ç™»å½•å†å²å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç™»å½•å†å²å¤±è´¥: {str(e)}")


# ==================== æ•°æ®åˆ é™¤åŠŸèƒ½ ====================

class DeleteDataRequest(BaseModel):
    """åˆ é™¤æ•°æ®è¯·æ±‚"""
    dates: list[str]  # æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
    data_type: str = "all"  # stock, sector, all


@router.get("/data/preview/{date}")
async def preview_delete_data(
    date: str,
    data_type: str = "all",
    current_user: User = Depends(require_admin)
):
    """
    é¢„è§ˆåˆ é™¤æ•°æ®çš„å½±å“
    
    Args:
        date: æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        data_type: stock, sector, all
    """
    try:
        from ..database import SessionLocal
        from ..db_models import DailyStockData, SectorDailyData
        from sqlalchemy import func
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        date_str = date.replace("-", "")
        
        db = SessionLocal()
        try:
            result = {
                "date": date_str,
                "stock_count": 0,
                "sector_count": 0
            }
            
            if data_type in ["stock", "all"]:
                stock_count = db.query(func.count(DailyStockData.id)).filter(
                    func.to_char(DailyStockData.date, 'YYYYMMDD') == date_str
                ).scalar()
                result["stock_count"] = stock_count or 0
            
            if data_type in ["sector", "all"]:
                sector_count = db.query(func.count(SectorDailyData.id)).filter(
                    func.to_char(SectorDailyData.date, 'YYYYMMDD') == date_str
                ).scalar()
                result["sector_count"] = sector_count or 0
            
            result["total_count"] = result["stock_count"] + result["sector_count"]
            
            return {
                "success": True,
                "preview": result
            }
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"é¢„è§ˆåˆ é™¤å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é¢„è§ˆåˆ é™¤å¤±è´¥: {str(e)}")


@router.delete("/data/{date}")
async def delete_data_by_date(
    date: str,
    data_type: str = "all",
    current_user: User = Depends(require_admin)
):
    """
    åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®
    
    Args:
        date: æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        data_type: stock, sector, all
    """
    try:
        from ..database import SessionLocal
        from ..db_models import DailyStockData, SectorDailyData
        from sqlalchemy import func
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        date_str = date.replace("-", "")
        
        db = SessionLocal()
        try:
            result = {
                "date": date_str,
                "stock_deleted": 0,
                "sector_deleted": 0
            }
            
            if data_type in ["stock", "all"]:
                stock_deleted = db.query(DailyStockData).filter(
                    func.to_char(DailyStockData.date, 'YYYYMMDD') == date_str
                ).delete(synchronize_session=False)
                result["stock_deleted"] = stock_deleted
                logger.info(f"åˆ é™¤è‚¡ç¥¨æ•°æ®: {date_str}, {stock_deleted} æ¡")
            
            if data_type in ["sector", "all"]:
                sector_deleted = db.query(SectorDailyData).filter(
                    func.to_char(SectorDailyData.date, 'YYYYMMDD') == date_str
                ).delete(synchronize_session=False)
                result["sector_deleted"] = sector_deleted
                logger.info(f"åˆ é™¤æ¿å—æ•°æ®: {date_str}, {sector_deleted} æ¡")
            
            db.commit()
            
            result["total_deleted"] = result["stock_deleted"] + result["sector_deleted"]
            
            # æ›´æ–°å¯¼å…¥çŠ¶æ€
            try:
                from scripts.import_state_manager import ImportStateManager, reload_state_managers
                
                if data_type in ["stock", "all"]:
                    stock_state = ImportStateManager("data_import_state.json")
                    stock_state.mark_deleted(date_str, "manual_delete", current_user.username)
                
                if data_type in ["sector", "all"]:
                    sector_state = ImportStateManager("sector_import_state.json")
                    sector_state.mark_deleted(date_str, "manual_delete", current_user.username)
                
                # åˆ·æ–°å•ä¾‹çŠ¶æ€
                reload_state_managers()
                    
            except Exception as state_err:
                logger.warning(f"æ›´æ–°å¯¼å…¥çŠ¶æ€å¤±è´¥: {state_err}")
            
            logger.info(f"ç®¡ç†å‘˜ {current_user.username} åˆ é™¤æ•°æ®: {date_str}, å…± {result['total_deleted']} æ¡")
            
            # é‡è½½ç¼“å­˜
            try:
                from ..core.startup import preload_cache
                logger.info("ğŸ”„ åˆ é™¤åé‡è½½ç¼“å­˜...")
                preload_cache()
                # v0.5.0: æ¸…ç†ç»Ÿä¸€ç¼“å­˜ç³»ç»Ÿçš„ API ç¼“å­˜å’Œçƒ­ç‚¹æ¦œç¼“å­˜
                cache.clear_api_cache()
                HotSpotsCache.clear_cache()
                logger.info("âœ… ç¼“å­˜é‡è½½å®Œæˆ (å«ç»Ÿä¸€ç¼“å­˜)")
            except Exception as cache_err:
                logger.warning(f"âš ï¸ ç¼“å­˜é‡è½½å¤±è´¥: {cache_err}")
            
            return {
                "success": True,
                "result": result,
                "message": f"å·²åˆ é™¤ {date_str} çš„æ•°æ®ï¼Œå…± {result['total_deleted']} æ¡"
            }
            
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ•°æ®å¤±è´¥: {str(e)}")


@router.post("/data/delete-batch")
async def delete_data_batch(
    delete_req: DeleteDataRequest,
    current_user: User = Depends(require_admin)
):
    """
    æ‰¹é‡åˆ é™¤å¤šä¸ªæ—¥æœŸçš„æ•°æ®
    """
    try:
        from ..database import SessionLocal
        from ..db_models import DailyStockData, SectorDailyData
        from sqlalchemy import func
        
        results = []
        total_stock = 0
        total_sector = 0
        
        db = SessionLocal()
        try:
            for date in delete_req.dates:
                date_str = date.replace("-", "")
                result = {"date": date_str, "stock_deleted": 0, "sector_deleted": 0}
                
                if delete_req.data_type in ["stock", "all"]:
                    stock_deleted = db.query(DailyStockData).filter(
                        func.to_char(DailyStockData.date, 'YYYYMMDD') == date_str
                    ).delete(synchronize_session=False)
                    result["stock_deleted"] = stock_deleted
                    total_stock += stock_deleted
                
                if delete_req.data_type in ["sector", "all"]:
                    sector_deleted = db.query(SectorDailyData).filter(
                        func.to_char(SectorDailyData.date, 'YYYYMMDD') == date_str
                    ).delete(synchronize_session=False)
                    result["sector_deleted"] = sector_deleted
                    total_sector += sector_deleted
                
                results.append(result)
            
            db.commit()
            
            # æ›´æ–°å¯¼å…¥çŠ¶æ€
            try:
                from scripts.import_state_manager import ImportStateManager, reload_state_managers
                
                if delete_req.data_type in ["stock", "all"]:
                    stock_state = ImportStateManager("data_import_state.json")
                    for date in delete_req.dates:
                        stock_state.mark_deleted(date.replace("-", ""), "batch_delete", current_user.username)
                
                if delete_req.data_type in ["sector", "all"]:
                    sector_state = ImportStateManager("sector_import_state.json")
                    for date in delete_req.dates:
                        sector_state.mark_deleted(date.replace("-", ""), "batch_delete", current_user.username)
                
                # åˆ·æ–°å•ä¾‹çŠ¶æ€ï¼ˆç¡®ä¿ä¸‹æ¬¡å¯¼å…¥èƒ½æ­£ç¡®è¯»å– deleted çŠ¶æ€ï¼‰
                reload_state_managers()
                        
            except Exception as state_err:
                logger.warning(f"æ›´æ–°å¯¼å…¥çŠ¶æ€å¤±è´¥: {state_err}")
            
            logger.info(f"ç®¡ç†å‘˜ {current_user.username} æ‰¹é‡åˆ é™¤æ•°æ®: {len(delete_req.dates)} å¤©, è‚¡ç¥¨ {total_stock} æ¡, æ¿å— {total_sector} æ¡")
            
            # é‡è½½ç¼“å­˜
            try:
                from ..core.startup import preload_cache
                logger.info("ğŸ”„ åˆ é™¤åé‡è½½ç¼“å­˜...")
                preload_cache()
                # v0.5.0: æ¸…ç†ç»Ÿä¸€ç¼“å­˜ç³»ç»Ÿçš„ API ç¼“å­˜å’Œçƒ­ç‚¹æ¦œç¼“å­˜
                cache.clear_api_cache()
                HotSpotsCache.clear_cache()
                logger.info("âœ… ç¼“å­˜é‡è½½å®Œæˆ (å«ç»Ÿä¸€ç¼“å­˜)")
            except Exception as cache_err:
                logger.warning(f"âš ï¸ ç¼“å­˜é‡è½½å¤±è´¥: {cache_err}")
            
            return {
                "success": True,
                "results": results,
                "summary": {
                    "dates_count": len(delete_req.dates),
                    "stock_deleted": total_stock,
                    "sector_deleted": total_sector,
                    "total_deleted": total_stock + total_sector
                },
                "message": f"å·²åˆ é™¤ {len(delete_req.dates)} å¤©çš„æ•°æ®ï¼Œå…± {total_stock + total_sector} æ¡"
            }
            
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ é™¤æ•°æ®å¤±è´¥: {str(e)}")
