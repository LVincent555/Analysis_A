#!/usr/bin/env python3
"""
æ¯æ—¥æ•°æ®æ›´æ–°å·¥å…· - ç»Ÿä¸€å…¥å£
æ”¯æŒå­å‘½ä»¤æ¨¡å¼å’Œç›´æ¥è°ƒç”¨

ä½¿ç”¨æ–¹æ³•ï¼š
    python update_daily_data.py                    # å¯¼å…¥æ•°æ®ï¼ˆé»˜è®¤ï¼‰
    python update_daily_data.py import             # å¯¼å…¥æ•°æ®
    python update_daily_data.py scan               # æ‰«ææ–‡ä»¶
    python update_daily_data.py clean --scan       # æŸ¥çœ‹è­¦å‘Š
    python update_daily_data.py clean --dry-run    # é¢„æ¼”æ¸…ç†
    python update_daily_data.py clean              # æ‰§è¡Œæ¸…ç†
    python update_daily_data.py delete --dates 20251117 --dry-run  # é¢„æ¼”åˆ é™¤æŒ‡å®šæ—¥æœŸ
    python update_daily_data.py delete --dates 20251117            # åˆ é™¤æŒ‡å®šæ—¥æœŸæ•°æ®
    python update_daily_data.py fix --scan         # æŸ¥çœ‹ä¿®è¡¥å†å²
    python update_daily_data.py fix --dates 20251117 --dry-run     # é¢„æ¼”ä¿®è¡¥æŒ‡å®šæ—¥æœŸ
    python update_daily_data.py fix --dates 20251117               # ä¿®è¡¥æŒ‡å®šæ—¥æœŸ
    python update_daily_data.py fix --rollback --dates 20251117    # å›æ»šä¿®è¡¥
"""
import sys
import argparse
from pathlib import Path
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
log_dir = Path(__file__).parent / "logs"
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f"update_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# æ·»åŠ backend/scriptså’Œbackendåˆ°è·¯å¾„
backend_scripts = Path(__file__).parent / "backend" / "scripts"
backend_dir = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_scripts))
sys.path.insert(0, str(backend_dir))


def cmd_import(args):
    """å¯¼å…¥æ•°æ®"""
    from data_importer import DataImporter
    from app.config import DATA_DIR
    
    start_time = datetime.now()
    logger.info("ğŸš€ æ¯æ—¥æ•°æ®æ›´æ–°ä»»åŠ¡å¼€å§‹")
    logger.info(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("")
    
    importer = DataImporter(Path(DATA_DIR))
    results = importer.import_all(skip_scan=args.skip_scan)
    
    # æ€»ç»“
    logger.info("")
    logger.info("=" * 70)
    logger.info("ğŸ“Š æ•°æ®æ›´æ–°ä»»åŠ¡å®Œæˆ")
    logger.info("=" * 70)
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f}ç§’")
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("")
    
    if results['stock_success'] and results['sector_success']:
        logger.info("âœ… æ‰€æœ‰æ•°æ®å¯¼å…¥æˆåŠŸï¼")
        return 0
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†æ•°æ®å¯¼å…¥å¤±è´¥")
        return 1


def cmd_scan(args):
    """æ‰«ææ–‡ä»¶å®Œæ•´æ€§"""
    from data_scanner import DataScanner
    from app.config import DATA_DIR
    
    logger.info("ğŸ” å¼€å§‹æ‰«ææ–‡ä»¶å®Œæ•´æ€§...")
    logger.info("")
    
    scanner = DataScanner(Path(DATA_DIR))
    
    if args.type in ['stock', 'all']:
        scanner.scan_stock_files()
    
    if args.type in ['sector', 'all']:
        scanner.scan_sector_files()
    
    logger.info("")
    logger.info("âœ… æ‰«æå®Œæˆ")
    return 0


def cmd_clean(args):
    """æ¸…ç†å­¤å„¿æ•°æ®"""
    from data_cleaner import DataCleaner
    
    logger.info("ğŸ—‘ï¸  å­¤å„¿æ•°æ®æ¸…ç†å·¥å…·")
    logger.info("")
    
    dates = None
    if args.dates:
        dates = [d.strip() for d in args.dates.split(',')]
    
    # æ‰«ææ¨¡å¼
    if args.scan:
        if args.type in ['stock', 'all']:
            logger.info("ğŸ“Š æ‰«æè‚¡ç¥¨æ•°æ®...")
            cleaner = DataCleaner('stock')
            cleaner.scan_warnings()
        
        if args.type in ['sector', 'all']:
            logger.info("\nğŸ“Š æ‰«ææ¿å—æ•°æ®...")
            cleaner = DataCleaner('sector')
            cleaner.scan_warnings()
        
        logger.info("\nğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¸…ç†æ•°æ®ï¼š")
        logger.info("   é¢„æ¼”: python update_daily_data.py clean --dry-run")
        logger.info("   æ‰§è¡Œ: python update_daily_data.py clean")
        return 0
    
    # æ¸…ç†æ¨¡å¼
    if not args.dry_run:
        logger.warning("âš ï¸  è­¦å‘Šï¼šå³å°†åˆ é™¤æ•°æ®åº“æ•°æ®ï¼")
        response = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): ")
        if response.lower() not in ['yes', 'y']:
            logger.info("âŒ å·²å–æ¶ˆ")
            return 1
    
    if args.type in ['stock', 'all']:
        logger.info("ğŸ“Š æ¸…ç†è‚¡ç¥¨æ•°æ®...")
        cleaner = DataCleaner('stock')
        cleaner.clean_warnings(dry_run=args.dry_run, dates=dates)
    
    if args.type in ['sector', 'all']:
        logger.info("\nğŸ“Š æ¸…ç†æ¿å—æ•°æ®...")
        cleaner = DataCleaner('sector')
        cleaner.clean_warnings(dry_run=args.dry_run, dates=dates)
    
    return 0


def cmd_delete(args):
    """ä¸»åŠ¨åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆå¼ºåˆ¶åˆ é™¤ï¼‰"""
    from data_cleaner import DataCleaner
    
    if not args.dates:
        logger.error("âŒ é”™è¯¯ï¼šå¿…é¡»æŒ‡å®š --dates å‚æ•°")
        logger.info("ç¤ºä¾‹: python update_daily_data.py delete --dates 20251117")
        logger.info("å¤šä¸ªæ—¥æœŸ: python update_daily_data.py delete --dates 20251117,20251116")
        return 1
    
    dates = [d.strip() for d in args.dates.split(',')]
    
    logger.info("ğŸ”¥ ä¸»åŠ¨åˆ é™¤æ•°æ®å·¥å…·")
    logger.info("")
    logger.warning("âš ï¸  æ­¤å‘½ä»¤å°†å¼ºåˆ¶åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼Œæ— è®ºå…¶çŠ¶æ€å¦‚ä½•")
    logger.warning("âš ï¸  é€‚ç”¨åœºæ™¯ï¼šæ•°æ®æºæœ‰é—®é¢˜ï¼Œéœ€è¦é‡æ–°å¯¼å…¥")
    logger.info("")
    logger.info(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {', '.join(dates)}")
    logger.info(f"ğŸ“Š æ•°æ®ç±»å‹: {args.type}")
    logger.info("")
    
    # ç¡®è®¤æ¨¡å¼
    if not args.dry_run:
        logger.warning("âš ï¸  è­¦å‘Šï¼šå³å°†æ°¸ä¹…åˆ é™¤æ•°æ®åº“æ•°æ®ï¼")
        logger.warning("   åˆ é™¤åéœ€è¦é‡æ–°å¯¼å…¥Excelæ–‡ä»¶æ‰èƒ½æ¢å¤æ•°æ®")
        logger.info("")
        response = input("ç¡®è®¤åˆ é™¤ï¼Ÿè¾“å…¥æ—¥æœŸä»¥ç¡®è®¤ (æˆ–è¾“å…¥ no å–æ¶ˆ): ")
        
        # éªŒè¯ç”¨æˆ·è¾“å…¥çš„æ—¥æœŸ
        if response.lower() in ['no', 'n']:
            logger.info("âŒ å·²å–æ¶ˆ")
            return 1
        
        if len(dates) == 1 and response != dates[0]:
            logger.error("âŒ æ—¥æœŸä¸åŒ¹é…ï¼Œå·²å–æ¶ˆ")
            return 1
        
        if len(dates) > 1 and response.lower() not in ['yes', 'y']:
            logger.error("âŒ è¯·è¾“å…¥ yes ç¡®è®¤åˆ é™¤å¤šä¸ªæ—¥æœŸ")
            return 1
    
    # æ‰§è¡Œåˆ é™¤
    if args.type in ['stock', 'all']:
        logger.info("ğŸ“Š åˆ é™¤è‚¡ç¥¨æ•°æ®...")
        cleaner = DataCleaner('stock')
        cleaner.clean_warnings(dry_run=args.dry_run, dates=dates, force=True)
    
    if args.type in ['sector', 'all']:
        logger.info("\nğŸ“Š åˆ é™¤æ¿å—æ•°æ®...")
        cleaner = DataCleaner('sector')
        cleaner.clean_warnings(dry_run=args.dry_run, dates=dates, force=True)
    
    if not args.dry_run:
        logger.info("")
        logger.info("âœ… åˆ é™¤å®Œæˆï¼")
        logger.info("ğŸ’¡ æç¤ºï¼šå¦‚éœ€æ¢å¤æ•°æ®ï¼Œè¯·å°†å¯¹åº”æ—¥æœŸçš„Excelæ–‡ä»¶æ”¾å…¥dataç›®å½•ï¼Œç„¶åè¿è¡Œï¼š")
        logger.info("   python update_daily_data.py import")
    
    return 0


def cmd_fix(args):
    """æ•°æ®ä¿®è¡¥å·¥å…·ï¼ˆæ¢æ‰‹ç‡ç­‰ï¼‰"""
    from data_fixer import TurnoverRateFixer
    from import_state_manager import ImportStateManager
    
    # æ‰«ææ¨¡å¼
    if args.scan:
        logger.info("ğŸ” æ‰«æä¿®è¡¥å†å²...")
        logger.info("")
        
        if args.type in ['stock', 'all']:
            logger.info("ğŸ“Š è‚¡ç¥¨æ•°æ®ä¿®è¡¥å†å²ï¼š")
            logger.info("=" * 70)
            state_mgr = ImportStateManager("data_import_state.json")
            _print_fix_history(state_mgr)
        
        if args.type in ['sector', 'all']:
            logger.info("\nğŸ“Š æ¿å—æ•°æ®ä¿®è¡¥å†å²ï¼š")
            logger.info("=" * 70)
            state_mgr = ImportStateManager("sector_import_state.json")
            _print_fix_history(state_mgr)
        
        return 0
    
    # å›æ»šæ¨¡å¼
    if args.rollback:
        if not args.dates:
            logger.error("âŒ é”™è¯¯ï¼šå›æ»šæ“ä½œå¿…é¡»æŒ‡å®š --dates å‚æ•°")
            return 1
        
        dates = [d.strip() for d in args.dates.split(',')]
        
        logger.info("ğŸ”„ å›æ»šæ¢æ‰‹ç‡ä¿®è¡¥")
        logger.info("")
        logger.info(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {', '.join(dates)}")
        logger.info(f"ğŸ“Š æ•°æ®ç±»å‹: {args.type}")
        logger.info("")
        
        if not args.dry_run:
            logger.warning("âš ï¸  è­¦å‘Šï¼šå°†æ’¤é”€æ¢æ‰‹ç‡ä¿®è¡¥ï¼Œæ¢å¤åŸå§‹æ•°æ®ï¼")
            response = input("ç¡®è®¤å›æ»šï¼Ÿ(yes/no): ")
            if response.lower() not in ['yes', 'y']:
                logger.info("âŒ å·²å–æ¶ˆ")
                return 1
        
        # æ‰§è¡Œå›æ»š
        success_count = 0
        for date_str in dates:
            if args.type in ['stock', 'all']:
                logger.info(f"\nğŸ“Š å›æ»šè‚¡ç¥¨æ•°æ® {date_str}...")
                fixer = TurnoverRateFixer(date_str, 'stock')
                result = fixer.rollback_database(dry_run=args.dry_run)
                if result.get('success') or result.get('dry_run'):
                    if not args.dry_run:
                        fixer.record_rollback_to_state(result)
                    success_count += 1
        
        if not args.dry_run:
            logger.info(f"\nâœ… å›æ»šå®Œæˆï¼æˆåŠŸ {success_count} ä¸ªæ—¥æœŸ")
        
        return 0
    
    # ä¿®è¡¥æ¨¡å¼
    if not args.dates:
        logger.error("âŒ é”™è¯¯ï¼šä¿®è¡¥æ“ä½œå¿…é¡»æŒ‡å®š --dates å‚æ•°")
        logger.info("ç¤ºä¾‹: python update_daily_data.py fix --dates 20251117")
        return 1
    
    dates = [d.strip() for d in args.dates.split(',')]
    
    logger.info("ğŸ”§ æ•°æ®ä¿®è¡¥å·¥å…·")
    logger.info("")
    logger.info(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {', '.join(dates)}")
    logger.info(f"ğŸ“Š æ•°æ®ç±»å‹: {args.type}")
    logger.info("")
    
    if not args.dry_run:
        logger.warning("âš ï¸  è­¦å‘Šï¼šå°†å¯¹æ•°æ®åº“æ•°æ®æ‰§è¡Œä¿®è¡¥æ“ä½œï¼")
        response = input("ç¡®è®¤ä¿®è¡¥ï¼Ÿ(yes/no): ")
        if response.lower() not in ['yes', 'y']:
            logger.info("âŒ å·²å–æ¶ˆ")
            return 1
    
    # æ‰§è¡Œä¿®è¡¥
    success_count = 0
    for date_str in dates:
        if args.type in ['stock', 'all']:
            logger.info(f"\nğŸ“Š ä¿®è¡¥è‚¡ç¥¨æ•°æ® {date_str}...")
            fixer = TurnoverRateFixer(date_str, 'stock')
            result = fixer.fix_database(dry_run=args.dry_run)
            if result.get('applied') or result.get('dry_run'):
                if not args.dry_run:
                    fixer.record_fix_to_state(result)
                success_count += 1
    
    if not args.dry_run:
        logger.info(f"\nâœ… ä¿®è¡¥å®Œæˆï¼æˆåŠŸ {success_count} ä¸ªæ—¥æœŸ")
    
    return 0


def _print_fix_history(state_mgr: 'ImportStateManager'):
    """æ‰“å°ä¿®è¡¥å†å²"""
    imports = state_mgr.state.get('imports', {})
    
    has_fixes = False
    for date_str in sorted(imports.keys(), reverse=True):
        import_info = imports[date_str]
        data_fixes = import_info.get('data_fixes', {})
        turnover_fix = data_fixes.get('turnover_rate_fix')
        
        if turnover_fix and turnover_fix.get('applied'):
            has_fixes = True
            logger.info(f"\nğŸ“… {date_str}:")
            logger.info(f"   çŠ¶æ€: âœ… å·²ä¿®è¡¥")
            logger.info(f"   å€æ•°: {turnover_fix.get('multiplier', 10000)}")
            logger.info(f"   å½±å“è¡Œæ•°: {turnover_fix.get('affected_rows', 0)}")
            logger.info(f"   ä¿®è¡¥å‰å¹³å‡: {turnover_fix.get('avg_before', 0):.6f}")
            logger.info(f"   ä¿®è¡¥åå¹³å‡: {turnover_fix.get('avg_after', 0):.4f}")
            logger.info(f"   ä¿®è¡¥æ—¶é—´: {turnover_fix.get('fixed_at', 'N/A')}")
    
    if not has_fixes:
        logger.info("   æ²¡æœ‰ä¿®è¡¥è®°å½•")
    
    logger.info("")


def main():
    parser = argparse.ArgumentParser(
        description='æ¯æ—¥æ•°æ®æ›´æ–°å’Œç»´æŠ¤å·¥å…·',
        epilog='ç¤ºä¾‹: python update_daily_data.py import'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # å¯¼å…¥å‘½ä»¤ï¼ˆé»˜è®¤ï¼‰
    import_parser = subparsers.add_parser('import', help='å¯¼å…¥æ•°æ®')
    import_parser.add_argument(
        '--skip-scan', 
        action='store_true', 
        help='è·³è¿‡æ–‡ä»¶æ‰«æ'
    )
    
    # æ‰«æå‘½ä»¤
    scan_parser = subparsers.add_parser('scan', help='æ‰«ææ–‡ä»¶å®Œæ•´æ€§')
    scan_parser.add_argument(
        '--type',
        choices=['stock', 'sector', 'all'],
        default='all',
        help='æ•°æ®ç±»å‹'
    )
    
    # æ¸…ç†å‘½ä»¤
    clean_parser = subparsers.add_parser('clean', help='æ¸…ç†å­¤å„¿æ•°æ®')
    clean_parser.add_argument('--scan', action='store_true', help='åªæ‰«æä¸æ¸…ç†')
    clean_parser.add_argument('--dry-run', action='store_true', help='é¢„æ¼”æ¨¡å¼')
    clean_parser.add_argument(
        '--type',
        choices=['stock', 'sector', 'all'],
        default='all',
        help='æ•°æ®ç±»å‹'
    )
    clean_parser.add_argument('--dates', type=str, help='æŒ‡å®šæ—¥æœŸï¼ˆé€—å·åˆ†éš”ï¼‰')
    
    # åˆ é™¤å‘½ä»¤ï¼ˆä¸»åŠ¨åˆ é™¤ï¼‰
    delete_parser = subparsers.add_parser('delete', help='ä¸»åŠ¨åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®')
    delete_parser.add_argument(
        '--dates',
        type=str,
        required=True,
        help='è¦åˆ é™¤çš„æ—¥æœŸï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚: 20251117 æˆ– 20251117,20251116ï¼‰'
    )
    delete_parser.add_argument('--dry-run', action='store_true', help='é¢„æ¼”æ¨¡å¼ï¼ˆä¸çœŸæ­£åˆ é™¤ï¼‰')
    delete_parser.add_argument(
        '--type',
        choices=['stock', 'sector', 'all'],
        default='all',
        help='æ•°æ®ç±»å‹ï¼ˆé»˜è®¤: allï¼‰'
    )
    
    # ä¿®è¡¥å‘½ä»¤ï¼ˆæ•°æ®ä¿®è¡¥ï¼‰
    fix_parser = subparsers.add_parser('fix', help='æ•°æ®ä¿®è¡¥å·¥å…·ï¼ˆæ¢æ‰‹ç‡ç­‰ï¼‰')
    fix_parser.add_argument('--scan', action='store_true', help='æŸ¥çœ‹ä¿®è¡¥å†å²')
    fix_parser.add_argument('--rollback', action='store_true', help='å›æ»šä¿®è¡¥æ“ä½œ')
    fix_parser.add_argument('--dates', type=str, help='è¦ä¿®è¡¥/å›æ»šçš„æ—¥æœŸï¼ˆé€—å·åˆ†éš”ï¼‰')
    fix_parser.add_argument('--dry-run', action='store_true', help='é¢„æ¼”æ¨¡å¼ï¼ˆä¸çœŸæ­£ä¿®è¡¥ï¼‰')
    fix_parser.add_argument(
        '--type',
        choices=['stock', 'sector', 'all'],
        default='stock',
        help='æ•°æ®ç±»å‹ï¼ˆé»˜è®¤: stockï¼Œæ¿å—æš‚ä¸æ”¯æŒï¼‰'
    )
    
    args = parser.parse_args()
    
    # é»˜è®¤å‘½ä»¤ä¸ºimport
    if not args.command:
        args.command = 'import'
        args.skip_scan = False
    
    try:
        if args.command == 'import':
            return cmd_import(args)
        elif args.command == 'scan':
            return cmd_scan(args)
        elif args.command == 'clean':
            return cmd_clean(args)
        elif args.command == 'delete':
            return cmd_delete(args)
        elif args.command == 'fix':
            return cmd_fix(args)
    except KeyboardInterrupt:
        logger.info("\n\nâŒ ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        logger.debug(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
